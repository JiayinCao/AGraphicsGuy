<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A Graphics Guy's Note</title>
    <link>https://agraphicsguynotes.com/posts/</link>
    <description>Recent content in Posts on A Graphics Guy&#39;s Note</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 05 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://agraphicsguynotes.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding The Math Behind ReStir DI</title>
      <link>https://agraphicsguynotes.com/posts/understanding_the_math_behind_restir_di/</link>
      <pubDate>Mon, 05 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/understanding_the_math_behind_restir_di/</guid>
      <description>&lt;p&gt;Ever since the introduction of RTX technology, the industry pushed really hard on real time GI solutions. ReStir (Reservoir Spatio-Temporal Importance Resampling) is one of the new popular topics lately. The algorithm can be applied in multiple applications, there are several different variations of the algorithm, like &lt;a href=&#34;https://research.nvidia.com/publication/2020-07_spatiotemporal-reservoir-resampling-real-time-ray-tracing-dynamic-direct&#34;&gt;ReStir DI&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/publication/2021-06_restir-gi-path-resampling-real-time-path-tracing&#34;&gt;ReStir GI&lt;/a&gt;, &lt;a href=&#34;https://research.nvidia.com/publication/2022-07_generalized-resampled-importance-sampling-foundations-restir&#34;&gt;ReStir PT&lt;/a&gt;, &lt;a href=&#34;https://graphics.cs.utah.edu/research/projects/volumetric-restir/&#34;&gt;Volumetric ReStir&lt;/a&gt;, etc.&lt;/p&gt;
&lt;p&gt;The original papers are clearly the best resources to learn the tech. Due to the complexity of the algorithm, understanding the theory and math remains challenging for graphics programmers who are not very familiar with sampling methods. There are also a few blog posts &lt;sup&gt;&lt;a href=&#34;http://www.zyanidelab.com/how-to-add-thousands-of-lights-to-your-renderer/&#34;&gt;[5]&lt;/a&gt; &lt;a href=&#34;https://gamehacker1999.github.io/posts/restir/&#34;&gt;[6]&lt;/a&gt;&lt;/sup&gt;
 available. However, there are still some fundamental mathematical questions not answered in the posts, which are fairly important for us to understand why the math works the way it does in ReStir.&lt;/p&gt;
&lt;p&gt;In this blog post, rather than talking about the different variations of ReStir, I would like to talk about the very original method, ReStir DI. Different from existing blog posts, I would not mention any implementation details. The whole blog will be focusing on the mathematics behind the original ReStir, with more in-depth analysis of why it works. This should be a complementary reading of the original ReStir DI paper. It is highly recommended to go through the original &lt;a href=&#34;https://research.nvidia.com/publication/2020-07_spatiotemporal-reservoir-resampling-real-time-ray-tracing-dynamic-direct&#34;&gt;paper&lt;/a&gt; first before start reading this post. Hopefully, after reading this post, readers would have a deeper understanding of how ReStir DI works mathematically.&lt;/p&gt;
&lt;h1 id=&#34;preliminary-reading&#34;&gt;Preliminary Reading&lt;/h1&gt;
&lt;p&gt;Since ReStir DI is not an easy algorithm to understand and lots of the details rely on some really basic theory. I think it makes a lot of sense to go through the basics first before starting to explain ReStir. So the first half of the post will be mainly about reviewing some fundamental concepts so that we would have a solid understanding of what is needed to understand ReStir.&lt;/p&gt;
&lt;h2 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h2&gt;
&lt;p&gt;This is the most fundamental concept in evaluating the rendering equation. Most of the details will be skipped as I would assume readers already have a decent understanding of it.&lt;/p&gt;
&lt;p&gt;To evaluate the integral of $\int f(x) dx$, we would need to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw N samples from a PDF&lt;/li&gt;
&lt;li&gt;Drop the N samples in the Monte Carlo estimator&lt;br&gt;
$I=\dfrac{1}{N} \Sigma_{i=1}^N f(x_i)/p(x_i)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is so easy to prove this that I would skip the math. If you would like to know how, check out my previous post &lt;a href=&#34;https://agraphicsguynotes.com/posts/monte_carlo_integral_with_multiple_importance_sampling/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, I would like to point out an important condition here.
The domain of p(x) has to cover the whole domain of f(x). If it doesn&amp;rsquo;t, this can&amp;rsquo;t work.&lt;/p&gt;
&lt;p&gt;To prove it, let&amp;rsquo;s say there is a subdomain in the f(x)&amp;rsquo;s domain $\Omega$ that is not covered by p(x), naming it $\Omega_1$.
This introduces a blind spot in the sampling domain. Clearly, we have&lt;/p&gt;

$$ \Omega = \Omega_0 &amp;#43; \Omega_1 $$
&lt;p&gt;Working out the math here&lt;/p&gt;

$$ \begin{array} {lcl} E[I] &amp;amp; = &amp;amp; \dfrac{1}{N} E[ \sum\limits_{i=1}^N \dfrac{f(x_{i})}{p(x_{i})}] \\\\ &amp;amp; = &amp;amp; \dfrac{1}{N} \int_{\Omega_0} \sum\limits_{i=1}^N \dfrac{f(x)}{p(x)} p(x) dx \\\\ &amp;amp; = &amp;amp; \int_{\Omega_0} f(x) dx \\\\ &amp;amp; = &amp;amp; \int_{\Omega} f(x)dx - \int_{\Omega_1} f(x) dx \end{array} $$
&lt;p&gt;As we can see, we have a cut from the expected result. Intuitively explaining, this is because the estimator is no longer a function of any signal inside the domain $\Omega_1$ anymore, while the target is a function of it. If anything is different inside that domain, it would cause a different result integral while this change can&amp;rsquo;t be detected by the estimator as the PDF is unable to capture any signal there. Mathematically speaking, we can see that as long as $\int_{\Omega_1} f(x) dx$ is not zero, the result won&amp;rsquo;t be what we want. And it is almost not zero all the time. As a matter of fact, if we know beforehand, f(x) integrates to 0 over the domain of $\Omega_1$, we would only need to evaluate the integral of the function in the domain of $\Omega_0$ in the first place.&lt;/p&gt;
&lt;h2 id=&#34;multiple-importance-sampling&#34;&gt;Multiple Importance Sampling&lt;/h2&gt;
&lt;p&gt;MIS is a common technique to improve sampling efficiency by taking multiple samples from different sources of PDFs, rather than just one.
This method commonly improves sampling efficiency and can deal with integrals that are hard to approximate with one single source of PDF.&lt;/p&gt;

$$ I_{mis}^{M,N} = \Sigma_{i=1}^M \dfrac{1}{N_i} \Sigma_{j=1}^{N_i} w_i(x_{ij}) \dfrac{f(x_{ij})}{p_i(x_{ij})} $$
&lt;p&gt;For MIS to be mathematically unbiased, it has to meet a few requirements.
In the domain of f(x),&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We must have $ \Sigma_{i=1}^M w_i(x) = 1 $&lt;/li&gt;
&lt;li&gt;The union of the domains of all source PDFs must cover the domain of f(x)&lt;/li&gt;
&lt;li&gt;$w_i(x)$ must be 0 whenever $p_i(x) = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since this post is about explaining ReStir, I would only put down some details that matter to ReStir. Other characteristics of MIS would be skipped. For a deeper explanation of MIS, please check out &lt;a href=&#34;https://graphics.stanford.edu/papers/veach_thesis/thesis.pdf&#34;&gt;Eric Veach&amp;rsquo;s Thesis&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;mathematical-proof-of-mis&#34;&gt;Mathematical Proof of MIS&lt;/h3&gt;
&lt;p&gt;In order to have a deeper understanding of this, let&amp;rsquo;s see how the math works under the hood first.&lt;/p&gt;

$$ \begin{array} {lcl} E[I_{mis}^{M,N}] &amp;amp; = &amp;amp; E[ \sum\limits_{i=1}^M \dfrac{1}{N_i} \sum\limits_{j=1}^{N_i} w_i(x_{ij})\dfrac{f(x_{ij})}{p_i(x_{ij})}] \\\\ &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^M \dfrac{1}{N_i} \sum\limits_{j=1}^{N_i} w_i(x)\dfrac{f(x)}{p_i(x)} p_i(x) dx \\\\ &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^M \dfrac{1}{N_i} \sum\limits_{j=1}^{N_i} w_i(x) f(x) dx \\\\ &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^M w_i(x) f(x) dx \end{array} $$
&lt;p&gt;It is obvious that as long as it meets the first condition ($ \Sigma_{i=1}^M w_i(x) = 1 $), we would have an unbiased approximation of the integral of our interest.&lt;/p&gt;

$$ \begin{array} {lcl} E[I_{mis}^{M,N}] &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^M w_i(x) f(x) dx \\\\ &amp;amp; = &amp;amp; \int f(x) dx \end{array} $$
&lt;p&gt;For the second requirement, since it is less relevant to ReStir, I would only put down some intuitive explanation here, leaving the math as a practice to readers (The proof should be fairly similar to what I showed in the previous section, but slightly more complicated). Same as the importance sampling blind spot issue mentioned above, if none of the PDF covers a sub-domain of the target function, it won&amp;rsquo;t pick up any changes in that domain either, while changes in that domain will indeed cause changes in the final result. This easily makes the estimate not accurate anymore.&lt;/p&gt;
&lt;p&gt;However, this is by no means to say that all source PDF&amp;rsquo;s domains would have to cover the domain of the function to be integrated. For any sub-domain, as long as one of them covers it, it is good enough. Though there is one important condition, which is the third one mentioned above. Since it is slightly trickier to understand, I&amp;rsquo;ll put down the math proof here,&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s imagine for one PDF $p_k$, it has zero probability of taking a sample in a sub-domain, $\Omega_1$. We can then divide the domain of the function $\Omega$ into two parts $\Omega_0$ and $\Omega_1$. So we have&lt;/p&gt;

$$ \Omega = \Omega_0 &amp;#43; \Omega_1 $$
&lt;p&gt;So let&amp;rsquo;s divide the previous MIS proof into two domains and see what happens,&lt;/p&gt;

$$ \begin{array} {lcl} E[I_{mis}^{M,N}] &amp;amp; = &amp;amp; E[ \sum\limits_{i=1}^M \dfrac{1}{N_i} \sum\limits_{j=1}^{N_i} w_i(x_{ij})\dfrac{f(x_{ij})}{p_i(x_{ij})}] \\\\ &amp;amp; = &amp;amp; E[ \sum\limits_{i=1, i!=k}^M \dfrac{1}{N_i} \sum\limits_{j=1}^{N_i} w_i(x_{ij})\dfrac{f(x_{ij})}{p_i(x_{ij})} &amp;#43; \dfrac{1}{N_k} \sum\limits_{j=1}^{N_k} w_k(x_{kj})\dfrac{f(x_{kj})}{p_k(x_{kj})}] \\\\ &amp;amp; = &amp;amp; \int_{\Omega} \sum\limits_{i=1,i!=k}^M \dfrac{1}{N_i} \sum\limits_{j=1}^{N_i} w_i(x)\dfrac{f(x)}{p_i(x)} p_i(x) dx &amp;#43; \int_{\Omega_0} \dfrac{1}{N_k} \sum\limits_{j=1}^{N_k} w_k(x)\dfrac{f(x)}{p_k(x)} p_k(x) dx \\\\ &amp;amp; = &amp;amp; \Sigma_{i=1, i!=k}^M \int_{\Omega} w_i(x)f(x)dx &amp;#43; \int_{\Omega_0}w_k(x)f(x)dx \\\\ &amp;amp; = &amp;amp; \Sigma_{i=1}^M \int_{\Omega} w_i(x)f(x)dx - \int_{\Omega_1}w_k(x)f(x)dx  \\\\ &amp;amp; = &amp;amp; \int f(x) dx - \int_{\Omega_1}w_k(x)f(x)dx \end{array} $$
&lt;p&gt;What happens here is that our estimator has an extra cut from the expected value just like the importance sampling blind spot issue mentioned above. And this is exactly why we need to make sure we meet the third condition. If we do meet the third condition, what would happen is that the extra cut will be canceled because $w_k(x)$ is zero, leaving the rest parts precisely what we want. Making the estimator unbiased again.&lt;/p&gt;
&lt;p&gt;As a side note, I would like to point out that in theory, we don&amp;rsquo;t really need to strictly meet the first or third condition to make the algorithm unbiased. There is a more generalized form&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First condition&lt;br&gt;
$\int \Sigma_{i=1}^M w(x) f(x) dx = \int f(x) dx$&lt;/li&gt;
&lt;li&gt;Third condition&lt;br&gt;
$\int_{\Omega_1} w_k(x) f(x) dx = 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in almost all contexts, it is a lot more tricky to meet this requirement without meeting the special form of them mentioned above. So this is rarely mentioned in most materials.&lt;/p&gt;
&lt;h3 id=&#34;commonly-used-mis-weight&#34;&gt;Commonly Used MIS Weight&lt;/h3&gt;
&lt;p&gt;Here I&amp;rsquo;d like to point out a few commonly used weight&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balance Heuristic&lt;br&gt;
This is the most commonly used weight function in MIS. The mathematical form of the weight is quite simple, $ w_k(x) = N_k p_k(x) / \Sigma_{i=1}^{M} N_i p_i(x)$ . If we pay attention to the details, we would notice that it automatically fulfills the first and third conditions, which is very nice. Even though the weight looks quite simple, sometimes it can get quite complicated, like in the algorithm of &lt;a href=&#34;https://agraphicsguynotes.com/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/&#34;&gt;Bidirectional Path Tracing&lt;/a&gt;.   What is even worse is that sometimes the PDFs are not even trackable, making it impossible to use this method.&lt;/li&gt;
&lt;li&gt;Power Heuristic&lt;br&gt;
This weight is sort of similar to the balance heuristic, except it adds a power on top of each element in the nominator and denominator, leaving the math form like this $ w_k(x) = (N_k p_k(x))^\beta / \Sigma_{i=1}^{M} (N_i p_i(x))^{\beta}$. We can easily tell here that the balance heuristic is nothing but a special form of power heuristic when $\beta$ is simply one. And this weight also meets the first and third conditions as well. It is less commonly used compared with the balance heuristic, but it is indeed used in some cases for better sampling efficiency.&lt;/li&gt;
&lt;li&gt;Uniform Weight&lt;br&gt;
As its name implies, it is the same across all samples. Since it is constant and it has to meet the first condition, we can easily imagine what the weight is, it should simply be $ w_k(x) = {1}/{N_k} $.&lt;br&gt;
Not really meeting the third condition makes this weight seems to be pretty ridiculous at the beginning. As a matter of fact, if we use this weight, it possibly won&amp;rsquo;t reduce the fireflies at all, making this weight very unattractive. However, in the case where the PDFs are not even trackable, this is the only weight that we can consider using when it comes to MIS. And special attention is needed to make sure the method is unbiased. To counter the bias, we would need to count the number of PDFs that are non-zero at any specific sample and use that as the denominator, it would correct the bias then. Readers can try this idea in the math proof, it should be pretty easy to understand how it works.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sample-importance-resampling-sir&#34;&gt;Sample Importance Resampling (SIR)&lt;/h2&gt;
&lt;p&gt;Like inversion sampling, SIR is a method for drawing samples given a PDF. Though SIR requires way less from the target PDF itself.
However, this method only approximates sampling the target distribution, it is not really drawing samples from the target PDF.&lt;/p&gt;
&lt;p&gt;Given a target PDF $\hat{p}(x)$ to sample from, the basic workflow of SIR is as below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Take M samples from a simpler and trackable PDF, we call this PDF proposal PDF, $p(x)$&lt;/li&gt;
&lt;li&gt;For each sample taken, we assign a weight for the sample, which is&lt;br&gt;
$ w(x) = {\hat{p}(x)}/{p(x)}$&lt;/li&gt;
&lt;li&gt;Next, draw a sample from this proposal sample set of M. The probability of each sample being picked is proportional to its weight. And throughout the rest of this post, this type of sample would be named as target sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The workflow is simple and straightforward. I&amp;rsquo;d like to point out a few different types of PDFs involved in this process&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposal PDF&lt;br&gt;
This is what we use to draw samples from in the first place. It should be trackable and cheap enough to take samples from as we would take multiple proposal samples from this PDF.
I also want to point out that if this proposal PDF is expensive, it won&amp;rsquo;t have any impact on the unbiasedness of the algorithm though, except that it would need some extra treatment to avoid the cost. For example, in the work of &lt;a href=&#34;https://research.nvidia.com/publication/2021-06_restir-gi-path-resampling-real-time-path-tracing&#34;&gt;ReStir GI&lt;/a&gt;, in order to generate a proposal sample, a ray needs to be shot, making it expensive, forcing the algorithm to take a lot less number of proposal samples compared with other variations of ReStir. The paper distributes the proposal samples across frames to achieve good performance.&lt;/li&gt;
&lt;li&gt;Target PDF&lt;br&gt;
Target pdf is what we used to assign the weight. Ideally, it shouldn&amp;rsquo;t be expensive either.
Just like the proposal PDF, it could be.&lt;br&gt;
One nice property of the target PDF is that it doesn&amp;rsquo;t even need to be normalized. This is quite a nice property and later we will see why this limitation doesn&amp;rsquo;t apply here.&lt;/li&gt;
&lt;li&gt;SIR PDF&lt;br&gt;
Some work called it real PDF. For consistency, I would call it SIR PDF throughout the whole post. This is the PDF that we are really drawing samples from. Clearly, we are not really drawing the final samples from the proposal PDF unless the number of proposal samples drawn is one. The truth is, unless the number of proposal samples is unlimited, we are not drawing from the target distribution either. We are actually drawing something in between, something that gradually converges to the target distribution from the proposal PDF as the number of proposal samples grows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The graph below is a good demonstration of what will happen as M grows. The target distribution (green curve) is defined below.
For better visual comparison, this target distribution is indeed a normalized distribution. Though it doesn&amp;rsquo;t have to be.&lt;/p&gt;

$$p(x) = (sin(x) &amp;#43; 1.5) / (3 Pi)$$
&lt;p&gt;The proposal PDF (yellow curve) is simply&lt;/p&gt;

$$p(x) = 1 / (2 Pi)$$
&lt;p&gt;If you try dragging the slider from left you right, meaning more proposal samples are taken, it is obvious that the SIR PDF curve converges to the target distribution we would like to take samples from in the first place.&lt;/p&gt;



























&lt;style&gt;
  .slider {
    width: 100%;               
    background: #d3d3d3;       
    outline: none;             
    opacity: 0.7;              
    -webkit-transition: .2s;   
    transition: opacity .2s;
  }

  .sliderlabel {
    color: #888888;
    text-align: center;
    font-size: 14px;
  }
&lt;/style&gt;

&lt;div style=&#34;margin: auto;&#34;&gt;
  &lt;canvas id=SIR&gt;&lt;/canvas&gt;
  &lt;div class=&#34;slidecontainer&#34;&gt;
    &lt;input type=&#34;range&#34; min=&#34;0&#34; max=&#34;4&#34; value=&#34;0&#34; class=&#34;slider&#34; id=&#34;slider_idSIR&#34;&gt;
    &lt;div id=&#34;sliderlabelSIR&#34; class=&#34;sliderlabel&#34;&gt;Default Label&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;


  
  &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;
  &lt;script src=&#34;https://cdn.jsdelivr.net/npm/chart.js&#34;&gt;&lt;/script&gt;


&lt;script&gt;
  (() =&gt; {
    
    function parsearraydata(arraydata){
        
        first = arraydata.indexOf(&#39;[&#39;)
        last = arraydata.indexOf(&#39;]&#39;)
        striped_str = arraydata.substring(first + 1, last)

        
        return striped_str.split(&#39;,&#39;)
    }

    
    function parsegraphdata(graphdata){
        
        striped_str_arr = parsearraydata(graphdata)

        
        return striped_str_arr.map(function(elem){
          return parseFloat(elem)
        })
    }

    
    function parsegraphs(inputstr){
      return inputstr.map(function(elem){
        return parsegraphdata(elem)
      })
    }

    
    function parsesliderlabels(sliderlabels){
      return sliderlabels.split(&#39;;&#39;)
    }

    
    graphs = parsegraphs(&#34;[0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159,0.159];[0.170,0.182,0.192,0.199,0.204,0.205,0.204,0.200,0.191,0.181,0.168,0.151,0.133,0.114,0.099,0.094,0.099,0.113,0.132,0.152,0.171];[0.169,0.189,0.208,0.223,0.232,0.234,0.230,0.222,0.208,0.189,0.167,0.141,0.114,0.091,0.076,0.070,0.076,0.091,0.114,0.140,0.167];[0.163,0.191,0.218,0.239,0.252,0.256,0.252,0.239,0.218,0.191,0.162,0.131,0.102,0.078,0.062,0.058,0.063,0.078,0.101,0.130,0.160];[0.159,0.192,0.222,0.245,0.260,0.265,0.260,0.245,0.222,0.192,0.159,0.126,0.097,0.073,0.058,0.053,0.058,0.073,0.097,0.126,0.159]&#34;.split(&#39;;&#39;))

    
    converged_graph = parsegraphdata(&#34;[0.159,0.192,0.222,0.245,0.260,0.265,0.260,0.245,0.222,0.192,0.159,0.126,0.097,0.073,0.058,0.053,0.058,0.073,0.097,0.126,0.159]&#34;)

    let isdarkmode = false

    function getgraphcolor() {
        return isdarkmode ? &#39;rgb(255, 192, 128)&#39; : &#39;rgb(255, 192, 64)&#39;;
    }

    function getconvergedgraphcolor() {
        return isdarkmode ? &#39;rgb(128, 196, 128)&#39; : &#39;rgb(128, 196, 128)&#39;;
    }

    function getframecolor() {
        return isdarkmode ? &#39;rgb(128, 128, 128)&#39; : &#39;rgb(32, 32, 32)&#39;;
    }

    function gettextcolor() {
        return isdarkmode ? &#39;rgb(128, 128, 128)&#39; : &#39;rgb(32, 32, 32)&#39;;
    }

    
    let data = {
      labels: parsearraydata(&#34;[0.0, , , , , , , , , , π, , , , , , , , , , 2π]&#34;),
      datasets: [
        {
          label: &#34;SIR PDF&#34;,
          data: graphs[0],
          fill: false,
          cubicInterpolationMode: &#39;monotone&#39;,
          tension: 0.4,
          borderColor: getgraphcolor(),
          backgroundColor: getgraphcolor(),
        },
        {
          label: &#34;Target Distribution&#34;,
          data: converged_graph,
          fill: false,
          cubicInterpolationMode: &#39;monotone&#39;,
          tension: 0.4,
          borderColor: getconvergedgraphcolor(),
          backgroundColor: getconvergedgraphcolor(),
        }
      ]
    };

    
    let chart = new Chart(document.getElementById(&#34;SIR&#34;), {
      type: &#39;line&#39;,
      data: data,
      options: {
        scales: {
          x: {
            grid: {
              color: getframecolor,
            },
            ticks: { color: gettextcolor }
          },
          y: {
            beginAtZero: true,
            min:  0 ,
            max:  0.35 ,
            grid: {
              color: getframecolor,
            },
            ticks: { color: gettextcolor }
          },
        },
        elements: {
          point:{
              radius: 0
          }
        },

        color: getframecolor,
      }
    });

    
    var slider = document.getElementById(&#34;slider_id&#34; + &#34;SIR&#34;);
    var sliderlabel = document.getElementById(&#34;sliderlabel&#34; + &#34;SIR&#34;);

    slider_labels = parsesliderlabels(&#34;1 Proposal Samples in SIR;2 Proposal Samples in SIR;4 Proposal Samples in SIR;16 Proposal Samples in SIR;\u0026#8734 Proposal Samples in SIR&#34;);
    slider.max = slider_labels.length - 1;
    sliderlabel.innerHTML = slider_labels[0];

    
    document.getElementById(&#39;dark-mode-toggle&#39;).addEventListener(&#39;click&#39;, () =&gt; {
        isdarkmode = !body.classList.contains(&#34;colorscheme-dark&#34;)

        chart.data.datasets[0].borderColor = getgraphcolor();
        chart.data.datasets[0].backgroundColor = getgraphcolor();

        chart.data.datasets[1].borderColor = getconvergedgraphcolor();
        chart.data.datasets[1].backgroundColor = getconvergedgraphcolor();

        chart.update()

        sliderlabel.style.color = gettextcolor()
    });

    slider.oninput = function() {
      chart.data.datasets[0].data = graphs[this.value];
      chart.data.datasets[1].data = converged_graph;
      chart.update();

      sliderlabel.innerHTML = slider_labels[this.value]
    };

    $( document ).ready(function() {
      isdarkmode = (document.getElementsByClassName(&#34;colorscheme-dark&#34;).length === 1);
      chart.update();
    });
  })()

&lt;/script&gt;


&lt;p&gt;As we can see from the above graph, starting from uniform sampling, when the proposal sample count goes to 16, the SIR PDF is fairly close to what we want. Of course, this is simply a very trivial PDF, which we can even use inversion sampling to draw samples from. In other cases, things could be a lot more complicated. It could totally be possible a way larger set of proposal samples is needed for a good approximation of sampling the target distribution.&lt;/p&gt;
&lt;p&gt;There are some pretty attractive characteristics of the sampling method&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike inversion sampling, which is one of the most widely used methods, we do not need any knowledge about the target distribution, apart from that we must be able to evaluate it given a sample. This alone gives us a lot of flexibility in choosing the target PDF.&lt;/li&gt;
&lt;li&gt;What makes the algorithm even more attractive is that we can even add a constant scaling factor on the target PDF as the algorithm would be the same. A relative scaling factor on all weights would mean no scaling factor applied at all. To explain it a bit, imagine we have two proposal samples with their corresponding weights being 2.0 and 8.0. The chance of picking the first sample are 20%. If we scale the target distribution by 5.0, the weights will be 10.0 and 40.0, resulting in exactly the same chances of picking each sample as well.&lt;br&gt;
This offers an extra level of freedom for us to choose our target PDF as a lot of the time we can&amp;rsquo;t really normalize our target PDF. This opens the door for us to choose literally any function we would like to use, rather than a PDF that has to integrate to 1 in its domain.&lt;br&gt;
To be more precise, I would call this target distribution from now on as it is not a traditional sense PDF that we used to know. Strictly speaking, it is not really a PDF, it is a distribution function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With all these limitations gone, it is not hard to imagine the value of SIR. It offers us a powerful tool to draw samples from distributions that we couldn&amp;rsquo;t sample with other methods, for example, the rendering equation itself. If we can take samples from a PDF that is proportional to $L(w_i) f_r(w_i, w_o) cos(\theta_i)$, we really just need one single sample to solve the rendering equation without any noise. This is really hard for traditional methods, like inversion sampling and rejection sampling. Because the distribution is not even a PDF, there is no easy way to normalize the distribution. Normalizing the distribution requires us to evaluate the integral in the first place, this is a chicken and egg problem itself. However, just because we can&amp;rsquo;t draw samples from this distribution doesn&amp;rsquo;t prevent us from approximating drawing a sample from it using SIR. This is exactly what ReStir GI does.&lt;/p&gt;
&lt;p&gt;With these limitations gone, it seems that we have the power to sample from any distribution now. However, there are clearly some downsides of this algorithm as well&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The biggest downside of this SIR method is that it is unfortunately untrackable. To be more specific, it means that we don&amp;rsquo;t really have a closed form of the SIR pdf that we take samples from given a specific M, proposal PDF, and target distribution.&lt;/li&gt;
&lt;li&gt;There is a big issue because of the untrackable nature of the SIR PDF. Given a random sample taken from somewhere else, we can&amp;rsquo;t easily know the probability of taking this given sample with this SIR method.&lt;br&gt;
To illustrate why it matters, if we take a re-visit of the previous MIS balance heuristic weight mentioned above. We can quickly realize that balance heuristic is not possible as long as one single PDF is a SIR pdf, meaning we take one single sample with this SIR method as the samples in MIS, balance heuristic is not an option anymore.&lt;/li&gt;
&lt;li&gt;Sometimes it would require a big number of proposal samples to achieve a good approximation of sampling the target distribution. This could be a deal breaker as generating a huge number of proposal samples doesn&amp;rsquo;t sound cheap itself. ReStir solves this problem by reusing sample signals from temporal history and neighbors, which greatly reduces the number of real proposal samples needed to be taken per-pixel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;resampled-importance-sampling-ris&#34;&gt;Resampled Importance Sampling (RIS)&lt;/h2&gt;
&lt;p&gt;RIS is a technique that takes advantage of SIR to approximate the integral in an unbiased manner.
It comes with a complete solution to approximate the integral without any bias, while SIR only cares about drawing a sample.&lt;/p&gt;
&lt;p&gt;To evaluate $\int f(x)dx$, RIS works as followings&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find a proposal pdf p(x) that is easy to draw samples from and draw M samples using this PDF.&lt;/li&gt;
&lt;li&gt;Find a target distribution $\hat{p}(x)$ that suits the function well&lt;/li&gt;
&lt;li&gt;For each proposal sample, assign a weight for it, $w(x)=\hat{p}(x)/(p(x) M)$&lt;/li&gt;
&lt;li&gt;Draw N samples from the proposal sample set with replacement, the probability of each proposal sample being taken should be proportional to its weight. Replacement means that the N samples taken out of the proposal sample set could be duplicated. This simplifies the problem into taking one sample based on the weights and repeating the process N times.&lt;/li&gt;
&lt;li&gt;Then we use the N samples to evaluate the integral approximation, but with a bias correction factor, which is $\Sigma_{i=1}^{M} w_i(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please be noted that the weight here is slightly different from the previously mentioned weight. This is fine as scaling the target distribution is allowed in both SIR and RIS. But the below function has to skip the 1/M part since it is merged into the weights already.&lt;/p&gt;
&lt;p&gt;The final form of our Monte Carlo estimator would be as below&lt;/p&gt;

$$I_{RIS} = \dfrac{1}{N} \Sigma_{i=1}^{N} \dfrac{f(x_i)}{\hat{p}(x_i)} \Sigma_{j=1}^{M} w(x_j)$$
&lt;p&gt;Here $x_i$ is only a subset of $x_j$. The way $x_i$ is picked can either be done through a binary search or linear processing like Reservoir algorithm. Binary search would require us to pre-process the proposal samples beforehand, which is not practical in the context of ReStir though.&lt;/p&gt;
&lt;p&gt;There are some requirements for this method as well&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M and N have to be both larger than 0.&lt;br&gt;
It is quite easy to understand this. If either the proposal sample count or the target sample count is zero, we are not really doing anything here.&lt;/li&gt;
&lt;li&gt;In the domain of f(x), both $\hat{p}(x)$ and p(x) have to be larger than zero as well.&lt;br&gt;
This is not hard to understand as well. If either of the PDFs is zero in a subdomain, what happens is that the SIR method would have no chance to pick samples from that domain at all. And this is precisely the blind spot issue mentioned above in the Importance Sampling section.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To Make it a bit easier to understand the workflow below is a graph demonstrating how it works
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/RIS.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/RIS.png&#34; width=&#34;500&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Please notice that the SIR algorithm only generates one target sample and feeds it into the RIS estimator.
The theory explained in this blog post would also work for cases where multiple target samples are sampled. However, since the original paper only uses one target sample in its explanation, I&amp;rsquo;d like to stick to its implementation for making things simpler to understand.&lt;/p&gt;
&lt;h3 id=&#34;mis-used-with-ris&#34;&gt;MIS used with RIS&lt;/h3&gt;
&lt;p&gt;Whenever it comes to multiple importance sampling, there are two ways that it could be used with RIS.&lt;/p&gt;
&lt;p&gt;The first option is when taking proposal samples from the proposal PDFs, there is really nothing that prevents us from drawing samples from different PDFs. In the previous section, we mentioned that all the proposal samples are coming from a same proposal PDF. There is no such limitation. We could draw samples from multiple different PDFs if we want to. Though, it is critically important to make sure the conditions of the MIS are met properly.&lt;/p&gt;
&lt;p&gt;As pointed out in the &lt;a href=&#34;https://research.nvidia.com/publication/2022-07_generalized-resampled-importance-sampling-foundations-restir&#34;&gt;GRIS paper&lt;/a&gt;, the weight for each proposal pdf could be generalized to this form&lt;/p&gt;

$$w_i(x) = w_{i,mis}(x) \hat{p}(x) / p_i(x)$$
&lt;p&gt;It is not hard to realize that the previous weight is nothing but a special form of this generalized form where the MIS weight is simply uniform weight and the proposal PDF is all the same.
Since that weight is a MIS weight, it has to make sure it would meet the two conditions involving weight in the MIS algorithm. Otherwise, it could introduce bias.&lt;/p&gt;
&lt;p&gt;Below is a graph demonstrating the workflow for MIS used in RIS.
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/MIS_RIS.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/MIS_RIS.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Another usage of MIS with RIS is to treat SIR as an individual sampling method itself.
So target samples drawn from different SIR methods could be used as different samples drawn from different PDFs in MIS. Put in another way, we essentially treat different SIRs as different PDFs in MIS. From the perspective of MIS estimator, it doesn&amp;rsquo;t really care about whether the sample is drawn from one SIR or another, or maybe even from a different method, like inversion sampling.
However, there is one tricky thing to point out, as balance heuristics would require evaluating samples on all PDFs, since SIR&amp;rsquo;s PDF is not even trackable, it would be difficult to use balance heuristics as weight.&lt;/p&gt;
&lt;p&gt;Same as before, here is a diagram showing how we can treat RIS as a regular sampling method in MIS.
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/RIS_MIS.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/RIS_MIS.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;math-proof-of-ris&#34;&gt;Math Proof of RIS&lt;/h3&gt;
&lt;p&gt;For those readers who don&amp;rsquo;t like taking things for granted, to fully understand the RIS method, it is necessary to understand why it is unbiased.
There is very clear proof of it mentioned in the appendix of &lt;a href=&#34;https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1662&amp;amp;context=etd&#34;&gt;Talbot&amp;rsquo;s paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Rather than repeating his derivation, I would like to put down another slightly different proof, which is similar to what the ReStir DI paper does when it talks about the biasness of the ReStir method.&lt;/p&gt;
&lt;p&gt;Since the original samples come from the proposal PDF(s), target samples are merely a subset of proposal samples without introducing new signals to the evaluation, we would only need to integrate the domain of the proposal samples to get the approximation.&lt;/p&gt;
&lt;p&gt;To simplify our derivation, let&amp;rsquo;s define a few terms first&lt;/p&gt;

$$w_i(x) = w_{i,mis}(x) \hat{p}(x) / p_i(x)$$

$$W = \Sigma_{i=1}^{M}w_i(x_i)$$
&lt;p&gt;Please notice that without losing generosity, I used different proposal PDFs here. To prove the RIS with same proposal PDF, we simply need to get rid of the subscript of $w_i(x)$.&lt;/p&gt;

$$ \begin{array} {lcl} E(\dfrac{f(x)}{\hat{p}(x)} \Sigma_{i=1}^{M}\dfrac{w_{i,mis}(x_i)\hat{p}(x)}{p_i(x)}) &amp;amp; = &amp;amp; E(\dfrac{f(x)}{\hat{p}(x)} W) \\\\ &amp;amp; = &amp;amp; \Sigma_{k=1}^{M} \underset{\text{M}}{\int \dots \int} \dfrac{f(x_k)}{\hat{p}(x_k)} W \dfrac{w_k(x_k)}{W} \Pi_{i=1}^{M} (p_i(x_i)) \space dx_1\ \dots dx_M\ \\\\ &amp;amp; = &amp;amp; \Sigma_{k=1}^{M} \underset{\text{M}}{\int \dots \int} f(x) w_{k,mis}(x_k) \Pi_{i=1, i!=k}^{M} (p_i(x_i)) \space dx_1\ \dots dx_M\ \\\\ &amp;amp; = &amp;amp; \Sigma_{k=1}^{M} \int f(x_k) w_{k,mis}(x_k) dx_k\ \underset{\text{M-1}}{\int \dots \int} \Pi_{i=1, i!=k}^{M} (p_i(x_i)) \space dx_1\ \dots dx_M\ \\\\ &amp;amp; = &amp;amp; \int \Sigma_{k=1}^{M} w_{k,mis}(x) f(x) dx\ \\\\ &amp;amp; = &amp;amp; \int f(x) dx\ \end{array}$$
&lt;p&gt;As we can see from the above derivation, lots of the items cancel, leaving a very simple form, the exact result that we would like to approximate in the first place. With this proof, we can easily see that it doesn&amp;rsquo;t matter if RIS picks proposal samples from different PDFs, this is exactly how MIS works in RIS in the first method I mentioned above.&lt;/p&gt;
&lt;h3 id=&#34;nice-properties-of-ris&#34;&gt;Nice Properties of RIS&lt;/h3&gt;
&lt;p&gt;Before moving to the next section, let&amp;rsquo;s summarize some of the nice properties of RIS.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There isn&amp;rsquo;t that much needed between the proposal PDF and target distribution, offering a lot of flexibility in terms of choosing the best PDF/distribution for your application. The only thing that is required is that they have to both cover the domain of target function f(x). To be more specific, if we are sampling from different proposal PDFs, the union of the domains of all proposal PDFs would need to cover the whole target function domain. If one of the proposal PDFs doesn&amp;rsquo;t cover the full target function domain, it is fine as long as the MIS weight is properly adjusted.&lt;/li&gt;
&lt;li&gt;This was mentioned in the previous section. I would like to emphasize it again since it is something important. The target distribution used in RIS doesn&amp;rsquo;t need to be a normalized PDF at all.  It can literally be anything. As a matter of fact, ReStir DI algorithm uses the unshadowed light contribution, $L(w_i)f_r(w_o,w_i)cos(\theta_i)$, as target function. With traditional sampling methods, it is very hard as this distribution is hardly a PDF itself.&lt;br&gt;
To see how it works in math, in the previous derivation, we can easily notice that the value of $W/\hat{p}(x)$ does not change when we scale target distribution since it is both in the denominator and nominator.&lt;/li&gt;
&lt;li&gt;As we mentioned before, the domains of the target distribution and proposal PDF both cover the domain of f(x). We can take advantage of this fact to predict if the SIR PDF is zero by inspecting target distribution and proposal PDF even if SIR PDF is untrackable. This is a useful feature that we would take advantage to counter the bias in ReStir DI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;weighted-reservoir-sampling-wrs&#34;&gt;Weighted Reservoir Sampling (WRS)&lt;/h2&gt;
&lt;p&gt;The second step of SIR algorithm requires us to pick a sample from all proposal samples and each sample&amp;rsquo;s probability of being picked is proportional to its weight. This application easily reminds us of a binary search algorithm with a prefix sum table. However, even if the time complexity is only O(lg(N)), the preprocessing step&amp;rsquo;s time complexity is still O(N). It would be suitable for cases where data is more static and can be preprocessed and used multiple times later, like HDRI sky importance sampling. In a more dynamic environment, it is less practical as the bottleneck will be O(N) during preprocessing. And it also requires O(N) space as well, which is another deal breaker for efficient SIR sampling. All these issues require us to seek an alternative solution, which is where WRS chimes in.&lt;/p&gt;
&lt;p&gt;Weighted reservoir sampling is an elegant algorithm that draws a sample from a stream of samples without pre-existed knowledge about how many samples are coming. Each sample in the stream would stand a chance that is proportional to its weight to be picked as the winner. For example, if we have three samples A, B, C, with their corresponding weight as 1.0, 2.0, 5.0, there will be 12.5% chance that A will be picked, 25% chance that B will be picked and 62.5% chance that C will be picked. Besides, if there is a fourth sample D coming even if after the WRS is executed, the algorithm is flexible enough to take the fourth sample in without breaking the rule that each sample would have a chance proportional to its weight of being picked.
And what makes it even more attractive is that it only requires O(1) space, rather than linear space. There are certainly more attractive properties of this algorithm, which we would mention at the end of this section after learning how the math works first.&lt;/p&gt;
&lt;p&gt;To be clear, there are certainly variations of WRS existed. For example, the algorithm could actually pick multiple samples rather than just one of them among the set. However, this is the only version we care about in ReStir DI as ReStir DI requires sampling proposal sample set with replacement. So all we need to do is to repeat the above algorithm multiple times. Though, there may be a more optimized algorithm existing to pick samples with replacement. But this is out of the scope of the topic in this post.&lt;/p&gt;
&lt;p&gt;In order to implement the algorithm, we would need to define a reservoir structure with a few necessary fields&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current picked sample&lt;/li&gt;
&lt;li&gt;Total processed weight&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An empty reservoir would have none as the current picked sample and 0 as total processed weight.&lt;/p&gt;
&lt;p&gt;Please be noted that these are the only two fields required for WRS algorithm. ReStir DI does require another field, which is the number of samples processed so far. But since this section is all about WRS, the processed sample count is ignored to avoid confusion.&lt;/p&gt;
&lt;p&gt;The basic workflow of WRS goes like this. For an incoming sample, it would either take the sample or ignore the sample depending on a random choice. A random number between 0 and 1 is drawn. If it is larger than a threshold, which is defined this way&lt;/p&gt;

$$threshold = W_{new} / (W_{new} &amp;#43; W_{total})$$
&lt;p&gt;The incoming sample will be ignored. Otherwise, the new sample will be picked as the winner replacing the previously picked sample, if existed. Regardless, the total weight will be updated accordingly. A side note here, pay special attention when your random number equals the threshold. This could be problematic depending on how we would like to deal with the incoming sample when it is equal.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we would like to pick it when it is equal, there is a non-zero chance to pick incoming samples with zero weight.&lt;/li&gt;
&lt;li&gt;If we would like to ignore it when it is equal, there is a non-zero chance that we would ignore the first sample, which should have always been picked. Of course, if the first sample has zero weight, it is even trickier.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How to deal with this sort of corner case is left to the readers as an exercise as this is not exactly what this post is about.
But be sure to deal with this situation when implementing the algorithm to avoid weight bugs.&lt;/p&gt;
&lt;h3 id=&#34;mathematics-proof&#34;&gt;Mathematics Proof&lt;/h3&gt;
&lt;p&gt;Compare with the previous math proof, this one is a lot simpler and requires nothing but junior high math.&lt;/p&gt;
&lt;p&gt;To prove how it works, let&amp;rsquo;s start with the first incoming sample with non-zero weight in the WRS algorithm. We don&amp;rsquo;t care about the first few samples with zero weight, we can pretend they don&amp;rsquo;t exist.
What will happen for sure is that the incoming sample will be taken as the random number will not be larger than the threshold, which is 1.0, forcing the new sample to be picked. And this indeed matches our expectation as if we only have one sample, the probability of this sample being picked is 100%.&lt;/p&gt;
&lt;p&gt;So assuming the algorithm works after processing N incoming samples, this is to assume that after processing N samples, each visited sample stands a chance of being picked that is precisely this&lt;/p&gt;

$$p(x_k) = W(x_k) / \Sigma_{i=1}^{N} W(x_i)$$
&lt;p&gt;As the way WRS works, we do have the denominator nicely recorded in a single variable&lt;/p&gt;

$$W_{total} = \Sigma_{i=1}^{N} W(x_i)$$
&lt;p&gt;Let&amp;rsquo;s see what would happen when a new sample comes in.
If the random number is smaller than the threshold mentioned above, the newcomer will be picked.
This means that the probability of the new sample being picked is&lt;/p&gt;

$$p(x_N) = \dfrac{W(x_{N&amp;#43;1})}{W_{total} &amp;#43; W(x_{N&amp;#43;1})} = \dfrac{W(x_{N&amp;#43;1})}{\Sigma_{i=1}^{N&amp;#43;1} W(x_i)}$$
&lt;p&gt;And this probability is exactly what we want for the new sample to be picked. And there is also a chance that the new sample will not be picked, which is&lt;/p&gt;

$$1-p(x_N) = \dfrac{\Sigma_{i=1}^{N} W(x_i)}{\Sigma_{i=1}^{N&amp;#43;1} W(x_i)}$$
&lt;p&gt;So each of the previous samples being picked is reduced by the risk of the new sample could be picked. And the probability goes down by exactly the above factor, leaving each of the previous samples standing a chance of this being picked,&lt;/p&gt;

$$p&amp;#39;(x_k) = p(x_k) \dfrac{\Sigma_{i=1}^{N} W(x_i)}{\Sigma_{i=1}^{N&amp;#43;1} W(x_i)} = \dfrac{W(x_k)}{\Sigma_{i=1}^{N} W(x_i)} \dfrac{\Sigma_{i=1}^{N} W(x_i)}{\Sigma_{i=1}^{N&amp;#43;1} W(x_i)} = \dfrac{W(x_k)}{\Sigma_{i=1}^{N&amp;#43;1} W(x_i)}$$
&lt;p&gt;And this probability again is also what we want for each of the old samples to hold to be picked.&lt;/p&gt;
&lt;p&gt;At this point, we can easily see that if WRS works for N incoming samples, it would work for N + 1 samples as well. Since we also proved that it indeed works when N is 1, it would mean by thinking it recursively, it would work for any number of incoming samples.&lt;/p&gt;
&lt;h3 id=&#34;treating-a-new-sample-as-a-reservoir&#34;&gt;Treating a New Sample as a Reservoir&lt;/h3&gt;
&lt;p&gt;As we previously mentioned, the reservoir structure really only needs to hold two members, the total processed weight and the picked sample. And if we think about each new sample, it really just has a weight. However, we can also regard a new sample as a mini reservoir with the sample itself picked and the total weight equals the sample weight.
From this perspective, there is no individual sample any more, everything is a reservoir. And the WRS algorithm would simply use the tracked reservoir and takes a new single sample reservoir.&lt;/p&gt;
&lt;p&gt;So the question is, can WRS also take reservoirs generated by other WRS execution, rather than just a single sample reservoir?&lt;/p&gt;
&lt;p&gt;Fortunately, the answer is yes.
Let&amp;rsquo;s say the WRS has processed N samples so far and it takes an incoming reservoir as its next input.
If the incoming reservoir has processed M samples first, each sample in the visited M samples would stand a chance of this of being picked&lt;/p&gt;

$$p(y_k) = \dfrac{w(y_k)}{\Sigma_{i=1}^{M} w(y_i)} $$
&lt;p&gt;I purposely used y here to indicate it is a different source of samples. Dropping it in the current reservoir processing, we would have each of the previously visited samples having this chance to be picked&lt;/p&gt;

$$p(y_k) = \dfrac{w(y_k)}{\Sigma_{i=1}^{M} w(y_i)} * \dfrac{\Sigma_{i=1}^{M} w(y_i)}{\Sigma_{i=1}^{N} w(x_i) &amp;#43; \Sigma_{i=1}^{M} w(y_i)} = \dfrac{w(y_k)}{\Sigma_{i=1}^{N} w(x_i) &amp;#43; \Sigma_{i=1}^{M} w(y_i)}$$
&lt;p&gt;So after being processed by the WRS algorithm, each of the previously visited M samples would have the correct probability to be picked as a whole. Even though this is done in two passes, rather than the previously mentioned one-pass algorithm.&lt;/p&gt;
&lt;p&gt;W.r.t the visited N samples, their probability of being picked is also reduced by this factor&lt;/p&gt;

$$\dfrac{\Sigma_{i=1}^{N} w(x_i)}{\Sigma_{i=1}^{N} w(x_i) &amp;#43; \Sigma_{i=1}^{M} w(y_i)}$$
&lt;p&gt;Applying this to the existing probability, we will have each visited sample having this chance of being picked&lt;/p&gt;

$$p&amp;#39;(x_k) = \dfrac{w(x_k)}{\Sigma_{i=1}^{N} w(x_i)} \dfrac{\Sigma_{i=1}^{N} w(x_i)}{\Sigma_{i=1}^{N} w(x_i) &amp;#43; \Sigma_{i=1}^{M} w(y_i)} = \dfrac{w(x_k)}{\Sigma_{i=1}^{N} w(x_i) &amp;#43; \Sigma_{i=1}^{M} w(y_i)}$$
&lt;p&gt;This nicely matches our expectations as well. So at this point, we can safely draw the conclusion that WRS can be performed in multiple recursive passes, it won&amp;rsquo;t have any impact on the probability of each sample being picked. It is also not hard to see that this recursive process certainly doesn&amp;rsquo;t get limited by two levels.&lt;/p&gt;
&lt;h3 id=&#34;nice-properties-of-wrs&#34;&gt;Nice Properties of WRS&lt;/h3&gt;
&lt;p&gt;Now we know the math behind this algorithm as well, it is time to summarize what properties we can take advantage later&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WRS doesn&amp;rsquo;t need to know the number of samples during its execution. It takes a stream of inputs, rather than a fixed array of inputs.&lt;/li&gt;
&lt;li&gt;WRS has O(1) space requirement even if the processed samples could be millions. It also requires only O(1) time to process each input, even if the input is a reservoir with millions of samples processed.&lt;/li&gt;
&lt;li&gt;WRS supports divide and concur. Imagine there is a source of a million samples, we can first divide them into multiple trunks and process each trunk on a different CPU thread. The order of each trunk being processed is irrelevant. After every thread is finished, we can resolve the result with a final gather pass. How and when we divide won&amp;rsquo;t have any impact on the probabilities at all.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;restir-di-introduction&#34;&gt;ReStir DI Introduction&lt;/h1&gt;
&lt;p&gt;With all the above context explained, we are now in a good position to move forward to discuss the detailed choices made by ReStir DI.&lt;/p&gt;
&lt;p&gt;Before doing it, I would like to give a brief introduction of what ReStir DI is and what problem it tries to tackle. It is highly recommended to read the original ReStir paper to get all the details.&lt;/p&gt;
&lt;h2 id=&#34;targeted-problem&#34;&gt;Targeted Problem&lt;/h2&gt;
&lt;p&gt;The original ReStir DI algorithm focuses on tackling the many light problem. More specifically, whenever an intersection is found and shading operation is needed, we would first need to pick a light among all light candidates to take samples from. There are several obvious options for toy renderers,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The most straightforward algorithm to pick a light is to randomly pick one light with equal probability for each light. Even though it doesn&amp;rsquo;t sound very promising, it can be done in O(1) time and requires almost no pre-processing.&lt;/li&gt;
&lt;li&gt;We could also construct weights by taking some factors into account when picking lights, like the emissive power of the light. As long as those factors are not shading point dependent, we could pre-calculate a table first and use binary search to find a light proporition to the weight. Pbrt 2nd uses this method for sampling a light.&lt;/li&gt;
&lt;li&gt;If the weights are shading point dependent, it commonly means that we would need to iterate through all lights to get a good sample. However, this is clearly a deal breaker for setups with millions of lights, it wouldn&amp;rsquo;t even work for hundreds of lights practically since iterating through all lights is a per-pixel operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, the above methods are the most trivial and fundamental solutions available. There is some existing work exploring different ideas in this domain &lt;sup&gt;&lt;a href=&#34;https://research.nvidia.com/publication/2019-07_dynamic-many-light-sampling-real-time-ray-tracing&#34;&gt;[12]&lt;/a&gt; &lt;a href=&#34;https://fpsunflower.github.io/ckulla/data/many-lights-hpg2018.pdf&#34;&gt;[13]&lt;/a&gt; &lt;a href=&#34;http://cwyman.org/papers/rtg2-manyLightReGIR.pdf&#34;&gt;[14]&lt;/a&gt;&lt;/sup&gt;
, etc. Among all of the existing work, ReStir DI scales really well, up to millions of lights. And it is also quite practical to implement on modern graphics hardware, especially Nvidia RTX cards. Strictly speaking, ReStir DI not only finds a good light candidate for shading, but also finds a good light sample. There is still subtle differences between sampling a light and sampling a light sample.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The former only cares about picking a light out of many, which would still require us to pick a sample on this light later. And in some situations, it could be quite tricky. For example, when the light is a huge area light and the brdf is very spicky there could be risk in producing lots of fireflies. Some readers may instantly consider MIS for such a case. However, MIS works well only with balanced heuristics or anything similar, which commonly requires all PDFs to be trackable. If for some reason, the way we pick light candidate among all lights is not even trackable, there is no easy way to properly evaluate heuristics, leaving us no choice but to use uniform weight, which would have a huge negative impact on convergence rate.&lt;/li&gt;
&lt;li&gt;On the other hand, sampling a light sample offers enough signal for the rest of the algorithm to simply take over. Since ReStir does pick quality sample in an efficient manner, the aforementioned problem simply doesn&amp;rsquo;t exist in it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basic-algorithm-workflow&#34;&gt;Basic Algorithm Workflow&lt;/h2&gt;
&lt;p&gt;The algorithm works for rendering 2D images. It works so efficiently that it could render in real time. Each pixel would keep track of its own reservoir across frames.&lt;/p&gt;
&lt;p&gt;Below is a very brief introduction of the workflow&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For every pixel on the screen, it first takes multiple initial light samples (M = 32 in their implementations). These light samples are treated as the proposal samples, which would later be used in RIS algorithm.&lt;br&gt;
In order to take the proposal samples, ReStir DI first pick a light randomly with the probability of each light being picked proportional to its emissive power. If there is HDRI sky, a 25% probability is given to it by default. As mentioned before, since the probability is proportional to its weight not the shading point, the algorithm could use the traditional binary search method for picking a light. And once the light is picked, a corresponding method is used for generating a random light sample on the light source based on whether it is a HDRI light or an area light.&lt;br&gt;
All of the proposal samples will be processed in RIS algorithm through weighted reservoir sampling. The target distribution is unshadowed light contribution. Mathematically, it is $L(w_i,w_o) f_r(w_o,w_i) cos(\theta_i)$, except that we don&amp;rsquo;t know if the light is visible to the shaded point or not.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An optional visibility reuse phase could be done here. This is introduced because even with unlimited number of proposal samples, the result could still be noisy due to the gap between the target distribution and the rendering equation, specificially, the visibility function. By introducing a visibility check here, we could use the real rendering equation (the integral part) as our target distribution, resulting in much less noise caused by visibility issues. If this algorithm is used, the algorithm would need to check visibility during neighbor target sample merging as well to preserve its unbiasness.&lt;br&gt;
This is an optional phase as it could introduce big performance impact due to a larger set of rays shot each pixel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second phase in the algorithm is temporal reuse. With some motion vector, the algorithm could find itself in the history of rendering the previous frames. And it combines the reservoir that belongs to the corresponding pixel in the previous frame. Just like TAA algorithm that improves the image quality by distributing cost of super sampling across multiple frames, the temporal reuse does exactly the same thing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the third phase, the algorithm could repeat in multiple iterations. In each iteration, it would try to find a random neighbor and combine its reservoir as well.&lt;br&gt;
This is where the number of &lt;strong&gt;indirectly visited proposal samples&lt;/strong&gt; for each pixel will soar. Imagine there is n iterations, k neighbors are picked. To make things simpler, let&amp;rsquo;s consider only the first frame. Each pixel after this phase would see $(k + 1) * M$ visited proposal samples. With multiple iterations, the theoretical maximum number of visited samples could be as high as $(k + 1)^n * M$. However, this is only at the cost of O(nk + M).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Since there is quite a good chance that visited proposal samples would be revisited indirectly again, it would be hard to reach to the theoretical maximum for this number. And this is why after several iterations, we get diminishing return in terms of quality.&lt;/li&gt;
&lt;li&gt;Please notice that I mentioned &amp;lsquo;indirectly visited proposal sample&amp;rsquo;. One important detail I&amp;rsquo;d like to point out is that this is by no means to say your current pixel&amp;rsquo;s RIS would visit this number of proposal samples, it actually &amp;lsquo;visited&amp;rsquo; them in a recursive manner indirectly. Quality-wise, it could be something similar, but not exactly the same thing. We&amp;rsquo;ll explain this later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is just about the first frame as the temporal reuse doesn&amp;rsquo;t bring anything useful due to the lack of history. But for later frames, temporal reuse will introduce way more signal only at O(1) time. With temporal reuse, it gains a second chance to make the number grow even faster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With all the three phases, the pixel now has a winner light sample, which survived among potentially millions of samples, easily making it a high-quality light sample. And the algorithm then uses RIS method to evaluate the light contribution here. It is only in here that a shadow ray is traced so the cost of the algorithm is totally within the capability of the modern hardware.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This same process is repeated every frame. As frame goes, each pixel essentially visits millions of proposal samples in their RIS algorithm eventually resulting in drawing samples quite close to the target distribution, the unshadowed light contribution.&lt;/p&gt;
&lt;p&gt;The above explanation really serves as a review of the algorithm to readers who read the paper. It could be a bit confusing for people who don&amp;rsquo;t know the algorithm.&lt;/p&gt;
&lt;h1 id=&#34;common-confusion-in-understanding-restir&#34;&gt;Common Confusion in Understanding ReStir&lt;/h1&gt;
&lt;p&gt;After my first time reading through the paper, I had a lot of confusion.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The math in the paper assumes the proposal PDFs are all the same. Even if the PDF for taking light candidate is exactly the same for all pixels&amp;rsquo; RIS execution, the PDF of taking initial light samples is the product of the PDF of picking a light candidate and the PDF of taking a light sample from the light candidate, the latter of which is not the same for all pixels. So does the implementation really uses one single PDF to take light samples?&lt;/li&gt;
&lt;li&gt;I had a really hard time to be convinced this is unbiased, especially the sampling neighbor part since the neighbor pixel&amp;rsquo;s RIS algorithm essentially uses a different target distribution than the current pixel. The RIS algorithm we derived above shows no sign of supporting multiple target distributions at all.&lt;/li&gt;
&lt;li&gt;The paper has a very clear explanation and proof that the expectation of the big W (the sum of all weights divided by target distribution and the number of visited proposal samples) is the reciprocal of the SIR PDF. However, it was not obvious to me why this would allow us to replace the reciprocal of SIR PDF in the math directly without introducing bias.&lt;/li&gt;
&lt;li&gt;We all know temporal AA is by far not an unbiased algorithm, even if it became an industry standard for AAA games. Why doesn&amp;rsquo;t the temporal reuse introduce any bias in this algorithm?&lt;/li&gt;
&lt;li&gt;From a mathematic stand point, what exactly is visibility reuse? It seems that it uses the shadowed light contribution as the target distribution. The problem is that it still uses unshadowed light contribution to pick a winner sample from all proposal light samples, while it uses the shadowed light contribution, a different target distribution, for picking the finalized target sample among the winner light sample and neighbor target samples in RIS algorithm. This, again, seems breaking the assumption made by RIS, which is only one single target distribution is allowed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Above are just some examples of my early confusion, it doesn&amp;rsquo;t cover everything.
With lots of confusion like these, I spend quite some time researching related materials and eventually got fully convinced that this algorithm is indeed unbiased. The following sections of this blog will be explaining my confusion, I hope this could be helpful to those who have similar confusion as well.&lt;/p&gt;
&lt;h2 id=&#34;same-light-proposal-pdf&#34;&gt;Same Light Proposal PDF?&lt;/h2&gt;
&lt;p&gt;It is a bit complicated when it comes to the proposal PDF used in ReStir. We would have to go through the following sections before answering this question. However, it is not that difficult to tell whether we are using the same proposal light sample PDF. The short answer is yes, it indeed uses exactly the same PDF even if there could be multiple types of light sources, like triangle area light and sky light mentioned in the paper.&lt;/p&gt;
&lt;p&gt;Even if it is the same PDF used in ReStir DI, it is by no means simply the discretized PDF used to pick a light candidate.&lt;/p&gt;
&lt;p&gt;To understand it, we would have to realize that there are two categories of PDFs involved in this sampling process&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDF L is used to pick a light canidates, this is also a discretized PDF. In the paper&amp;rsquo;s implementation, this is consistent across all pixels&amp;rsquo; execution of RIS algorithm.&lt;/li&gt;
&lt;li&gt;PDF S is used to pick a light sample on the selected light candidate, this is a continuous PDF. It could be different based on light types. It is even different for lights of the same type as the size of the area light could be different or the HDRI could have different radiance distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The finalized PDF is a product of the two&lt;/p&gt;

$$ p(k, s) = p_L(k) p_{S,k}(s) $$
&lt;p&gt;k is the id of the picked light candidate and s is the light sample drawn from this light candidate. Even if $p_{S,k}(s)$ could be different per light, the product of the two remains consistent across all RIS&amp;rsquo; execution. By hiding the light candidate id, which we don&amp;rsquo;t really care about much in ReStir, we can use the product PDF as our light sample PDF. And this is already consistent.&lt;/p&gt;
&lt;p&gt;If this is not obvious enough, let&amp;rsquo;s find an example that we are already very familiar with it, the sky importance sampling.
The method that Pbrt choose is quite a similar case to the light sample sampling in ReStir. In order to take a sample from HDRI sky, we pre-calculate CDFs, each row gets a CDF by accumulating its pixels&amp;rsquo; value in it, and there is one more CDF generated by accumulating the sum of pixel values on each row. In order to take a sample, we first randomly choose a row by binary searching the CDF for all rows. Once a row is picked, its corresponding CDF is used for picking the final sample. The second CDF used is clearly different across different rows, just like our light sample PDF could be different based on the exact instance of light.&lt;/p&gt;
&lt;p&gt;To relate the two use cases, we can notice that both take samples through a two steps process. In the light sample case, it picks a light candidate first, in the HDRI sky importance sampling case, it picks a row. Once the first phase is done, both would choose to take a finalized sample with a conditional PDF, the only minor difference between the two is that it is a continuous PDF in the case of light sampling while it is discretized in the other application.&lt;/p&gt;
&lt;p&gt;We almost never doubt that the sky importance sampling PDF is different based on row picking. This clearly indicates that the light sample picking in ReStir is also consistent across all pixels as well.&lt;/p&gt;
&lt;p&gt;Luckily, the complexity of the proposal light sample PDF is pretty much independent from the rest of the algorithm.&lt;/p&gt;
&lt;h2 id=&#34;what-exactly-is-neighbor-sampling&#34;&gt;What Exactly is Neighbor Sampling?&lt;/h2&gt;
&lt;p&gt;By neighbor sampling, I mean the process to take samples from its neighbors&amp;rsquo; reservoirs. I think lots of readers would have confusion similar to mine as well. Since the neighbor pixels don&amp;rsquo;t share exactly the same target distribution, there seems no theoretical basis for taking samples from neighbors. The truth is it can take signals from its neighbors and it is also unbiased. In order to understand why, let&amp;rsquo;s draw a graph first&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_1.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_1.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Let&amp;rsquo;s observe two pixels, on the left, there is pixel A and on the right, it is pixel B.
This graph is about pixel A picking its light samples. In the beginning, both A and B would pick their own proposal samples generated by the light PDF. For pixel A, it would pick neighbors for more signals as well. In this case, let&amp;rsquo;s assume it picks one of its neighbors, B. Once B is picked, B&amp;rsquo;s target samples would be fed into A&amp;rsquo;s weighted reservoir sampling algorithm. And for pixel A, it would use its own target distribution to pick one winner sample between the winner sample in the proposal light samples that belong to pixel A and the neighbor&amp;rsquo;s target sample. Eventually, A&amp;rsquo;s finalized target sample will be fed into the RIS estimator for evaluating rendering equation.&lt;/p&gt;
&lt;p&gt;There are a few things I would like to point out in this graph&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For simplicity purposes, this graph is only for the very first frame of rendering with ReStir. Later frames will be more complicated, but they are essentially doing the same thing. It would be a lot easier to understand it once we know how this graph works mathematically.&lt;/li&gt;
&lt;li&gt;Since it is the first frame, there is no temporal reuse as there is no valid history for it to read. Of course, the graph for the whole ReStir DI algorithm would be a bit more complicated.&lt;/li&gt;
&lt;li&gt;Only one neighbor pixel&amp;rsquo;s reservoir is borrowed in this graph. Borrowing multiple neighbor&amp;rsquo;s target samples is not essentially different from this one. And the iteration in the neighbor sampling is skipped as well.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With so much simplification, we can still see the complexity of this graph is still beyond the two graphs we showed before. Specifically, how MIS can be used in RIS. If we take a look at the previous two graphs, we would find that it is not immediately obvious if this is MIS + RIS. In order to simplify things, let&amp;rsquo;s further simplify the graph by taking advantage of the facts of WRS algorithm that were explained before,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The outcome in WRS is independent of the execution order. It doesn&amp;rsquo;t matter which proposal sample comes first and the result would be precisely the same.&lt;/li&gt;
&lt;li&gt;Since divide and concur works in WRS as well, it means we can also choose to combine separate WRS executions as well by going the other way around.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So if we combine the two WRS algorithm execution for pixel A in the previous graph, we would have this new simplified graph then&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_2.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_2.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;By observing this graph, it is still not obvious if this is MIS with RIS. But we can quickly notice one important thing here, the target sample generated by neighbor pixel is essentially a SIR algorithm itself. As previously mentioned, SIR is an algorithm that approximates drawing samples from a target distribution. So if we swap the top right corner graph with a single PDF, which is essentially a SIR PDF, we would have this graph then.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_3.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_3.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Please take a look at the first graph in &lt;a href=&#34;https://agraphicsguynotes.com/posts/understanding_the_math_behind_restir_di/#mis-used-with-ris&#34;&gt;this section&lt;/a&gt; again, I think it should be quite clear that we are essentially just using MIS in RIS. To further explain it, it treats the target sample from its neighbors as a proposal sample here and the proposal PDF for this sample is the SIR PDF from its neighbors. As we mentioned in the previous sections, RIS does allow different proposal PDFs to co-exist together as long as the target distribution is consistent.&lt;/p&gt;
&lt;p&gt;At this point, it is clear how we can answer the question of this section. The behavior of borrowing neighbor pixels&amp;rsquo; target sample is essentially introducing a new PDF that is different than the light sample PDF. And this mathematically is totally fine and introduces no bias at all as &lt;a href=&#34;https://agraphicsguynotes.com/posts/understanding_the_math_behind_restir_di/#math-proof-of-ris&#34;&gt;proved&lt;/a&gt; before. This is precisely why even if neighbor pixels have their own target distributions the algorithm is still unbiased.&lt;/p&gt;
&lt;p&gt;One subtle thing that deserves our attention is that ReStir DI really uses different proposal PDFs even if all light samples are drawn from the same light sample PDF. This is because the target samples from neighbors, which are treated as proposal samples, all come from different SIR PDFs.&lt;/p&gt;
&lt;h2 id=&#34;the-proposal-samples-visited-by-ris&#34;&gt;The Proposal Samples Visited by RIS&lt;/h2&gt;
&lt;p&gt;One common misunderstanding of ReStir DI algorithm is that if a neighbor pixel&amp;rsquo;s reservoir has visited 100 samples, by &amp;lsquo;merging&amp;rsquo; the target sample from it, our current pixel&amp;rsquo;s visited samples count will be grown by 100. This was briefly mentioned before, I&amp;rsquo;d like to explain with a bit more text here.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s keep talking about pixel A and B in the above graph. Except this time, for simplicity, we only pretend that each of them generates two proposal samples. For pixel A, its two proposal samples are $x_0$, $x_1$ and for pixel B, they are $y_0$ and $y_1$.&lt;/p&gt;
&lt;p&gt;In order to count the fact that neighbor pixel uses a different target distribution, the new weight for this target sample of pixel B is&lt;/p&gt;

$$\dfrac{\hat{p_a}(y)}{\hat{p_b}(y)} {r_b}.w_{sum}$$
&lt;p&gt;Assuming that pixel B picks $y_0$ as its target sample. If we drop this weight in the SIR sampling execution in the pixel of A during the neighbor sampling phase, we would easily see the final total weight for the pixel A would be&lt;/p&gt;

$$\dfrac{\hat{p_a}(x_0)}{p(x_0)} &amp;#43; \dfrac{\hat{p_a}(x_1)}{p(x_1)} &amp;#43; \dfrac{\hat{p_a}(y_0)}{p(y_0)} &amp;#43; \dfrac{\hat{p_a}(y_0)}{\hat{p_b}(y_0)}\dfrac{\hat{p_b}(y_1)}{p(y_1)}$$
&lt;p&gt;If pixel A visits the four samples by itself, rather than borrowing from its neighbors, the total weight we should see is&lt;/p&gt;

$$\dfrac{\hat{p_a}(x_0)}{p(x_0)} &amp;#43; \dfrac{\hat{p_a}(x_1)}{p(x_1)} &amp;#43; \dfrac{\hat{p_a}(y_0)}{p(y_0)} &amp;#43; \dfrac{\hat{p_a}(y_1)}{p(y_1)}$$
&lt;p&gt;This mismatch confused me for quite a while. It doesn&amp;rsquo;t look like borrowing from neighbor pixel B&amp;rsquo;s target sample would result in precisely the same math with iterating through B&amp;rsquo;s proposal samples by itself. My understanding of it is that they are not the same.
And please be mindful that I purposely removed the subscript to give us an illusion that all proposal PDFs are the same. Again, even if the light sample PDFs are all consistent across the implementation, the SIR PDF&amp;rsquo;s presence easily breaks this assumption. So in reality, the above math could be slightly more complicated.&lt;/p&gt;
&lt;p&gt;The same theory is hidden in the second graph shown the above section as well. The fact that we used B&amp;rsquo;s target distribution can&amp;rsquo;t be changed by the time A tries to merge B&amp;rsquo;s target sample. And RIS doesn&amp;rsquo;t support different target distribution. This is already proof that there is no mathematical basis that pixel A&amp;rsquo;s reservoir would visit any of the proposal samples of pixel B even if it merges B&amp;rsquo;s target sample.&lt;/p&gt;
&lt;p&gt;Rather than thinking about it that way, I would prefer to think that pixel A would only see its own initial light samples first. For each neighbor sampling, it sees nothing but one single higher quality sample based on a SIR PDF that is a lot closer to the target distribution compared with the light sample proposal PDFs. The more samples visited by neighbor&amp;rsquo;s reservoir, the higher quality its target sample would be. To some degree, we can think about it this way, pixel A visits pixel B&amp;rsquo;s proposal samples indirectly, definitely not in a way that regular proposal samples are visited. Of course, this is just an intuitive explanation.&lt;/p&gt;
&lt;p&gt;To emphasize it, we need to be aware that by merging neighbor&amp;rsquo;s target sample it doesn&amp;rsquo;t give us the mathematical equivalent of visiting neighbor&amp;rsquo;s light proposal samples. But it does offer a higher quality sample than the initial light samples, which means that we would still benefit a lot if neighbor pixels have visited tons of proposal samples.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-mis-weight&#34;&gt;What is the MIS Weight?&lt;/h2&gt;
&lt;p&gt;With the above explanation, I think some readers may have already had a question at this point.
What is the MIS weight in this case then? Can we use balance heuristics at all?&lt;/p&gt;
&lt;p&gt;There are a few things making the problem a bit tricky,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SIR PDF of neighbor&amp;rsquo;s sampling method is not even trackable. This doesn&amp;rsquo;t meet the need for balance heuristics in the first place and is a clear deal breaker.&lt;/li&gt;
&lt;li&gt;Assuming there is N samples to be processed, the time complexity for evaluating balance heuristics for all of them will be $O(N^2)$. This may not sound that crazy at the beginning if we only have 32 initial samples along with a few neighbor target samples. Besides the time complexity, there is also storage requirement that will be O(N) as well.&lt;/li&gt;
&lt;li&gt;And since each balanced heuristic weight would require knowing all proposal PDFs during its evaluation, this would mean that the set of proposal PDFs would be known even during the first samples evaluating its MIS weight, breaking the streaming nature of the ReStir algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this point, it is not hard to imagine why the paper&amp;rsquo;s implementation went for a uniform weight as it can easily be isolated out and doesn&amp;rsquo;t have the problems mentioned above.&lt;/p&gt;
&lt;p&gt;However, uniform weight comes at a cost here. As previously explained, the third condition in &lt;a href=&#34;https://agraphicsguynotes.com/posts/understanding_the_math_behind_restir_di/#multiple-importance-sampling&#34;&gt;MIS&lt;/a&gt; is not met by the uniform weight. And this is precisely the essential root cause of why ReStir DI would introduce bias. The paper does explain why it is biased, but seeing the same problem from another perspective commonly offers us a bit deeper understanding. And the solution the paper proposed does exactly the method explained metioned in the constant weight section before.&lt;/p&gt;
&lt;p&gt;Strictly speaking, there is chances to use balance heuristics by using the target distribution rather than the untrackable SIR PDF. However, even if this tackles the first problem, the two remaining issues could still be a performance bottleneck. Besides, the balanced heuristic commonly requires each PDF to be normalized, while the target distribution may not necessarily be normalized, which will cause no bias, but inefficiency in terms of convergence rate.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-proposal-pdf-for-neighbors-target-sample&#34;&gt;What is the Proposal PDF for Neighbors Target Sample?&lt;/h2&gt;
&lt;p&gt;In order to perform RIS algorithm, we would need to know the proposal PDF for each incoming proposal sample. However, there is a particularly tricky problem, the neighbor pixel&amp;rsquo;s SIR PDF is not trackable. There is no easy way to evaluate what the PDF is.&lt;/p&gt;
&lt;p&gt;Luckily enough, the paper does explain that the big W, which is shown below, takes the role of reciprocal of SIR PDF,&lt;/p&gt;

$$W(y) = \dfrac{1}{M} \dfrac{1}{\hat{p}(y)}\Sigma_{i=1}^{M}\dfrac{\hat{p}(x_i)}{p_i(x_i)}$$
&lt;p&gt;Here $y$ is the lucky winner sample that was eventually picked through WRS.&lt;/p&gt;
&lt;p&gt;To be more mathematical, the expectation of W is precisely the reciprocal of SIR PDF evaluated at the target sample as long as there is no proposal PDF that is zero there. If there is, we can easily avoid bias by adjusting the constant weight as well.&lt;/p&gt;

$$E(W(y)) = \dfrac{1}{p_{ris}(y)}$$
&lt;p&gt;So even if the SIR PDF is untrackable, we can indeed know the unbiased approximation of the reciprocal of the SIR PDF once a target sample is drawn. This is a very important property that allows us to merge target sample without introducing bias.&lt;/p&gt;
&lt;p&gt;There is one question that needs a bit of clarification. Why can we replace the PDF with this just because the expectation is a match?
It is actually not difficult to prove this, let&amp;rsquo;s assume that there are only K - 1 proposal samples coming from the lights, there is also a Kth proposal sample borrowed from a neighbor.&lt;/p&gt;

$$ \begin{array} {lcl} E(I_{ris}) &amp;amp; = &amp;amp; E(\dfrac{1}{N}\Sigma_{i=1}^{N}(\dfrac{1}{M}\dfrac{f(y)}{\hat{p}(y)} (\Sigma_{k=1}^{M-1}\dfrac{\hat{p}(x_k)}{p_k(x_k)} &amp;#43; \hat{p}(x_M) W(x_M)) \\\\ &amp;amp; = &amp;amp; E(\dfrac{1}{M}\dfrac{f(y)}{\hat{p}(y)} (\Sigma_{k=1}^{M-1}\dfrac{\hat{p}(x_k)}{p_i(x_k)} &amp;#43; \hat{p}(x_M) E(W(x_M))) \\\\ &amp;amp; = &amp;amp; E(\dfrac{1}{M}\dfrac{f(y)}{\hat{p}(y)} (\Sigma_{k=1}^{M-1}\dfrac{\hat{p}(x_k)}{p_k(x)} &amp;#43; \dfrac{\hat{p}(x_M)}{p_M(x_M)})) \\\\ &amp;amp; = &amp;amp; E(\dfrac{1}{M}\dfrac{f(y)}{\hat{p}(y)} \Sigma_{k=1}^{M}\dfrac{\hat{p}(x_k)}{p_k(x_k)}) dx\ \\\\ &amp;amp; = &amp;amp; \int f(x) dx\ \end{array}$$
&lt;h2 id=&#34;importance-of-neighbors-target-samples&#34;&gt;Importance of Neighbors&amp;rsquo; Target Samples&lt;/h2&gt;
&lt;p&gt;Careful readers at this point may have already realized that the weight for the neighbor target sample I showed above doesn&amp;rsquo;t match what the paper used, which is $\hat{p}(x) W(x_M) N$.
N is the number of visited proposal samples in the neighbor&amp;rsquo;s reservoir execution, either directly or indirectly.
So where does this N fit in the math?&lt;/p&gt;
&lt;p&gt;To answer the question, let&amp;rsquo;s take a look at another question that has not been answered yet. Assuming there is only one neighbor reservoir sampled just like before. In each frame, for each pixel, there is 32 initial light samples. If the neighbor reservoir has visited N samples so far, it is clearly likely to be a higher quality sample than the light samples if N is relatively big. How does the WRS algorithm know? The truth is it may not know. In order to emphasize the importance of neighbor&amp;rsquo;s target samples, we can choose to feed them into the current pixel N times, with N being the number of proposal samples visited by the neighbor&amp;rsquo;s reservoir directly and indirectly. So a target sample that is picked from a million proposal samples will be a million times more respected than an initial light sample drawn in the first phase.&lt;/p&gt;
&lt;p&gt;Mathematically, the likelihood of picking this sample will clearly be boosted by N times and since we feed it N times, the number of visited proposal samples in this reservoir would need to be bumped by N as well, not 1. Mathematically, we don&amp;rsquo;t really need to feed the neighbor target sample by N times at all, we only need to feed it once, but with the weight scaled by N and bump the visited sample count by N in the current reservoir. This all can be done within O(1) time.&lt;/p&gt;
&lt;p&gt;Below is the algorithm 4 in the original paper,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;&#34;&gt;
&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272&#34;&gt;1
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272&#34;&gt;2
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272&#34;&gt;3
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272&#34;&gt;4
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272&#34;&gt;5
&lt;/span&gt;&lt;span style=&#34;white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;
&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;function combineReservoirs(q, r1, r2, ..., rk)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Reservoir s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;for&lt;/span&gt; r in {r1, r2, ..., rk} do
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    s.update(r.y, p_hat(r.y) * r.W * r.M)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  s.M = s1.M + s2.M + ... + sk.M
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  s.W = s.w_sum / (p_hat(s.y) * s.M)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Please notice that the 4th line re-update the M of the reservoir even if it is already updated in the update function on the 3rd line already. It actually overrides that. The M is bumped by one in the update function as it assumes the incoming input sample is only one sample. However, as we essentially treat each target sample as it has multiple duplications and feed them multiple times, we would need to update the visited sample count properly.&lt;/p&gt;
&lt;h2 id=&#34;temporal-reuse&#34;&gt;Temporal Reuse&lt;/h2&gt;
&lt;p&gt;At this point, it is not hard to guess that the temporal reuse is nothing but a new target sample treated as a proposal sample. The only requirement for keeping the unbiased nature when merging neighbor pixels is correctly counting the denominator, only count the ones with higher than zero probability. So even if the previous frame&amp;rsquo;s corresponding pixel is not quite accurate, this is not a problem. It would some negative impact on the convergence, but not the unbiasness nature of the algorithm.&lt;/p&gt;
&lt;p&gt;Below is a graph looping all processes ReStir DI does for a pixel, though it doesn&amp;rsquo;t count the repetitive loop of neighbor sampling.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_4.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_4.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;To answer the previous question, why TAA is biased while ReStir temporal reuse is unbiased. In TAA, the lighting signal sampled in the previous frame is already evaluated, which could be incorrect in this frame because of lots of factors, like lighting, animation, occlusion, etc. While in the temporal reuse of ReStir, it only just takes samples, the evaluation hasn&amp;rsquo;t happened yet. The worst thing that could happen is that it takes a bad sample from some unrelated pixel, but it won&amp;rsquo;t introduce bias mathematically.&lt;/p&gt;
&lt;h2 id=&#34;correcting-the-bias-in-restir&#34;&gt;Correcting the Bias in ReStir&lt;/h2&gt;
&lt;p&gt;In order to correct bias for uniform MIS weight, we would need to evaluate how many PDFs would have non-zero contribution at a specific sample position. Once again, we encounter the same problem as before, the SIR PDF is not trackable.&lt;/p&gt;
&lt;p&gt;However, what we need from the SIR PDF is really whether it is large than zero or not, it is a binary information. We don&amp;rsquo;t really need to know what exactly the value it is. In order to predict that, as we previously mentioned, we can choose to use target distribution and proposal PDF to predict if SIR PDF is zero. If either of them is zero, the SIR PDF will be zero. On the other hand, if both of them are not zero, the SIR PDF is larger than zero. And we can also take advantage of the fact that any proposal sample positions to be evaluated for bias correction is already on a light source because they originally came in as an initial light sample. Either through direct light proposal PDF or SIR PDF, they indeed made it this far, meaning their proposal PDF is already larger than zero. So all we need to do is to test if the target distribution is larger than zero or now, which is exactly what the paper does.&lt;/p&gt;
&lt;h2 id=&#34;visibility-reuse&#34;&gt;Visibility Reuse&lt;/h2&gt;
&lt;p&gt;This is the last topic I would like to explain.
I almost had myself convinced it was a biased solution for quite a while until recently when I realized it is not.
To begin with, let&amp;rsquo;s start with this graph,&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_5.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_5.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Compared with the first ReStir graph we showed, this simply adds a visibility check (purple diamond shape) before even merging the samples. If it is occluded, it would ignore this sample. If we regard the visibility to be part of the target distribution, we would simplify the graph this way&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_6.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_6.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;It seems that there could be a problem here, we use two different target distributions here for the current pixel. And RIS doesn&amp;rsquo;t support it mathematically. For the top left part of the graph, for quite a while, I thought the algorithm should have checked visibilitiy for each proposal light sample, rather than only the winner light sample to make sure the target distribution is consistent for this pixel&amp;rsquo;s RIS algorithm.&lt;/p&gt;
&lt;p&gt;However, it turns out that we don&amp;rsquo;t necessarily need to do it even if it is not a wrong thing to do. Just like we regard the whole top right part as a single execution of RIS. We could think about the light samples exactly the same way. The essential truth behind it is that the current pixel&amp;rsquo;s reservoir never sees the individual light proposal samples when visibility reuse is used, it sees a target sample drawn from a SIR method with unshadowed light contribution as target distribution. Just like a neighbor target sample is drawn from a different target distribution, this simply uses another target distribution for its own mini RIS execution.&lt;/p&gt;
&lt;p&gt;With this explanation, the above graph boils down to this simplified form that we are very familiar with&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_7.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/understanding_the_math_behind_restir_di/ReStir_7.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Needless to explain at this point, this is just MIS used in RIS, something we have visited a few times in this blog. The key observation here is that with visibility reuse, the current pixel&amp;rsquo;s reservoir no longer sees the light proposal samples anymore and this is why we can use different a target distribution to filter samples as the later RIS execution sees only one sample drawn from a SIR method, it doesn&amp;rsquo;t care what target distribution it uses.&lt;/p&gt;
&lt;p&gt;Now we can see that this has its theoretical basis that it is not biased. But we have another follow-up question here, since this is just like sampling a neighbor target sample, we would need to have one single target sample with the proper weight coming in. So if we would like to take a sample from a RIS pdf, we would need to use the weight of $\hat{p}&amp;rsquo;(x) W(x) N$, where N is the number of light proposal samples and $\hat{p}&amp;rsquo;(x)$ is the shadowed light contribution $L(w_i) f_r(w_i, w_o) cos(\theta_i) V(p, p&amp;rsquo;)$, W is just the big W we talked about &lt;a href=&#34;http://localhost:1313/posts/understanding_the_math_behind_restir_di/#what-is-the-proposal-pdf-for-neighbors-target-sample&#34;&gt;before&lt;/a&gt;. It seems that a more self-explanatory implementation is to use one reservoir to pick light sample winners and another empty reservoir to merge all the target samples, either from light sample filtering, temporal reuse or neighbor pixels&amp;rsquo; reservoir. However, rather than starting with a new reservoir in the temporal reuse phase, the paper reuses the light samples reservoir directly, which doesn&amp;rsquo;t match the graph we showed above. The only thing that the paper does is to adjust the reservoir big W to zero if the winner light sample is occluded. This reuse of reservoir easily gives people a false impression that the current pixel&amp;rsquo;s reservoir has visited all the light proposal samples directly and then the neighbor&amp;rsquo;s target sample as they seem sharing the same reservoir. So it is very easy to have this confusion about the two different target distribution issues. The truth is the paper does nothing wrong, so let&amp;rsquo;s see what happens.&lt;/p&gt;
&lt;p&gt;Right after phase 1, where we finished all proposal sample generation and resolved a winner light sample, the current pixel&amp;rsquo;s reservoir state is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;y -&amp;gt; The selected winner sample.&lt;/li&gt;
&lt;li&gt;$w_{sum}$ -&amp;gt; The sum of all light samples&amp;rsquo; weights. Please be noted that the light sample weight&amp;rsquo;s nominator uses the unshadowed target distribution, rather than the shadowed one we will use to pick a final sample for this pixel.&lt;/li&gt;
&lt;li&gt;N -&amp;gt; The number of direct light proposal samples it has visited so far. (The paper used M here, in order to stay consistent with my previous derivation, N is used here. It is the same thing.)&lt;/li&gt;
&lt;li&gt;W -&amp;gt; This is a derived member that merely serves as a cache. It doesn&amp;rsquo;t have any extra signal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a reminder, the big W is defined as&lt;/p&gt;

$$W(x) = \dfrac{1}{N} \dfrac{1}{\hat{p}(x)}\Sigma_{i=1}^{N}\dfrac{\hat{p}(y_i)}{p(y_i)}$$
&lt;p&gt;Please be noted that I swapped the notation of x and y here, but it is the same thing, except that x is a subset of y now. And M is swapped with N as well. None of these changes the essential meaning of this equation. One subtle change I made here is the lack of subscripts in proposal PDF, this is fine as the paper does indicate it only uses one single proposal PDF for light sample picking, which we have explained as well.&lt;/p&gt;
&lt;p&gt;If we would start with a brand new empty reservoir, we would need to treat the resolved target light sample with this weight, $\hat{p}&amp;rsquo;(x) W(x) N$. Since the new target distribution is nothing but the original unshadowed target distribution with an extra binary signal on top of it. We have&lt;/p&gt;

$$\hat{p}&amp;#39;(x) = V \hat{p}(x)$$
&lt;p&gt;If we combine the two math equations and drop a new sample generated by SIR method in an empty reservoir, we would have this as its weight for the incoming proposal sample in the execution of this pixel&amp;rsquo;s RIS execution,&lt;/p&gt;

$$ \begin{array} {lcl} \hat{p}&amp;#39;(x) W(x) N &amp;amp; = &amp;amp; V \hat{p}(x) N \dfrac{1}{N} \dfrac{1}{\hat{p}(x)}\Sigma_{i=1}^{N}\dfrac{\hat{p}(y_i)}{p(y_i)} \\\\ &amp;amp; = &amp;amp; V \Sigma_{i=1}^{N}\dfrac{\hat{p}(y_i)}{p(y_i)} \end{array} $$
&lt;p&gt;So if we would start from a new reservoir, the weight for the one single sample, which is the resolved target light sample, is exactly the sum of all previous light samples&amp;rsquo; weight, but with an extra visibility term. Please be mindful that the nominator in the sum is the original unshadowed target distribution, rather than the new shadowed one.&lt;/p&gt;
&lt;p&gt;Depending on the occlusion situation, we have two possibilities here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the resolved light target sample is not occluded, we have precisely what is already cached in the reservoir after phase 1.&lt;/li&gt;
&lt;li&gt;If it is occluded, this single new sample will have zero weight as its target distribution is zero. All we need to do is to adjust the weight to zero, which is exactly what the paper does.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is a lot easier to understand the other members in the reservoir data structure,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the picked sample, regardless which of the two implementations is used, after the optional phase, they all pick the resolved light sample. Of course, if it is occluded, nothing is picked.&lt;/li&gt;
&lt;li&gt;And for the number of visited samples in the reservoir, since we pretend to duplicate the resolved light target sample by the number of visited proposal light samples, this is a perfect match as well.&lt;/li&gt;
&lt;li&gt;Since the big W is nothing but a cache, it is also a match as all its variables are already matched.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So now we can see that the paper does indeed implement the same thing if we take a deeper look at the math.
Hopefully, this would convince readers that this algorithm is indeed unbiased.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this blog post, I summarized my understanding of the original ReStir algorithm. I dug into the mathematical details and explained why the algorithm is unbiased. This blog is what I collected during my learning process. I put it down here in the hope that it could be helpful to those people who have similar confusion like myself. Hopefully, this could be helpful for someone to get a deeper understanding of the tech.&lt;/p&gt;
&lt;p&gt;This blog doesn&amp;rsquo;t mention any engineering problems that we could face during implementing ReStir, like memory catch issue when sampling millions of lights, floating point precision overflow issue after a few iterations, etc. For more details about that part, I would recommend watching this &lt;a href=&#34;https://developer.nvidia.com/blog/new-video-light-resampling-in-practice-with-rtxdi/&#34;&gt;talk&lt;/a&gt; or taking a look at the source code of &lt;a href=&#34;https://developer.nvidia.com/rtx/ray-tracing/rtxdi&#34;&gt;RTX DI&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://research.nvidia.com/publication/2020-07_spatiotemporal-reservoir-resampling-real-time-ray-tracing-dynamic-direct&#34;&gt;Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://research.nvidia.com/publication/2021-06_restir-gi-path-resampling-real-time-path-tracing&#34;&gt;ReSTIR GI: Path Resampling for Real-Time Path Tracing&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://research.nvidia.com/publication/2022-07_generalized-resampled-importance-sampling-foundations-restir&#34;&gt;Generalized Resampled Importance Sampling: Foundations of ReSTIR&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://graphics.cs.utah.edu/research/projects/volumetric-restir/&#34;&gt;Fast Volume Rendering with Spatiotemporal Reservoir Resampling&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://www.zyanidelab.com/how-to-add-thousands-of-lights-to-your-renderer/&#34;&gt;How to add thousands of lights to your renderer and not die in the process&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;https://gamehacker1999.github.io/posts/restir/&#34;&gt;Spatiotemporal Reservoir Resampling (ReSTIR) - Theory and Basic Implementation&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&#34;https://graphics.stanford.edu/papers/veach_thesis/thesis.pdf&#34;&gt;Robust Monte Carlo Methods For Light Transport Simulation&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&#34;https://agraphicsguynotes.com/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/&#34;&gt;Practical implementation of MIS in Bidirectional Path Tracing&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&#34;https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1662&amp;amp;context=etd&#34;&gt;Importance Resampling for Global Illumination&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&#34;https://blog.demofox.org/2022/03/02/sampling-importance-resampling/&#34;&gt;Sampling Importance Resampling&lt;/a&gt;&lt;br&gt;
[11] &lt;a href=&#34;https://agraphicsguynotes.com/posts/monte_carlo_integral_with_multiple_importance_sampling/&#34;&gt;Monte Carlo Integral with Multiple Importance Sampling&lt;/a&gt;&lt;br&gt;
[12] &lt;a href=&#34;https://research.nvidia.com/publication/2019-07_dynamic-many-light-sampling-real-time-ray-tracing&#34;&gt;Dynamic Many-Light Sampling for Real-Time Ray Tracing&lt;/a&gt;&lt;br&gt;
[13] &lt;a href=&#34;https://fpsunflower.github.io/ckulla/data/many-lights-hpg2018.pdf&#34;&gt;Importance Sampling of Many Lights with Adaptive Tree Splitting&lt;/a&gt;&lt;br&gt;
[14] &lt;a href=&#34;http://cwyman.org/papers/rtg2-manyLightReGIR.pdf&#34;&gt;Rendering Many Lights With Grid-Based Reservoirs&lt;/a&gt;&lt;br&gt;
[15] &lt;a href=&#34;https://de45xmedrsdbp.cloudfront.net/Resources/files/TemporalAA_small-59732822.pdf&#34;&gt;High Quality Temporal SuperSampling&lt;/a&gt;&lt;br&gt;
[16] &lt;a href=&#34;https://www.pbr-book.org/&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;br&gt;
[17] &lt;a href=&#34;https://blog.demofox.org/2022/03/02/sampling-importance-resampling/&#34;&gt;Sampling Importance Resampling (blog)&lt;/a&gt;&lt;br&gt;
[18] &lt;a href=&#34;https://blog.demofox.org/2022/03/01/picking-fairly-from-a-list-of-unknown-size-with-reservoir-sampling/&#34;&gt;Picking Fairly From a List of Unknown Size With Reservoir Sampling&lt;/a&gt;&lt;br&gt;
[19] &lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcfall20-a21210/&#34;&gt;Rendering Games With Millions of Ray-Traced Lights&lt;/a&gt;&lt;br&gt;
[20] &lt;a href=&#34;https://developer.nvidia.com/blog/new-video-light-resampling-in-practice-with-rtxdi/&#34;&gt;Light Resampling In Practice with RTXDI&lt;/a&gt;&lt;br&gt;
[21] &lt;a href=&#34;https://github.com/EmbarkStudios/kajiya/blob/main/docs/gi-overview.md&#34;&gt;Global illumination overview in Kajiya Renderer&lt;/a&gt;&lt;br&gt;
[22] &lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s32639/&#34;&gt;RTXDI: Details on Achieving Real-Time Performance&lt;/a&gt;&lt;br&gt;
[23] &lt;a href=&#34;https://research.nvidia.com/publication/2021-07_rearchitecting-spatiotemporal-resampling-production&#34;&gt;Rearchitecting Spatiotemporal Resampling for Production&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making a Shading Language for my Offline Renderer</title>
      <link>https://agraphicsguynotes.com/posts/making_a_shading_lagnauge_for_my_offline_renderer/</link>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/making_a_shading_lagnauge_for_my_offline_renderer/</guid>
      <description>&lt;p&gt;As a graphics programmer, I don&amp;rsquo;t usually spend too much time on something that is not strictly related to computer graphics or game engine. However, I did spend four months in my spare time last year building a shading language for my renderer &lt;a href=&#34;https://sort-renderer.com&#34;&gt;SORT&lt;/a&gt;, which I call &lt;a href=&#34;https://tiny-shading-language.com/&#34;&gt;Tiny Shading Language (TSL)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the beginning, I didn&amp;rsquo;t know how it would end up eventually due to the lack of knowledge about how compilers work in general, this is not something graphics programmers touch regularly. However, it does turn out without too much work, this thing can be done by one person in a few months.&lt;/p&gt;
&lt;p&gt;In this blog, I will briefly cover some of my thoughts in designing this shading language library. To be more specific, this blog is about how the system is designed and how it works with an offline CPU renderer, instead of the detailed language implementation.&lt;/p&gt;
&lt;p&gt;The following is a screenshot of the &lt;a href=&#34;https://github.com/JiayinCao/Tiny-Shading-Language/tree/master/src/tsl_sample&#34;&gt;example tutorial&lt;/a&gt; that comes with the TSL library.
The patterns on the surface of the two spheres are procedurally generated in TSL.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/tsl_example.jpg&#34; width=&#34;700&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Whenever people heard about my new shading language, the first thing they asked was always, &amp;lsquo;why do you want to make your own shading language since there is already OSL&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;This is a fairly good question, I had the same doubts for more than half a year before putting my hands on it.
There are generally several reasons,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By working on my own shading language, I can learn everything from scratch by myself. This is clearly the biggest reason that I chose to work on it. The knowledge gained in the process of making it would be valuable to my career in the future, at least it should have some indirect impact on my work. It should allow me to have a much deeper understanding of how a programming language compiler works.&lt;/li&gt;
&lt;li&gt;Having my own code base will allow me to change the library anyway I see fit. This alone offers me a lot more flexibility than OSL since I&amp;rsquo;m not familiar with their implementation.&lt;/li&gt;
&lt;li&gt;Since Apple is currently in the transition from Intel chips to ARM, future MacOSs will be shipped on ARM architecture. Building OSL on ARM will require building all its dependencies on ARM too. If there is any implementation that is x86 specific in any of its dependencies, I will have to find a workaround implementation on ARM too. Without OSL on ARM, there is no way to port my renderer on Apple Silicon. Supporting Apple Silicon is in OSL&amp;rsquo;s roadmap, but it is unclear when it will be available by the time this blog was written.&lt;/li&gt;
&lt;li&gt;OSL heavily uses this library called &lt;a href=&#34;https://sites.google.com/site/openimageio/home&#34;&gt;Open Image IO&lt;/a&gt;, which is another open-source project. While OIIO further depends on several other small libraries like, &lt;a href=&#34;https://www.openexr.com/&#34;&gt;OpenExr&lt;/a&gt;, &lt;a href=&#34;http://www.libpng.org/pub/png/libpng.html&#34;&gt;libpng&lt;/a&gt;, &lt;a href=&#34;http://www.libtiff.org/&#34;&gt;libtiff&lt;/a&gt; and a few others. Some of the basic data structures are only defined in OIIO, so decoupling OSL with OIIO would require quite some work. This was one of the options that I have considered, but after a second thought, it will also make updating OSL in my renderer very hard since it is locally modified.&lt;/li&gt;
&lt;li&gt;Having too many dependencies does make OSL a bit &amp;lsquo;heavy&amp;rsquo; than expected. My original expectation was just to have one OSL lib as dependencies, it certainly didn&amp;rsquo;t end up the way I planned. Eventually, there are other libs too and on some platforms, some of the libraries have to be dynamically linked too. Ideally, I could have spent much more time to make sure all of the dependencies are statically compiled, but it is way more sophisticated than it sounds. I wouldn&amp;rsquo;t want to spend too much time building those libraries from source code. Also, some of the pre-compiled libraries are OS-dependent on Ubuntu, which means that I will have to compile it another time on another version of Ubuntu. At the end of the day, after solving all these problems, features in OIIO are not even used in my renderer at all and I do not have a plan to use them in my renderer since doing it from scratch would be lots of fun, it is a dependency purely because OSL needs it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are other reasons that motivate me to implement my own shading language. However, those are mostly related to OSL. I would like to avoid having comments about OSL since at the time this blog was written, I was an employee of Sony (Naughty Dog).&lt;/p&gt;
&lt;p&gt;With the above-mentioned reasons, I hope I have made it clear why I decided to implement my own shading language. Of course, I am fully aware that my own implementation will be way less robust than OSL since there is a team behind it and this tech has been involved for more than ten years. So my next question after finalizing my decision was whether this is doable by myself in a few months, I definitely didn&amp;rsquo;t want to deviate from my trail too much, I&amp;rsquo;m a graphics programmer anyway, no one will expect me to know too many details in designing a compiler.&lt;/p&gt;
&lt;h1 id=&#34;taking-advantage-of-existed-work&#34;&gt;Taking Advantage of Existed Work&lt;/h1&gt;
&lt;p&gt;The image below demonstrates some basic stages of compiling a programming language into machine code
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/compiler_stages.png&#34;/&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Starting from scratch and making everything by myself sounds crazy and it is not likely that I can finish everything in a few months.
After some basic research and learning, I found some useful tools that could help me make my shading language a reality.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Flex_(lexical_analyser_generator)&#34;&gt;Flex&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Flex is one of the commonly used tools as a lexical analyzer. Taking a string stream, it will basically tokenize all of the string in a pre-defined manner.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/GNU_Bison&#34;&gt;Bison&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
Bison is a syntax analyzer. It will take tokens generated from Flex and generate an &lt;a href=&#34;https://en.wikipedia.org/wiki/Abstract_syntax_tree&#34;&gt;abstract syntax tree&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://llvm.org/&#34;&gt;LLVM&lt;/a&gt;&lt;/strong&gt;&lt;br&gt;
LLVM is short for low level virtual machine. It is a complex infrastructure that could help convert LLVM IR to machine code on different target architectures, like PC, X86, etc. It also does optimization too.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the these tools, it sounds like I only need to do the following things&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make a configuration file for Flex to tokenize my shading language.&lt;/li&gt;
&lt;li&gt;Make a configuration file for Bison to use the tokens generated from Flex and generate AST.&lt;/li&gt;
&lt;li&gt;Generate LLVM IR with the AST generated from Bison.&lt;/li&gt;
&lt;li&gt;Compile the IR into JITed machine code with LLVM.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is way more manageable than writing everything by myself. And it did give me some confidence that this is doable by one person.&lt;/p&gt;
&lt;p&gt;However, apart from the basic language support, which is to convert my shading language to JITed machine code, this is far from enough since a programmable shading language for CPU ray tracers is fundamentally different from its GPU counterpart.
A large proportion of the project is to design a user-friendly interface and let it fit well in my renderer.
This is no less than the amount of work to be done just to convert high level language to machine code.&lt;/p&gt;
&lt;p&gt;This blog is mainly about the latter. It won&amp;rsquo;t mention anything about the former part since there is plenty of resources on the internet that could help in those topics. &lt;a href=&#34;https://llvm.org/docs/tutorial/MyFirstLanguageFrontend/index.html&#34;&gt;Kaleidoscope&lt;/a&gt; is a very good example that comes with the LLVM library.&lt;/p&gt;
&lt;h1 id=&#34;where-does-it-fit-in-a-ray-tracer&#34;&gt;Where does It Fit in a Ray Tracer&lt;/h1&gt;
&lt;p&gt;Before we dive into the details of this shading language, I would like to put down some notes to offer the big picture of how this library fit in a ray tracer so that readers should have a brief idea about how it works with the rest of the system.&lt;/p&gt;
&lt;p&gt;A barebone path tracing algorithm could work like this,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Spawn primary ray for each pixel sample.&lt;/li&gt;
&lt;li&gt;Find the nearest intersection with the scene.&lt;/li&gt;
&lt;li&gt;Use the material information to construct a BSDF.&lt;/li&gt;
&lt;li&gt;Evaluate the BSDF and update the throughput and accumulate it in the result.&lt;/li&gt;
&lt;li&gt;Importance sampling the BSDF and generate secondary rays if needed.&lt;/li&gt;
&lt;li&gt;Get back to step 2 and loop.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The loop stops when there is no intersection found in step 2. Of course, this is also a simplified workflow since there is no volumetric rendering and subsurface scattering in it. But this is more than good enough for me to explain where TSL should fit.&lt;/p&gt;
&lt;p&gt;I guess all readers could have guessed at this point. The TSL execution should happen in step 3, which is exactly what the shader language is for. In PBRT 3rd, there are pre-defined materials with hard-coded BSDF and parameters of BXDF are exposed through pbrt input file with limited flexibility. With TSL, a ray tracer can translate shader script during runtime without itself being recompiled and this offers great flexibility. The purpose of TSL shader execution is to reconstruct BRDFs in the target BSDF, this purpose is very similar with what PBRT does with its material implementation, except that it is not hard coded in the ray tracer itself. Shader authoring can happen later when artists are working on assets.&lt;/p&gt;
&lt;h1 id=&#34;language-system-design&#34;&gt;Language System Design&lt;/h1&gt;
&lt;p&gt;TSL is designed to be simple and easy to use. The syntax of the language is very C-like, just like GLSL, HLSL, OSL. This is user friendly to even new shader authors.&lt;/p&gt;
&lt;p&gt;However, a CPU ray tracer shading language has lots of differences that are fundamentally different from a GPU shading language, which has a big impact on its language system design. TSL works in a very simliar manner with OSL, which also works on CPU. The followings are some of the major differences that TSL has compared with a GPU shading language.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In a graphics API, before issuing a draw call, we need to send information from host(CPU) to GPU, commonly including constant buffer, vertex buffer, index buffer, etc. Once all of the shader inputs are set, along with other render states, a draw call is issued. Depending on the exact situation, the number of shader executions could be up to a million or even more. In a nutshell, with everything setup once, shaders are usually executed on a massive scale.
While TSL&amp;rsquo;s shader setup and execution is almost always one vs one. This is defined by the nature of CPU ray tracers. CPU&amp;rsquo;s parallelism is not utilized in the same way as GPU since different threads are not synchronized at all, there is simply nothing like Warp or Wavefront. Without synchronization, since different threads almost never run the same instructions like GPU does, executing shaders multiple times makes almost no sense at all. SIMD optimization would be a fairly bad fit for shaders since there are no four instances of shader executions at the same time.&lt;br&gt;
This is only true in my renderer. OSL has SIMD in its roadmap and some commercial renderers do take advantage of it to run batched shader execution for better performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Modern game engines all support material graph, which is more of a visual programming language tool for technical artists to &amp;lsquo;code&amp;rsquo;. However, the GPU shader compilers are not aware of such a thing at all. There is something called &amp;lsquo;shader builder&amp;rsquo; that builds the finalized shader source code from different shader fragments collected from the material graph. These shaders are usually called material shaders. The other type of shaders in game engines is commonly called in-game shader, which is usually programmed by graphics programers. And the GPU shader compiler will take the shader source code without even knowning whether it is an in-game shader or a composited material shader.&lt;br&gt;
There is generally no &amp;lsquo;in-game&amp;rsquo; shader in an offline renderer. But material shader concept is a necessarity to support a wide variety of material appearance on the surfaces. Different from GPU shader compiler, I chose to implement the &amp;lsquo;shader builder&amp;rsquo; algorithm inside TSL library just like OSL did so that renderers with TSL integrated only need to take shader fragments, which I call shader unit template, and TSL will be responsible for building the finalized shader code, which the renderer doesn&amp;rsquo;t even get a chance to see. The benefit of doing this is that renderers are free of the responsibility of implementing a shader builder to support shader-graph-like material.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In a GPU shader, the inputs are textures, constants and vertics. The output is usually a single plain data structure that has data for the following pipeline stages.
The input of TSL is similar to GPU shading language, it has a global constant, global texture handles. While the output and shader source code definition is dramatically different from its GPU counterpart.&lt;br&gt;
It is generally a bad idea to do all BXDF evaluation like game engines do in TSL alone for two reasons.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, BXDF supports not only its evaluation, but also interfaces for importance sampling. If everything is done in TSL, it means a shader graph must be translated into a few types of shaders, that support evaluation, importance sampling, etc. And whenever it needs more interfaces for BXDF, we need a new type of shader generated in TSL.&lt;/li&gt;
&lt;li&gt;Second, which is equally important, lots of renderers already have solid implementation of quite a few BXDFs. Having everything done in TSL will means for renderers integrating TSL, they need to re-implement everything they did in their BXDF implementation in TSL again. And not to mention there is no stl support in TSL, it will be very hard to implement sophisticated BXDF that could have been easily done in C++.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Due to the above reasons, the output of TSL is designed to be a closure tree just like OSL. A closure basically indicates a BXDF in a renderer. Different from a real-time rendering engines, which mostly use Microfacet and Lambert as its shading model, offline renderers&amp;rsquo; material system could be a lot more complex, which is usually defined by a BSDF. A BSDF is composed of multiple BXDFs with them linearly combined. What makes it a bit more complex is that some BRDF takes other BSDF as an input argument, which essentially converts the BSDF to a tree.  In TSL, instead of returning a color value, it generates a tree of closures, which matches perfectly well with BSDF&amp;rsquo;s definition. This will be covered in more detail later in this post. With the introduction of the type of closure tree, the only thing TSL needs to evaluate for real is actually the arguments to reconstruct the BSDF. For example, the base color for a lambert closure.&lt;br&gt;
Here it is worth mentioning that closure concept is just a place holder with its input argument values. It doesn&amp;rsquo;t necessarily match to BXDF. In the context of volumetric rendering, it can match to medium. As a matter of fact, it can match to anything to be evaluated later. I only used them to evaluate BXDF and medium in my renderer though.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unlike most GPU shading languages, TSL does allow call stack. As a matter of fact, TSL eventually will be resolved into CPU executable JITed code, there is simply no reason not to support call stack. Having call stacks will allow shader authors to implement algorithms like traversing a binary tree a lot easier than GPU shaders.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Due to the nature of GPU, shader execution is always synchronized across multiple shader cores for better performance. While in a CPU ray tracer, synchronized shader evaluation across multiple threads doesn&amp;rsquo;t offer too much benefit. TSL will be executed in a synchronized manner in its own thread though. Essentially, executing a TSL shader is nothing but calling a function from the host side, except that the machine code is compiled by TSL, not C++ compiler.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;shader-unit-template&#34;&gt;Shader Unit Template&lt;/h2&gt;
&lt;p&gt;Shader unit template is the very basic compilation unit of TSL shaders. The corresponding concept in OSL is called shader layer.
A shader unit template is nothing but a piece of shader code that will be compiled independently.&lt;/p&gt;
&lt;p&gt;For example, the following code is an example of shader unit template source code&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;/*
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt; * Shader entry for the shader unit template that makes a lamber closure
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt; */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader make_closure_lambert( in  color   basecolor,  &lt;span style=&#34;color:#007f7f&#34;&gt;// input basecolor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;                             in  vector  normal,     &lt;span style=&#34;color:#007f7f&#34;&gt;// intput normal
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;                             out closure o0          &lt;span style=&#34;color:#007f7f&#34;&gt;// output closure
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;                           )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// making a lambert closure with basecolor and its normal
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    o0 = make_closure&amp;lt;lambert&amp;gt;( basecolor , normal );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This piece of shader code can be compiled into a shader unit template object. It has one output argument, which is a closure that indicates a lambert BRDF. Of course, it is eventually up to my renderer to explain how to interpret this into the final BRDF, which could totally be another type of BRDF too. But commonly, I don&amp;rsquo;t do it that way.&lt;/p&gt;
&lt;p&gt;It is almost a perfect match for a node in a graph-based material system like what Blender offers.
Following is an example of a SORT material in Blender,&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/shader_graph.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/shader_graph.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;As we can see, the above shader unit template can be used as the representation of the diffuse node in this graph.
Both of them take two arguments as inputs and output one argument. Of course, shader unit template doesn&amp;rsquo;t have to output closure all the time. For example, the image node in the above texture could be defined this way&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;texture2d g_texture;  &lt;span style=&#34;color:#007f7f&#34;&gt;// texture handle
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;shader ImageShaderLinear( vector UVCoordinate ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt;  UVTiling ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          out color Result ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          out &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; Alpha ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          out &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; Red ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          out &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; Green ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                          out &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; Blue ){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    vector scaledUV = UVCoordinate * UVTiling;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Result = texture2d_sample&amp;lt;g_texture&amp;gt;( scaledUV.x , scaledUV.y );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Red = Result.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Green = Result.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Blue = Result.z;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Alpha = texture2d_sample_alpha&amp;lt;g_texture&amp;gt;( scaledUV.x , scaledUV.y );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As a matter of fact, this is exactly how this node is defined in my renderer with TSL.&lt;/p&gt;
&lt;p&gt;Careful readers might have noticed that we haven&amp;rsquo;t got a chance to specify the value of some of the input arguments, like the normal in diffuse TSL shader code. And both of the two input arguments are not set with values in the texture sampling TSL code. These will be set during shader group template construction.
A benefit of this design is that shader code doesn&amp;rsquo;t need to be recompiled just because the default value for arguments are different.
This means that the upper limit of the number of shader compilation is decided by the number of shader types, which commonly matches the type of material nodes. It is totally independent of the occurrences of each shader node in SORT materials.&lt;/p&gt;
&lt;p&gt;Also, there is a global texture handle defined in the above shader.
The texture2d_sample&amp;lt;g_texture&amp;gt; code will actually trigger a call back function which eventually delegates things back to renderers.
This way, renderers can perform all kinds of complex things like texture filtering, wrapping, or even texture cache system in native C++ programming language instead of TSL.&lt;/p&gt;
&lt;h2 id=&#34;shader-group-template&#34;&gt;Shader Group Template&lt;/h2&gt;
&lt;p&gt;Shader group template is what I used to represent a material with TSL in my renderer. The corresponding concept in OSL is called shader group.&lt;/p&gt;
&lt;p&gt;Unlike shader unit template, shader group template doesn&amp;rsquo;t offer an interface to take any TSL shader code at all.
Shader group template commonly takes the following information&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shader unit templates&lt;/li&gt;
&lt;li&gt;Shader unit template connections&lt;/li&gt;
&lt;li&gt;The default value for arguments in shader unit templates&lt;/li&gt;
&lt;li&gt;Exposed arguments of the shader group templates&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// the shader group template that represent the material
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;auto&lt;/span&gt; shader_group = shading_context-&amp;gt;begin_shader_group_template(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;example material&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// add the two shader units in this group
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;shader_group-&amp;gt;add_shader_unit(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;blend_unit&amp;#34;&lt;/span&gt;, shader_unit_blend, &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;true&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;add_shader_unit(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;diffuse_unit&amp;#34;&lt;/span&gt;, shader_unit_diffuse);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;add_shader_unit(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;mirror_unit&amp;#34;&lt;/span&gt;, shader_unit_mirror);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;add_shader_unit(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;texture_unit&amp;#34;&lt;/span&gt;, shader_unit_texture);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// setup connections between shader units
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;shader_group-&amp;gt;connect_shader_units(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;diffuse_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;o0&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;shader_unit_blend&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;in_bxdf0&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;connect_shader_units(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;mirror_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;o0&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;shader_unit_blend&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;in_bxdf1&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;connect_shader_units(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;texture_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;Result&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;diffuse_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;basecolor&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Setup the default value for arguments
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;shader_group-&amp;gt;init_shader_input(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;texture_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;UVCoordinate&amp;#34;&lt;/span&gt;, Tsl_Namespace::make_tsl_global_ref(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;uvw&amp;#34;&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;init_shader_input(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;texture_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;UVTiling&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;init_shader_input(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;diffuse_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;normal&amp;#34;&lt;/span&gt;, Tsl_Namespace::make_float3(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.0f&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.0f&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;init_shader_input(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;mirror_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;normal&amp;#34;&lt;/span&gt;, Tsl_Namespace::make_float3(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.0f&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.0f&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader_group-&amp;gt;init_shader_input(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;mirror_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;basecolor&amp;#34;&lt;/span&gt;, Tsl_Namespace::make_float3(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Expose arguments for the shader group template
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;shader_group-&amp;gt;expose_shader_argument(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;blend_unit&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;out_bxdf&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// indicate to the library the construction of shader group template is done
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;shading_context-&amp;gt;end_shader_group_template(shader_group.get());
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the above code is pretty self-explanatory. One detail that deserves our attention is the default value for UVCoordinate argument in texture_unit. Since the above code happens during shader initialization, there is no exact data of texture coordinate at this point at all. TSL allows its users to delay the value setup by referring to a global constant to be set up right before shader execution.&lt;/p&gt;
&lt;p&gt;The above code just demonstrates a simple shader graph. In real-world practical examples, there are cases the shader group template could be a lot more sophisticated than the above code. But the general idea holds the same.&lt;/p&gt;
&lt;h2 id=&#34;tsl-global-constant&#34;&gt;TSL Global Constant&lt;/h2&gt;
&lt;p&gt;It is more than necessary for the host program to provide some input so that shader can be executed with correct information. Such inputs are commonly texture coordinate, vertex position, vertex normal, etc. Basically, it is per shader execution data that can&amp;rsquo;t be defined during shader authoring or shader compilation. These data will only be available right before shader execution.&lt;/p&gt;
&lt;p&gt;To pass the correct information, it is first necessary to define the memory layout of the data structure.
And this memory layout needs to be visible to both of TSL shader code and host c++ program. In order to make things easier for TSL users, I made a macro inside TSL library. So to define a global constant, we only need to write down the following code&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Following code needs to appear in a header file
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;DECLARE_TSLGLOBAL_BEGIN(TslGlobal)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DECLARE_TSLGLOBAL_VAR(Tsl_float3, uvw)          &lt;span style=&#34;color:#007f7f&#34;&gt;// UV coordinate, W is preserved for now.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;DECLARE_TSLGLOBAL_VAR(Tsl_float3, position)     &lt;span style=&#34;color:#007f7f&#34;&gt;// this is world space position
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;DECLARE_TSLGLOBAL_END()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Following code should typically appear in a cpp file
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;IMPLEMENT_TSLGLOBAL_BEGIN(TslGlobal)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;IMPLEMENT_TSLGLOBAL_VAR(Tsl_float3, uvw)          &lt;span style=&#34;color:#007f7f&#34;&gt;// UV coordinate, W is preserved for now.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;IMPLEMENT_TSLGLOBAL_VAR(Tsl_float3, position)     &lt;span style=&#34;color:#007f7f&#34;&gt;// This is world space position
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;IMPLEMENT_TSLGLOBAL_END()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above code will silently create a data structure called TslGlobal and make it visible to both the shader code and its host program.
Right before shader execution, this data structure needs to be filled so that shaders can be executed with the correct data.&lt;/p&gt;
&lt;p&gt;It is technically possible to have different constant data layout for different shaders. But I didn&amp;rsquo;t choose to do it to avoid the cost of run-time branching even though modern CPUs might be good at branch prediction.&lt;/p&gt;
&lt;h2 id=&#34;shader-instance&#34;&gt;Shader Instance&lt;/h2&gt;
&lt;p&gt;Both of shader unit template and shader group template are only templates, they only define things. To execute a TSL shader, a shader instance needs to be constructed from either a shader unit template or a shader group template.
It is shader instance that eventually holds the JITed function pointer for the host program to call.&lt;/p&gt;
&lt;p&gt;A shader instance is thread-safe in the way that it can be used to execute shader on multiple threads simultaneously with no performance penalty.&lt;/p&gt;
&lt;p&gt;Following is a piece of host c++ code demonstrating how to execute a TSL shader&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;void&lt;/span&gt; ExecuteSurfaceShader( Tsl_Namespace::ShaderInstance* shader , BSDF&amp;amp; bsdf ){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// get the surface intersection data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;const&lt;/span&gt; SurfaceInteraction&amp;amp; intersection = bsdf.GetInteraction();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// fill in the tsl global constant
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    TslGlobal global;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    global.uvw = make_float3(intersection.u, intersection.v, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.0f&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    global.position = make_float3(intersection.position.x, intersection.position.y, intersection.position.z);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// shader execution
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    ClosureTreeNodeBase* closure = &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;nullptr&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;auto&lt;/span&gt; raw_function = (&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;void&lt;/span&gt;(*)(ClosureTreeNodeBase**, TslGlobal*))shader-&amp;gt;get_function();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    raw_function(&amp;amp;closure, &amp;amp;global);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// parse the surface shader
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    ProcessSurfaceClosure(closure, bsdf);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;closure&#34;&gt;Closure&lt;/h2&gt;
&lt;p&gt;TSL&amp;rsquo;s closure tree concept is very similar to OSL&amp;rsquo;s closure tree, if not the same.
The introduction of closure tree is probably the biggest difference between TSL and GPU shaders.&lt;/p&gt;
&lt;p&gt;Closure is a concept for deferred execution. It is something to be evaluated later, but the real evaluation is left to the renderers, not the shaders. And it should have enough signal for renderers to pick them up. This usually means that closure will have its type id and of course, the arguments constructing the closure, which will be evaluated in TSL shader execution.&lt;/p&gt;
&lt;p&gt;As explained before, we won&amp;rsquo;t evaluate BSDF in TSL.
If we don&amp;rsquo;t evaluate BSDF in TSL, the only choice is to delay the operations of BXDF later to a point where host C++ program can take control.
This is somewhat similar to sampling texture in TSL, in which case the real sampling code is still written in C++, not TSL.
But it is slightly different since there is no call back function for BSDF operations because these are only needed by the renderer, not the shading language.
And most importantly, BSDF reconstruction doesn&amp;rsquo;t need to be done before TSL shader execution is finished. Closure is pretty much the bridge concept between shader and ray tracer.&lt;/p&gt;
&lt;h3 id=&#34;closure-type&#34;&gt;Closure Type&lt;/h3&gt;
&lt;p&gt;Closure is like an indication of BXDF, with specific types and arguments so that it has enough information for renderers to reconstruct the BSDF of its interest. Of course, TSL library itself will have absolutely no knowledge of what type of BXDFs are supported by renderers. It is up to us to register a closure type in renderers before compiling shaders. And renderers need to parse them correspondingly when needed. This can be done this way&lt;/p&gt;
&lt;p&gt;To register a closure type, we need to specify a list of arguments first so that we know what information the closure takes. Again, this data structure needs to be visible for both host and target TSL programs. There is a macro in TSL library for us to claim the data structure&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// This is the declaration of the data structure of Lambert closure.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// This needs to appear in a header file.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;DECLARE_CLOSURE_TYPE_BEGIN(ClosureTypeLambert, &lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;lambert&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DECLARE_CLOSURE_TYPE_VAR(ClosureTypeLambert, Tsl_float3, base_color)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DECLARE_CLOSURE_TYPE_VAR(ClosureTypeLambert, Tsl_float3, normal)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DECLARE_CLOSURE_TYPE_END(ClosureTypeLambert)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// This is the definition of the data structure of Lambert closure
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// It has to match what is shown above. This commonly appears in a cpp file.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;IMPLEMENT_CLOSURE_TYPE_BEGIN(ClosureTypeLambert)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;IMPLEMENT_CLOSURE_TYPE_VAR(ClosureTypeLambert, Tsl_float3, base_color)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;IMPLEMENT_CLOSURE_TYPE_VAR(ClosureTypeLambert, Tsl_float3, normal)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;IMPLEMENT_CLOSURE_TYPE_END(ClosureTypeLambert)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have the closure type, registering the type in TSL library could be easily done through this code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// register lambert closure
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;ClosureTypeLambert::RegisterClosure();
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once it is registered in TSL&amp;rsquo;s system. TSL shader will recognize the closure type and generate the correct data if it is seen.
It is already shown in the first piece of code in this blog&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;o0 = make_closure&amp;lt;lambert&amp;gt;( basecolor , normal );
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we can see, there are two arguments in this closure type, they are all float3.
And the name of the closure type has to match what is specified in the macro.&lt;/p&gt;
&lt;h3 id=&#34;closure-operations-supported-in-tsl&#34;&gt;Closure Operations Supported in TSL&lt;/h3&gt;
&lt;p&gt;Closure almost behaves a color in TSL, except that it only supports the following operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Closure can add with another closure, but not with a regular color in TSL.
This does make sense since it makes no sense for a BXDF to blend with a constant color in offline renderers.&lt;/li&gt;
&lt;li&gt;Closure can only multiple with a regular color, closure can&amp;rsquo;t multiply with another closure.
The color is pretty much the weight for the closure, weighting a closure with another closure makes absolutely no sense at all.&lt;/li&gt;
&lt;li&gt;Results of the above two operations will be a closure type, it is not a regular color.
So the results of the above operations will have to stick to the above two rules too.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since closure could be treated as an indication of BXDF, the above rule makes sense. The same rule applies in cases where closure is used for other deferred process, like medium.
It is shader authors&amp;rsquo; responsibility to make sure no invalid math operations will be performed for closure type in TSL,
which will easily result in compilation errors.&lt;/p&gt;
&lt;h3 id=&#34;closure-tree&#34;&gt;Closure Tree&lt;/h3&gt;
&lt;p&gt;A closure tree is usually what is generated for renderers to parse so that the expected BSDF can be reconstructed through its information.
It should have the type of closures with the specific values for each argument.&lt;/p&gt;
&lt;p&gt;For example, with the above example, a closure tree could be like this&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/closure.jpg&#34; width=&#34;700&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;We can clearly notice from the above data structure that non-closure nodes have been resolved in the final closure tree.
It is not up to renderers to evaluate the non-closure nodes, TSL should do it. There are two implicit closure types generated&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Closure Add&lt;/strong&gt;&lt;br&gt;
This simply adds two closure together&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Closure Multiply&lt;/strong&gt;&lt;br&gt;
This indicates that there is a closure with a weight.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the above data structure, it is easy for any renderers to parse it and generate the BSDF with 70% lambert and 30% mirror in it.
And the exact arguments for constructing the BXDF is already packed in the above data structure by TSL.
Renderers should have enough information to reconstruct the expected BSDF.&lt;/p&gt;
&lt;h3 id=&#34;recursive-closure&#34;&gt;Recursive Closure&lt;/h3&gt;
&lt;p&gt;One of the advanced closure usages is that a closure can be used as an input argument for another closure.
This doesn&amp;rsquo;t belong to the above two operations mentioned. This is pretty much the only exception.&lt;/p&gt;
&lt;p&gt;The following code is borrowed from my renderer SORT, it demonstrates the idea pretty well&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// functions defined by c library
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; powf( &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; base , &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; exp );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; logf( &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; x );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; helper( &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; x , &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; inv ){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; y = logf(x) * inv;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;return&lt;/span&gt; y * y;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;shader bxdf_coat(   &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt;     IndexofRefraction ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt;     Roughness ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    color     ColorTint ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    closure   Surface ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    vector    Normal ,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    out closure Result ){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; inv = &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0&lt;/span&gt; / ( &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;5.969&lt;/span&gt; - &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.215&lt;/span&gt; * Roughness + &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;2.532&lt;/span&gt; * powf(Roughness,&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;2.0&lt;/span&gt;) - &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;10.73&lt;/span&gt; * powf(Roughness,&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;3.0&lt;/span&gt;) + &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;5.574&lt;/span&gt; * powf(Roughness,&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;4.0&lt;/span&gt;) + &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.245&lt;/span&gt; * powf(Roughness, &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;5.0&lt;/span&gt;) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    color sigma;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    sigma.r = helper(ColorTint.r,inv);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    sigma.g = helper(ColorTint.g,inv);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    sigma.b = helper(ColorTint.b,inv);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Result = make_closure&amp;lt;coat&amp;gt;( Surface , Roughness , IndexofRefraction , sigma , Normal );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we can notice from the above code, the first argument of Coat closure is a closure itself, which is passed in.
To give readers a better idea of its use case in my renderer, the following is a screenshot of a SORT material&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/coat.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/coat.png&#34; width=&#34;800&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;The Coat material node does take one surface node, which essentially just matches to a closure tree, as its arguments.
This offers the flexibility of supporting coating any other materials in the renderer.
In the above case, a Disney BRDF is coated with this type of material, this gives a nice coating later on top of Disney&amp;rsquo;s BRDF.
Disney BRDF has its own coat layer though, but their BRDF model&amp;rsquo;s coat layer shares the same normal data, which means that we will lose the smooth coating. Of course, it won&amp;rsquo;t be too hard to slighly modify Disney BRDF to support multiple layers of normal. But the point here is that recursive closure offers more flexibility.&lt;/p&gt;
&lt;p&gt;Following is a screenshot of the final rendered shot, the above material is for the top of the helmet&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/helmet.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/helmet.png&#34; width=&#34;700&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;We can easily notice that the coating layer and the underlying layer have a different normal map, which is exactly what we expected as specified in the source material.&lt;/p&gt;
&lt;h1 id=&#34;improvement&#34;&gt;Improvement&lt;/h1&gt;
&lt;p&gt;There aren&amp;rsquo;t many improvements I made compared with OSL during the development of TSL.
Most of the improvements are pretty minor. The only big one is that I made shader group template a type of shader unit template.&lt;/p&gt;
&lt;h2 id=&#34;shader-group-template-being-a-shader-unit-template&#34;&gt;Shader Group Template being a Shader Unit Template&lt;/h2&gt;
&lt;p&gt;This is by far one of my biggest improvements. I had this feature in mind the first day I initiated the project.
So in TSL, shader group template is composed of multiple shader unit templates. These shader unit templates are connected so that TSL knows how to generate the target machine code.&lt;/p&gt;
&lt;p&gt;Shader group template being shader unit template opens a new door for lots of flexibility. It essentially turns what originally is a tree structure into a multi-dimensional tree structure. Imagine a tree has multiple nodes, each of which could be a tree themselves.
A typical use case of this would be &lt;a href=&#34;https://developer.nvidia.com/rtx/raytracing/dxr/DX12-Raytracing-tutorial-Part-1&#34;&gt;TLAS and BLAS&lt;/a&gt; in a real-time ray tracing program.
This idea applies exactly the same in shader group template. With shader group template being shader unit template, it means that we can add a shader group template in another shader group template. And as a matter of fact, there is no limitation on how many extra dimensions can be added. We can totally have a shader group template A with another shader group template B in it and B can have its own shader group template C in it.&lt;/p&gt;
&lt;p&gt;This might not sound like a big feature to expend rendering features. But it does offer lots of value during my asset authoring.
Below is a real use case in my renderer&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/blender.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/making_a_shading_lagnauge_for_my_offline_renderer/blender.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;On the right side is the root material graph for a material. We can notice that all of the inputs of the Disney BRDF node comes from a group node, which is defined and shown on the left panel. The group node itself is another shader group template itself, which is embedded in the root shader group template used to represent the material.&lt;/p&gt;
&lt;p&gt;The biggest flexibility is that this group node can be used in multiple materials. Any change made in this group material node will automatically propagate to all its client materials.&lt;/p&gt;
&lt;p&gt;I did have this feature implemented in my Blender plugin when OSL was used as my shading languages, but there was no clear match for it, I have to hack it in some way that the plugin silently ungroup everything before generating the OSL code. This could be unnecessarily complex if shader group is a shader layer itself. With this feature, it is implemented in a much easier and robust way.&lt;/p&gt;
&lt;h2 id=&#34;other-misc-improvements&#34;&gt;Other Misc Improvements&lt;/h2&gt;
&lt;p&gt;Apart from the above-mentioned improvement, I also adjust the library in the following way I desired&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Simple Building Process&lt;/strong&gt;&lt;br&gt;
Building TSL is fairly easy. For detailed steps, please check out &lt;a href=&#34;https://tiny-shading-language.com/&#34;&gt;TSL&amp;rsquo;s main page&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-thread Shader Compilation&lt;/strong&gt;&lt;br&gt;
Both Flex and Bison codes have been adjusted to support multi-thread processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hidden Closure Registration&lt;/strong&gt;&lt;br&gt;
Closure register is done through an interface, there is no need to have a sperate header file shipped with the final executable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shader Unit Template Order&lt;/strong&gt;&lt;br&gt;
There is no need to pass shader unit templates to shader group template in a specific order, TSL will resolve it for its user.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Light Weight Library&lt;/strong&gt;&lt;br&gt;
The library is light-weight in a way that no extra dependencies will be needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this blog post, I put down some notes of my thoughts during the design and implementation of the Tiny-Shading-Language system.
The purpose of this blog is to provide readers a big picture of how a toy programmable shading language can be implemented for CPU ray tracers.
And it is also for me to keep things recorded somewhere so that I won&amp;rsquo;t be confused by my implementation when picking this up a few years later.&lt;/p&gt;
&lt;p&gt;This article by no means intends to offer a detailed specification of TSL. It only covers the very big picture of it. No detailed interfaces are mentioned. Sadly, I didn&amp;rsquo;t choose to write a language spec because I have no time to maintain such an expensive thing. For readers who are interested in how it works in a real ray tracer, please check out the example tutorial coming along with TSL library. Of course, it is also suggested to check out my TSL integration in &lt;a href=&#34;https://sort-renderer.com&#34;&gt;SORT&lt;/a&gt; to get a better idea of how this could fit in a slightly more sophisticated ray tracer.&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;With only limited time, TSL is by no means a very stable programmable shading language.
There are easily a thousand ways to crash the library without even a warning message by just writing invalid shaders, or even valid ones.
Incorrect setup of TSL interface will also potentially crash the system too.
However, during my usage of this library in my own renderer, once the shaders are written, it rarely crashes inside TSL shaders.&lt;/p&gt;
&lt;p&gt;If I have time, I would like to improve the following things&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More robust language syntax support&lt;/li&gt;
&lt;li&gt;More explanatory error and warning output&lt;/li&gt;
&lt;li&gt;Python interface to parse metadata of a shader unit template&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately, I have shifted my focus on real-time rendering in my spare time a while ago. I still hope I may have time to pick it up sometime later.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://llvm.org/devmtg/2010-11/Gritz-OpenShadingLang.pdf&#34;&gt;LLVM for Open Shading Language&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://www.guru99.com/compiler-design-phases-of-compiler.html&#34;&gt;Phases of Compiler with Example&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://www.freecodecamp.org/news/the-programming-language-pipeline-91d3f449c919/&#34;&gt;I wrote a programming language. Here’s how you can, too&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://gnuu.org/2009/09/18/writing-your-own-toy-compiler/&#34;&gt;Writing Your Own Toy Compiler Using Flex, Bison and LLVM&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://www.jonathanbeard.io/tutorials/FlexBisonC++&#34;&gt;FLEX AND BISON IN C++&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;https://efxa.org/2014/05/25/how-to-create-an-abstract-syntax-tree-while-parsing-an-input-stream/&#34;&gt;How to create an abstract syntax tree while parsing an input stream&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&#34;https://steemit.com/utopian-io/@drifter1/writing-a-simple-compiler-on-my-own-abstract-syntax-tree-principle-c-flex-bison&#34;&gt;Writing a simple Compiler on my own - Abstract Syntax Tree Principle [C][Flex][Bison]&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&#34;https://steemit.com/utopian-io/@drifter1/writing-a-simple-compiler-on-my-own-combine-flex-and-bison&#34;&gt;Writing a simple Compiler on my own - Combine Flex and Bison&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&#34;https://www.coder4.com/archives/3978&#34;&gt;Make a reentrant parser with Flex and Bison&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&#34;https://gist.github.com/38/39e7b514d916ed6fa6a2bba629fdf6ff&#34;&gt;A Minimal LLVM JIT example for LLVM-5&lt;/a&gt;&lt;br&gt;
[11] &lt;a href=&#34;https://www.youtube.com/watch?v=WL9FBwV0-3E&#34;&gt;Open Shading Language Community Meeting&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Practical Tips for Implementing Subsurface Scattering in a Ray Tracer</title>
      <link>https://agraphicsguynotes.com/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/</guid>
      <description>&lt;p&gt;Subsurface scattering has always been a challenge in computer graphics. Even in offline rendering, some of the common practical solutions are nowhere near unbiased. In certain algorithms, there are some pretty aggressive assumptions made to make things under control.
However, despite the complexity in subsurface scattering algorithms, it is by no means a feature that can be missing from a practical rendering engine. Major commercial renderers, like &lt;a href=&#34;https://www.arnoldrenderer.com/&#34;&gt;Arnold&lt;/a&gt;, &lt;a href=&#34;https://www.maxon.net/en-us/products/cinema-4d/overview/&#34;&gt;Cinema 4D&lt;/a&gt;, etc, all have very sophisticated and solid implementation of SSS. Robust implementation of SSS also exists in open-source projects, like &lt;a href=&#34;https://www.cycles-renderer.org/&#34;&gt;Cycles&lt;/a&gt; and &lt;a href=&#34;https://luxcorerender.org/&#34;&gt;luxcorerenderer&lt;/a&gt;, and even some research-based projects maintained by only a handful of people, like &lt;a href=&#34;https://pbrt.org/&#34;&gt;PBRT&lt;/a&gt; and &lt;a href=&#34;https://www.mitsuba-renderer.org/&#34;&gt;mitsuba&lt;/a&gt;. What is even amazing is that real-time rendering also has practical and efficient subsurface scattering implementation. In game industry, there is no shortage of amazing games with stunning and convincing skin rendering, for example, like &lt;a href=&#34;https://www.playstation.com/en-us/games/the-last-of-us-part-ii-ps4/&#34;&gt;the Last of Us Part II&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I happened to have some time last year and eventually finished an implementation that I&amp;rsquo;m satisfied with after quite a few iterations. Following is a shot using &lt;a href=&#34;https://sort-renderer.com&#34;&gt;my renderer&lt;/a&gt; and it is a good example demonstrating the SSS feature in it.
Also, all other images in this blog are rendered using my renderer too.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/blender_splash.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;There are quite some resources about SSS theories on the internet.
However, I rarely see any material revealing the specific details to be noted in implementing SSS algorithm.
This could totally be because I haven&amp;rsquo;t done enough research work in this field. But it is still worth my time to put down all the notes I have during my iterations.
Starting from integrating PBRT 3rd edition&amp;rsquo;s implementation, I quickly noticed that the implementation works for some cases, but there are cases where it fails to converge efficiently. By comparing with Cycles implementation, I made some progress with more iterations.&lt;/p&gt;
&lt;p&gt;This blog is about the iterations that I have made on top of PBRT&amp;rsquo;s implementation.
I will briefly mention the basics of SSS theory in PBRT. My introduction covers only part of what PBRT talks about in subsurface scattering.
It is highly suggested to read PBRT&amp;rsquo;s SSS chapter first before moving forward with reading this blog.
Most of this blog will be talking about the iterations that I made to make it more robust.&lt;/p&gt;
&lt;h1 id=&#34;what-is-subsurface-scattering&#34;&gt;What is Subsurface Scattering&lt;/h1&gt;
&lt;p&gt;Below is a comparison of a dragon with and without subsurface scattering.&lt;/p&gt;


















&lt;style&gt;
    .ba-slider1d9792c1cdd84287b704ca1876c4de13 {
        position: relative;
        overflow: hidden;
        width: 100%;
         
    }
    
    .ba-slider1d9792c1cdd84287b704ca1876c4de13 img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-slider1d9792c1cdd84287b704ca1876c4de13 .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-slider1d9792c1cdd84287b704ca1876c4de13 .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-slider1d9792c1cdd84287b704ca1876c4de13 .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-container1d9792c1cdd84287b704ca1876c4de13 {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-left1d9792c1cdd84287b704ca1876c4de13 {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-left1d9792c1cdd84287b704ca1876c4de13 {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-right1d9792c1cdd84287b704ca1876c4de13 {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-right1d9792c1cdd84287b704ca1876c4de13 {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centered1d9792c1cdd84287b704ca1876c4de13 {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-slider1d9792c1cdd84287b704ca1876c4de13&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-container1d9792c1cdd84287b704ca1876c4de13&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/regular_dragon.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-right1d9792c1cdd84287b704ca1876c4de13&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/sss_dragon.png&gt;
        &lt;div class=&#34;ba-bottom-left1d9792c1cdd84287b704ca1876c4de13&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;1d9792c1cdd84287b704ca1876c4de13&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;As we can easily see from the comparison, subsurface scattering shows a more waxy feeling, it feels more transparent than the Lambert BRDF.
In a nutshell, subsurface scattering means the incoming light penetrates through the surface of an object and leaves from another position that may not be the same as the position where it enters the surface. The most common example in our everyday life is human skin.
So the question is, what makes it a difficult challenge in computer graphics? If we get back to the rendering equation,&lt;/p&gt;

$$ {L_{o}(p_{o},\omega_{o}) = L_{e}(p_o, \omega_{o}) &amp;#43; \int_{\Omega} L_{i}(p_o, \omega_{i}) *f(p_o, \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;It is not hard to notice that there is no support for subsurface scattering in this equation at all since the whole integral is just about one single position. When we talk about BRDF, it is a simplified version of BSSRDF. We can treat BRDF as a delta function version of BSSRDF. If we look at the rendering equation in a more generalized way, it can be written like this,&lt;/p&gt;

$$ {L_{o}(p_{o},\omega_{o}) = L_{e}(p_o, \omega_{o}) &amp;#43; \int_A \int_{\Omega} L_{i}(p_o, \omega_{i}) *f(p_o, p_i, \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i} dA } $$
&lt;p&gt;As we can see, we gained two more dimensions by considering subsurface scattering. In other words, instead of just considering light coming from all directions at a single point on the surface, we also need to evaluate the contribution of light coming from all other positions. The direction domain is an isolated domain that is generally not affected by other factors. However, the same thing doesn&amp;rsquo;t go true for positions, which highly depends on the shape of the geometry, meaning accurate importance sampling position is a lot harder than sampling directions like what we used to do in BRDF evaluation.&lt;/p&gt;
&lt;p&gt;And even if we have a good importance sampling algorithm, integrating the implementation with the existed path tracing algorithm that used to only consider BRDF also requires close attention to avoid mistakes that could accidentally introduce bias. A practical render engine also supports multiple layers of BSSRDFs, like Cycles. I also implemented the algorithm to support multiple layers of BSSRDF so that there is more flexibility of asset authoring.&lt;/p&gt;
&lt;h1 id=&#34;basic-idea-of-sss&#34;&gt;Basic Idea of SSS&lt;/h1&gt;
&lt;p&gt;It makes very little sense to put down all of the math details of subsurface scattering here since there are already lots of resources available online.
However, to avoid confusing readers, it is still worthwhile to introduce the basic concept briefly.
Since I started by integrating PBRT&amp;rsquo;s implementation, the basic theory mentioned below is just a brief introduction to the implementation of SSS in PBRT 3rd edition.
For a deeper understanding of the way it works, PBRT has a few chapters that have a pretty solid explanation of every single detail in it.&lt;/p&gt;
&lt;p&gt;Whenever we have a new BRDF model, the two most important things we need to do are always to figure out a proper way to evaluate the BRDF and a solid importance sampling algorithm so that the implementation could converge efficiently. BSSRDF implementation sounded a bit intimidated to me at the beginning when I literally knew nothing about it. But the reality is that this is not that different from adding a new BRDF model, we still need to figure out a proper way to evaluate the function and an efficient importance sampling algorithm, for both direction and position. The only extra thing that is needed is to adjust the path tracing algorithm to make it aware of SSS and evaluate it when necessary.&lt;/p&gt;
&lt;h2 id=&#34;evaluating-bssrdf&#34;&gt;Evaluating BSSRDF&lt;/h2&gt;
&lt;p&gt;This &lt;a href=&#34;https://graphics.stanford.edu/papers/bssrdf/bssrdf.pdf&#34;&gt;paper&lt;/a&gt; first introduced the model of separable BSSRDF to make things manageable.
It achieved so by decoupling factors from each other.
PBRT&amp;rsquo;s work is based on top of it.&lt;/p&gt;

$$ S(p_o, p_i, \omega_o, \omega_i) \approx ( 1 - F_r(cos\theta_o) ) \space S_p(p_o, p_i) \space S_w(w_i)$$
&lt;p&gt;With three parts in this model, the first part is responsible for accounting for the energy loss during light leaving the surface from inside.
 $ S_p(p_o, p_i) $ 
  is mainly responsible for approximating the falloff factor due to the distance between  $ p_o $ 
 and  $ p_i $ 
 .
Like the first part, the last one is for evaluating light energy loss due to light coming in through the surface.&lt;/p&gt;
&lt;p&gt;The complexity of the simplified function is already a lot simpler with the approximation.
To simplify things even more,  $ S_p(p_o, p_i) $ 
  will only consider the distance of the two input points instead of the relative positions with each other on the surface of the object.
This greatly reduces the complexity of the problem again since BSSRDF will need no knowledge of the specific shape of the object, where the two points are lying on, to be evaluated.
And this is what we usually call &amp;lsquo;diffusion profile&amp;rsquo;.
A way to visualize it is to shoot a lazer thin ray on a totally flat surface with subsurface scattering.
One nice thing about this simplification is that the model is independent of the specific model of diffusion profile, which means that we can choose whatever we want as long as it is normalized.
Mathematically, the equation below needs to hold true,&lt;/p&gt;

$$ \int_0^{2\pi} \int_0^{\infty} S_p(r) \space r \space dr \space d\theta = 1$$
&lt;h2 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h2&gt;
&lt;p&gt;Importance sampling of BSSRDF involves two things, sampling an incoming direction and sampling a position where the incoming light enters the volume of the SSS object from the surface.
Sampling a direction is no different from sampling a direction using BRDF. As a matter of fact, PBRT uses a wrapper BXDF to hide the  $ S_w $ 
 part.
And with the BRDF setup, it easily reuses existed code infrastructure to sample it.&lt;/p&gt;
&lt;p&gt;Taking a sample around the point to be shaded is a bit more complex than taking a random direction around the same point since it depends on the shape of the geometry.
There are numerous methods that can help to take samples. For example, in PBRT 2nd edition, they construct a KD-Tree for saving randomly generated points on the surface of the geometry.
When taking samples, the algorithm can take advantage of the KD-Tree to &amp;lsquo;randomly&amp;rsquo; pick nearby samples. The method that I choose to implement in my renderer is from this &lt;a href=&#34;http://library.imageworks.com/pdfs/imageworks-library-BSSRDF-sampling.pdf&#34;&gt;paper&lt;/a&gt;,
which is also implemented in PBRT 3rd edition.
The basic idea is to assume the geometry is totally flat and randomly pick a sample on the disk that floats right above the shading point.
With the point picked on the disk, it then projects a ray (the green one) in the opposite direction of the surface normal at the shading point.
The intersection of the short ray with the original geometry will be the sampled point.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/projection.png&#34; width=&#34;550&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;In the above image,  $r_{max}$ 
 is decided by the diffusion profile. And r is a random sample based on the chosen pdf.
Luckily enough, the diffusion profile Disney offers in their &lt;a href=&#34;https://graphics.pixar.com/library/ApproxBSSRDF/paper.pdf&#34;&gt;paper&lt;/a&gt; already has a perfect pdf.
One detail to be noted here is that we should have three  $r_{max}$ 
 because the diffusion profile has three channels. Here I assume we are not doing the real spectrum rendering.
It is needed to pick one channel randomly first and having it taken into account eventually when evaluating its pdf.&lt;/p&gt;
&lt;p&gt;The problem is more than what it appears to be in the above image. Whenever there is an assumption that breaks, problems arise.
Of course, the geometries won&amp;rsquo;t be flat most of the time.
Depending on the geometry and diffusion profile, there could be multiple intersections instead of just one.
And also sometimes, when the projection ray is almost perpendicular to the geometry surface it intersects, it can result in fireflies.
Those problems are addressed in PBRT 3rd edition. Please check &lt;a href=&#34;http://www.pbr-book.org/3ed-2018/Light_Transport_II_Volume_Rendering/Sampling_Subsurface_Reflection_Functions.html&#34;&gt;here&lt;/a&gt; for further detail.&lt;/p&gt;
&lt;h2 id=&#34;how-does-it-fit-in-a-path-tracing-algorithm&#34;&gt;How Does It Fit in a Path Tracing Algorithm&lt;/h2&gt;
&lt;p&gt;In order to figure out how to integrate SSS implementation in a path tracer, it is necessary to know how a barebone path tracer works in general.
My previous &lt;a href=&#34;https://agraphicsguynotes.com/posts/basics_about_path_tracing/&#34;&gt;blog post&lt;/a&gt; mentioned some basics about it.
Below is the pseudo-code of a minimal path tracer algorithm with BSSRDF in it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;color li(scene, ray, max_bounce_cnt){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    color ret(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.0f&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;int&lt;/span&gt; bounces = &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;auto&lt;/span&gt; r = ray;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    color throughput(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1.0f&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;while&lt;/span&gt;( bounces &amp;lt; max_bounce_cnt ){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 1 - Get the intersection between the ray and the scene.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;//     Bail if it hits nothing. This is commonly the place to return sky color,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;//     while it is skipped for simplicity since it is not relevant.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        Intersection intersection;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;if&lt;/span&gt;( (intersection = get_intersect(scene, r)).IsInvalid() )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;break&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 2 - Construct BSDF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        intersection.bsdf = construct_bsdf(intersection);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 3 - Evaluate the contribution from light source
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        ret += evaluate_light(intersection) * throughput;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 4 - Importance sample for the bounced ray
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; bsdf_pdf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        vector dir;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;auto&lt;/span&gt; bsdf_evaluation = intersection.bsdf-&amp;gt;sample_f(&amp;amp;dir, &amp;amp;bsdf_pdf);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 5 - Update throughput
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        throughput *= bsdf_evaluation * dot(intersection.n, dir) / bsdf_pdf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 6 - Update the bounce ray
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        r = Ray(intersection.position , dir);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// This is where we handle BSSRDF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;if&lt;/span&gt;(intersection.bssrdf){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &amp;lt;&amp;lt;&amp;lt; Handle BSSRDF &amp;gt;&amp;gt;&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;// 7 - Russian roulette. rand() returns value from 0 to 1 randomly and 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#007f7f&#34;&gt;//     the random samples are uniformly distributed.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;if&lt;/span&gt;(rand() &amp;lt; &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.3f&lt;/span&gt; &amp;amp;&amp;amp; bounces &amp;gt; &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;3&lt;/span&gt; )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            throughput /= &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0.3f&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;else&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;break&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ++bounces;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;return&lt;/span&gt; ret;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;PBRT&amp;rsquo;s path tracing implementation is a lot more complex than the above code. But in order to keep things in control, I stripped the irrelevant parts with only the core left.
To make it clear, basically, it does the following things inside the loop (pretending that there is no bssrdf)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each ray, try getting the nearest intersection with the scene first. If the ray hits nothing, it will simply break out of the loop.&lt;/li&gt;
&lt;li&gt;If there is an intersection found, it will construct bsdf, which commonly represents the material.&lt;/li&gt;
&lt;li&gt;For a valid material, it will evaluate light contribution through the bsdf.&lt;/li&gt;
&lt;li&gt;Importance sampling for the next ray based on the bsdf, this is essentially a greedy method.&lt;/li&gt;
&lt;li&gt;Update the throughput value so that the contribution of the next light bounce will take this one into account.&lt;/li&gt;
&lt;li&gt;Update the bounce ray.&lt;/li&gt;
&lt;li&gt;Russian roulette to make things under control and unbiased.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This above logic is the very essential core of a path tracing algorithm with lots of detailed implementation skipped, for example like multiple importance sampling, sky rendering, volumetric rendering, efficient Russian roulette, etc.
But this is more than enough as a playground for us to introduce how a BSSRDF can fit in it.
In the case there is bssrdf available, we only need to implement the branch of the condition.
Following is the code snippet of &amp;lt;&amp;lt;&amp;lt; Handle BSSRDF &amp;gt;&amp;gt;&amp;gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Take a random sample by projecting a ray.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;Intersection bssrdf_intersection;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; bssrdf_pdf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;auto&lt;/span&gt; bssrdf_evaluation = intersection.bssrdf-&amp;gt;sample_s(scene, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                               &amp;amp;bssrdf_pdf, &amp;amp;bssrdf_intersection);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Bail if there is no intersection.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;if&lt;/span&gt;(bssrdf_intersection.IsInvalid())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;break&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Update the throughput
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;throughput *= bssrdf_evaluation / bssrdf_pdf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Evaluate direct illumination
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;ret += evaluate_light(intersection) * throughput;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Take another sample of direction for the bounce ray.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; bsdf_pdf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector dir;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;auto&lt;/span&gt; bsdf_evaluation = bssrdf_intersection.bsdf-&amp;gt;sample_f(&amp;amp;dir, &amp;amp;bsdf_pdf);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Update throughput
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;throughput *= bsdf_evaluation * dot(intersection.n, dir) / bsdf_pdf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Update the bounce ray
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;r = Ray(intersection.position , dir);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What it does is pretty self-explanatory. Below is a breakdown of computation it does in order to evaluate BSSRDF.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It first takes a sampled position near the shading point. The importance sampling of the diffusion profile happens under the hood.
One important detail to be noted here is that the interface &lt;strong&gt;sample_s&lt;/strong&gt; will silently add a fake brdf in the bsdf of the intersection for importance sampling the ray direction of the next iteration in the loop.&lt;/li&gt;
&lt;li&gt;If nothing is found, it will simply bail. This will be a source of bias and it is why SSS evaluation tends to tint with a different color at the thin parts of the object.&lt;/li&gt;
&lt;li&gt;Update the throughput, this line will count the attenuation caused by the distance between the shading point and sampled point. It is essentially  $ S_p(\omega_i, \omega_o) $ 
  mentioned previously. Please note that the first Fresnel part is already handled in the BXDF right before the condition.&lt;/li&gt;
&lt;li&gt;Starting from here, we need to evaluate both direct illumination and indirect illumination to take both into account.
&lt;ul&gt;
&lt;li&gt;Direct illumination evaluation is fairly simple in the way that we can take advantage of the existed code infrastructure.&lt;/li&gt;
&lt;li&gt;With the fake BRDF added previously, we can take a sample direction based on it. This direction will be used as the bounce ray for the next iteration in the loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is a lot of mathematics to derive to further prove what the above code does makes sense.
However, this is not the purpose of this blog. Instead of focusing on the mathematical theory, I would prefer to talk more about the real engineering problems that I encountered during my implementation.
So I will stop introducing the basics here and move forward with talking about my iterations.&lt;/p&gt;
&lt;h1 id=&#34;enhancements&#34;&gt;Enhancements&lt;/h1&gt;
&lt;p&gt;After my functional integration of PBRT&amp;rsquo;s subsurface scattering algorithm, I found that there are still more problems to solve.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sometimes there are too many fireflies.&lt;/li&gt;
&lt;li&gt;One SSS material can&amp;rsquo;t blend with other SSS materials.&lt;/li&gt;
&lt;li&gt;Subsurface scattering implementation is not flexible enough to be blended with other BRDFs.&lt;/li&gt;
&lt;li&gt;There is some space for better performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It appears that &lt;a href=&#34;https://www.cycles-renderer.org/&#34;&gt;Cycles&lt;/a&gt;, the internal offline renderer in &lt;a href=&#34;https://www.blender.org/&#34;&gt;Blender&lt;/a&gt;, has a more robust implementation.
Luckily, Cycles is also an open-source project. So I spend quite some time digging into what they improved over my integration from PBRT.
With lots of iterations, my SSS implementation is a lot more robust than it was at the beginning.&lt;/p&gt;
&lt;h2 id=&#34;reduce-fireflies&#34;&gt;Reduce Fireflies&lt;/h2&gt;
&lt;p&gt;The first and most annoying issue that I would like to solve was the fireflies.
These fireflies won&amp;rsquo;t simply go away as we bump up the number of samples per pixel.
With the SSS integration in my renderer, this was what I got at the beginning.
Please be noted that there is no coating layer and it is on purpose because I would like to avoid any other potential sources of fireflies.
This is already 1k samples and even by bumping the spp up to 8k, fireflies won&amp;rsquo;t be totally gone, if not getting worse.
The firefly problem renders the algorithm impractical in my renderer since it is so obvious.
Of course, we can also rely on some post-processing, like a denoiser, to mitigate the problem.
But it would be nice to know what the sources of the problems are and fix them for real.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/iteration0.png&#34; width=&#34;600&#34;/&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;dont-evaluate-sss-unnecessarily&#34;&gt;Don&amp;rsquo;t Evaluate SSS Unnecessarily&lt;/h3&gt;
&lt;p&gt;One observation that I made during my iteration was that the impact of SSS objects on its neighbors with SSS materials is very close to what a Lambert model has.
Taking the advantage of the assumption, I chose to silently replace SSS objects with lambert if the previous reflection model has SSS.
Below is a comparison of the same shot with and without the trick, the difference is barely noticeable to me.
It is worth mentioning that I clamped pixels with high radiance value to avoid fireflies in both shots.&lt;/p&gt;


















&lt;style&gt;
    .ba-slidera88c3b9bbf4073074127b0afa2b16857 {
        position: relative;
        overflow: hidden;
        width: 100%;
         
    }
    
    .ba-slidera88c3b9bbf4073074127b0afa2b16857 img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-slidera88c3b9bbf4073074127b0afa2b16857 .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-slidera88c3b9bbf4073074127b0afa2b16857 .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-slidera88c3b9bbf4073074127b0afa2b16857 .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-containera88c3b9bbf4073074127b0afa2b16857 {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-lefta88c3b9bbf4073074127b0afa2b16857 {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-lefta88c3b9bbf4073074127b0afa2b16857 {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-righta88c3b9bbf4073074127b0afa2b16857 {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-righta88c3b9bbf4073074127b0afa2b16857 {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centereda88c3b9bbf4073074127b0afa2b16857 {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-slidera88c3b9bbf4073074127b0afa2b16857&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-containera88c3b9bbf4073074127b0afa2b16857&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/regular_sss.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-righta88c3b9bbf4073074127b0afa2b16857&#34;&gt;Allow SSS to SSS bounces&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/disconnect_sss.png&gt;
        &lt;div class=&#34;ba-bottom-lefta88c3b9bbf4073074127b0afa2b16857&#34;&gt;Disallow SSS to SSS bounces&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;a88c3b9bbf4073074127b0afa2b16857&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;Clearly, this will avoid some importance sampling of BSSRDF and converts a BSSRDF to a simple lambert model.
Most BXDF importance sampling algorithms are commonly done locally without the knowledge of the surroundings.
Importance sampling algorithm used to randomly sample points on its nearby surfaces in this SSS implementation involves shooting short rays.
This is easily an order of magnitude more expensive than BXDF&amp;rsquo;s importance sampling algorithm.
However, given the rare cases where there are two consecutive SSS bounces in the same path, the differences in terms of computation in a practical scene like the above are close to minimal.
In some cases where SSS surfaces face each other, there is a greater chance for a path to have multiple consecutive samples on the same mesh with SSS. For example, like the one shown below.
Even in these cases, the performance gain is not too much, roughly 10% reduction in rendering speed.
However, the biggest gain in terms of performance is that it opens a door for our next iteration by making it possible, which we will cover later.&lt;/p&gt;


















&lt;style&gt;
    .ba-sliderdc405557b70643524e0e792520a81a8c {
        position: relative;
        overflow: hidden;
        width: 70%;
         
    }
    
    .ba-sliderdc405557b70643524e0e792520a81a8c img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-sliderdc405557b70643524e0e792520a81a8c .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-sliderdc405557b70643524e0e792520a81a8c .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-sliderdc405557b70643524e0e792520a81a8c .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-containerdc405557b70643524e0e792520a81a8c {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-leftdc405557b70643524e0e792520a81a8c {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-leftdc405557b70643524e0e792520a81a8c {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-rightdc405557b70643524e0e792520a81a8c {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-rightdc405557b70643524e0e792520a81a8c {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centereddc405557b70643524e0e792520a81a8c {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-sliderdc405557b70643524e0e792520a81a8c&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-containerdc405557b70643524e0e792520a81a8c&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/iteration1.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-rightdc405557b70643524e0e792520a81a8c&#34;&gt;After&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/iteration0.png&gt;
        &lt;div class=&#34;ba-bottom-leftdc405557b70643524e0e792520a81a8c&#34;&gt;Before&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;dc405557b70643524e0e792520a81a8c&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;The above two shots are rendered both with 1024 samples per pixel. We can easily notice that the optimization turns out to be very effective in terms of reducing fireflies, which was my primary reason for considering implementing it at the cost of slightly extra biased results.
Even with the SPPs bumped to 8k, the fireflies still doesn&amp;rsquo;t go away in the original implementation.
With the optimization, it almost gets rid of more than 90% of the total fireflies at no performance cost.
And we saw from the bunny example, it doesn&amp;rsquo;t have a big impact on the final result.
Of course, we can argue that the SSS to SSS bounces is not as many as it is in this example, the visual comparison of those shots may not represent the biggest potential bias we could eventually get.
But since the SSS algorithm has some pretty aggressive assumptions made at the very beginning, I would be willing to sacrifice a bit of accuracy to implement a reasonable SSS algorithm with limited fireflies.&lt;/p&gt;
&lt;h4 id=&#34;what-happens-under-the-hood&#34;&gt;What Happens Under the Hood&lt;/h4&gt;
&lt;p&gt;I think it is worth a little bit of explanation here to make it clear why such a trick can get rid of the majority of the fireflies.
To answer the question, it is necessary to figure out where do these fireflies come from?
By taking a deeper look into what happens here, I noticed that the volume inside the outer ball is not empty at all. It has multiple layers of geometries.
Following is a demonstration of what could happen inside the volume of the outer ball.
Of course, there are no three parallel surfaces in the outer ball mesh, but it does lead to rays bouncing between its surfaces, so this representation doesn&amp;rsquo;t lose its generality.
Upon the first intersection of the primary ray, it will pick three samples on the surface of the objects, some of which happen to be inside the volume and is kind of unfortunate.
It is unfortunate because the next sample it picks will also be from the exact same material with SSS.
As we can see from the path  $ P_0 \rightarrow P_{11} \rightarrow P_{2} \rightarrow P_{31} \rightarrow P_{4} \rightarrow P_{5} \rightarrow P_{63}$ 
 , which is essentially three bounces.
Please be noted that the transition from  $ P_0 $ 
 to  $ P_{11} $ 
 is not considered a bounce, it is just taking a sample of BSSRDF. Same goes true about the transition from  $ P_5 $ 
 to  $ P_{63} $ 
 .
The real problem behind this is that every single path is trying to make its way out, while only a very minority group of lucky paths will make it.
However, the ones made out will make an excessive amount of contribution to the evaluation since their PDF is extremely low.
In practice, the situation is way worse than just a few bounces.
I think there are a few reasons for this problem,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unnecessary complex internal structure of the outer ball.
There is no real solution for this unless we ask data authors to change the way they do things, which could be a big limitation to artists.&lt;/li&gt;
&lt;li&gt;The distance profile is nowhere near being physically accurate.
There is also no easy solution for this one. It because is those bold assumptions that make this algorithm feasible.&lt;/li&gt;
&lt;li&gt;Importance sampling does not always stick to the target function, which is the distance profile.
Although the Disney distance profile can be analytically sampled with a perfect PDF, the unknown shape of geometry easily makes the sampling algorithm less efficient. We will cover it later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Replacing SSS with lambert if the previous material has SSS will almost prevent this from happening. Because the path will be blocked when going from  $ P_{11} $ 
 to  $ P_2 $ 
 .
Rays won&amp;rsquo;t bounce between complex meshes with SSS materials.
Of course, light does bounce between complex shapes with SSS materials in reality, but without a good importance sampling algorithm and distance profile, this will easily lead to fireflies.
And considering the multi-bounces inside SSS materials don&amp;rsquo;t have the biggest impact on the final visual, I chose to ignore it by replacing SSS with lambert if necessary.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/bounces_diagram.png&#34;/&gt;
&lt;/figure&gt;

&lt;h4 id=&#34;the-catch-of-replacing-sss-with-lambert&#34;&gt;The Catch of Replacing SSS with Lambert&lt;/h4&gt;
&lt;p&gt;Given a specific path, depending on the direction of evaluation, the end result of rendering equation evaluation could be different.
Imagine there is a path like this
 $ E \rightarrow A \rightarrow B \rightarrow L $ 
 ,
where &lt;strong&gt;E&lt;/strong&gt; stands for eye, &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;B&lt;/strong&gt; stand for two points on surfaces with SSS, &lt;strong&gt;L&lt;/strong&gt; represents light. If the direction of evaluation is from the eye to the light, &lt;strong&gt;A&lt;/strong&gt; will be evaluated with SSS model, but &lt;strong&gt;B&lt;/strong&gt; will be a lambert model.
While if the direction of the evaluation goes the other way, &lt;strong&gt;B&lt;/strong&gt; will be evaluated with SSS, and &lt;strong&gt;A&lt;/strong&gt; will be simplified with lambert model. This clearly introduces some bias between path evaluation from different directions.
This might not be a problem at all for unidirectional path tracing or light tracing, where evaluation is always one direction. For algorithms like bidirectional path tracing, this will introduce an extra layer of bias.
Same reason here, considering the multiple sources of biases in the algorithm, I don&amp;rsquo;t mind having another one here.&lt;/p&gt;
&lt;h3 id=&#34;evaluate-at-all-intersections&#34;&gt;Evaluate at all Intersections&lt;/h3&gt;
&lt;p&gt;The above-mentioned trick allows the algorithm renders with SSS in a way that converges a lot faster.
Clearly, this is not a flawless solution since there are still quite some fireflies in the shot.
These fireflies must come from other sources in the sampling process.&lt;/p&gt;
&lt;p&gt;Fireflies commonly show up whenever the importance sampling algorithm is not very efficient.
In the beginning, I didn&amp;rsquo;t choose to believe it since that diffusion profile from the Disney &lt;a href=&#34;https://graphics.pixar.com/library/ApproxBSSRDF/paper.pdf&#34;&gt;paper&lt;/a&gt; has a perfect pdf function.
However, after taking a deeper look into the process, I found a problem that could cause it.&lt;/p&gt;
&lt;p&gt;There are clearly some other factors that will affect the efficiency of the importance sampling, like the light source.
It would be very hard, if not impossible, to take lights into account when taking samples with importance sampling algorithm.
But the multi-intersection nature of the algorithm does lead to some problems.
Like we mentioned before, there is a possibility that the projection ray will hit multiple surfaces with the same objects, instead of just one.
In these cases, PBRT will choose to uniformly pick one random intersection and divide the diffusion profile PDF by the number of intersections that the short ray hits in total.
This essentially adds a weighting factor for the original pdf.
Mathematically, the final pdf should be as below,&lt;/p&gt;

$$ P(X,R) = P_{Disney}(R) * P_{uniform}(X|R=r)$$
&lt;p&gt;R is the distance between the sampled point on the disk to the center of the disk.
X is the index of the point it picks among all intersections.
So we can clearly see from this equation, the Disney importance sampling pdf is just one part of the final pdf we have. The second pdf uniformly takes one intersection among all randomly, it is a conditional probability distribution.
And it is the efficiency of the final pdf that matters. Even if the first pdf is a perfect match of the original diffusion profile, the second extra pdf does have a chance to make it way less efficient. Not to mention, the first pdf is only a perfect pdf considering flat surfaces.&lt;/p&gt;
&lt;p&gt;To explain it in a more intuitive way, if we look at the image below.
The chances of sampling both red and green short rays are exactly the same since their distance to the center of the disk is the same.
However, we have five times more chances of picking the red point than any of the green ones even if the red point has more distance compared with any of the green ones.
To manually make things worse, if we place a light source near one of the green dots.
We will have a much bigger contribution by picking the green dot that is near to the light source with a five times lower chance of picking it.
This is a very typical example of why this importance sampling algorithm is not efficient in some cases. It can even get worse in practical situations.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/diagram_multi_intersection.png&#34; width=&#34;700px&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;It would be very hard to uniformly sample all intersections in this algorithm globally since we don&amp;rsquo;t know how many intersections there will be in total.
The truth is there is not even a number, it is a continuous domain.
But one thing that we can do to mitigate the problem is to avoid randomly picking one intersection among all but to evaluate all of them.
This was not possible before the first iteration since rays will grow exponentially without the above trick.
But since I have it implemented, I can totally consider evaluating all intersections instead of just randomly picking one of them.
Following is a comparison of the shots with and without this method, it is obvious how efficient it is after this iteration.
It is worth noting that instead of having the same spp this time, I choose to configure the settings differently so that the rendering time is roughly the same, which makes the comparison fairer.&lt;/p&gt;


















&lt;style&gt;
    .ba-slidera83234c266e188debc03a310fe6b6382 {
        position: relative;
        overflow: hidden;
        width: 70%;
         
    }
    
    .ba-slidera83234c266e188debc03a310fe6b6382 img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-slidera83234c266e188debc03a310fe6b6382 .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-slidera83234c266e188debc03a310fe6b6382 .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-slidera83234c266e188debc03a310fe6b6382 .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-containera83234c266e188debc03a310fe6b6382 {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-lefta83234c266e188debc03a310fe6b6382 {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-lefta83234c266e188debc03a310fe6b6382 {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-righta83234c266e188debc03a310fe6b6382 {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-righta83234c266e188debc03a310fe6b6382 {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centereda83234c266e188debc03a310fe6b6382 {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-slidera83234c266e188debc03a310fe6b6382&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-containera83234c266e188debc03a310fe6b6382&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/iteration2.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-righta83234c266e188debc03a310fe6b6382&#34;&gt;After&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/iteration1.png&gt;
        &lt;div class=&#34;ba-bottom-lefta83234c266e188debc03a310fe6b6382&#34;&gt;Before&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;a83234c266e188debc03a310fe6b6382&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;Given the result of the shot, I can hardly tell any firefly in it. This is already a firefly free solution to me.&lt;/p&gt;
&lt;h2 id=&#34;no-fresnel-above-sss-material&#34;&gt;No Fresnel above SSS material&lt;/h2&gt;
&lt;p&gt;It is common that renderers will introduce a coating layer to simulate a glossy surface with SSS under it to deliver a wax-like feeling. PBRT uses fresnel to pursue better physically-based accuracy. But it is also not uncommon to see articles mentioning SSS without the fresnel part. Strictly speaking, subsurface scattering should have no connection with the coating layer since they are not tightly couple all the time.
Also, because I have my plan to support flexible blending between bssrdf and bxdf in my material system, which will be covered later in this blog, I can totally approximate the fresnel part in my shader, instead of hardcode it in the bssrdf.&lt;/p&gt;
&lt;p&gt;Ideally, my goal here is to make sure I can guarantee this equation,&lt;/p&gt;

$$ \lim\limits_{s \rightarrow 0} \int_A \int_{\Omega} L(p_i, \omega_i) \space bssrdf( s, \omega_i, \omega_o ) cos(\theta_i) d\omega_i dA = \int_{\Omega} L(\omega_i) \space brdf_{lambert}(\omega_i, \omega_o) cos(\theta_i) d\omega_i$$
&lt;p&gt;The benefit of securing such an equation is that it allows me to have a proper transition between sss and lambert model without introducing a noticeable boundary in between.
This doesn&amp;rsquo;t sound like a big deal, but it does offer a bit more flexibility of data authoring.
For example, the shots below clearly demonstrate the differences. The mean free path goes to zero gradually as the position goes lower, eventually reaching zero somewhere around the neck of the monkey head. It is not too obvious, but we can notice the line at the bottom of the monkey head by taking a closer look. This is because the above equation is not true for the original implementation.&lt;/p&gt;


















&lt;style&gt;
    .ba-slider0662fea70917668a921a23d43c10e1da {
        position: relative;
        overflow: hidden;
        width: 70%;
         
    }
    
    .ba-slider0662fea70917668a921a23d43c10e1da img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-slider0662fea70917668a921a23d43c10e1da .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-slider0662fea70917668a921a23d43c10e1da .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-slider0662fea70917668a921a23d43c10e1da .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-container0662fea70917668a921a23d43c10e1da {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-left0662fea70917668a921a23d43c10e1da {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-left0662fea70917668a921a23d43c10e1da {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-right0662fea70917668a921a23d43c10e1da {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-right0662fea70917668a921a23d43c10e1da {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centered0662fea70917668a921a23d43c10e1da {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-slider0662fea70917668a921a23d43c10e1da&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-container0662fea70917668a921a23d43c10e1da&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/transition0.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-right0662fea70917668a921a23d43c10e1da&#34;&gt;Smooth Transition&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/transition1.png&gt;
        &lt;div class=&#34;ba-bottom-left0662fea70917668a921a23d43c10e1da&#34;&gt;No Smooth Transition&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;0662fea70917668a921a23d43c10e1da&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;The way I achieve this smooth transition is by killing the two Fresnel parts and adding a denominator of $ \pi $. So the equation of my BSSRDF goes like this,&lt;/p&gt;

$$ S(p_o, p_i, \omega_o, \omega_i) \approx \dfrac{S_p(p_o, p_i)}{\pi} $$
&lt;p&gt;As the mean free path length approaches to zero, the  $S_p(p_o, p_i)$ 
 becomes linearly proportional to &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirac_delta_function&#34;&gt;dirac-delta&lt;/a&gt; function. Also, as we know, a dirac-delta function will simplify integral in Monta Carlo estimation by dropping one dimension. The constant factor is what we need to figure out here. Since the pdf, which is proportional to this BSSRDF, the only difference between this BSSRDF and the pdf is the color tint part. Because when the mean free path approaches zero, it is very likely there is only one single intersection. This means that we don&amp;rsquo;t need to take the conditional probability density function into account. So the pdf and bssrdf have this connection&lt;/p&gt;

$$ S_p(p_o, p_i) = A * p(p_o,p_i) $$
&lt;p&gt;By taking a look at the Monte Carlo estimation, we got this&lt;/p&gt;

$$ \lim\limits_{s \rightarrow 0} S(p_o, p_i, \omega_o, \omega_i) \approx \lim\limits_{s \rightarrow 0} \Sigma {\dfrac{S_p(p_o, p_i)}{p(p_o,p_i)}} = \dfrac{A}{\pi} $$
&lt;p&gt;And dropping this equation back to the one we want to fulfill, it is obvious that this is a perfect match, which is also proved by the above shot.&lt;/p&gt;
&lt;h2 id=&#34;avoid-finding-intersection-with-other-meshes&#34;&gt;Avoid Finding Intersection with other Meshes&lt;/h2&gt;
&lt;p&gt;One detail in SSS implementation is to tell whether an intersection is a valid one during importance sampling of nearby positions.
PBRT considers it a valid intersection as long as the material is the same. In case there are two meshes with the same SSS material, they will find intersections on each other, which will result in incorrect approximation.&lt;/p&gt;
&lt;p&gt;In my renderer, I chose to instance subsurface scattering materials so that each mesh object has its own material instance with a unique id. This will avoid the problem. A material instance is just a thin wrapper of the original material, the memory overhead is close to minimal in most cases.&lt;/p&gt;
&lt;h2 id=&#34;refactor-material-system-in-my-renderer&#34;&gt;Refactor Material System in my Renderer&lt;/h2&gt;
&lt;p&gt;Prior to implementing sss in my renderer, it used to only have a linear combination of BXDFs in a data structure called BSDF. Strictly speaking, it is not just linearly blended BXDFs, since some BRDF, like Coat, can take a BSDF as an input argument. This converts the data into a tree. But this is irrelevant to the topic in this blog, we can ignore it.
To extend the system with sss features, I added a pointer for each material so that they can access more than BSDF data.
This can be easily integrated in the above pseudo-code. However, there are some limitations with this implementation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If a material has BSSRDF, it won&amp;rsquo;t be allowed to have other BXDF that is not relevant. The algorithm won&amp;rsquo;t work very well. It will always pick BSSRDF if one existed. This limitation means that we can&amp;rsquo;t blend BSSRDF with other BXDF stored in the BSDF.&lt;/li&gt;
&lt;li&gt;The system can only have one single BSSRDF since there is only one single pointer holding it. But sometimes, we might want to have multiple diffusion profiles blended together to achieve better subsurface scattering results, like skin rendering. The current system clearly doesn&amp;rsquo;t offer this flexibility.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to support flexible blending between BXDF and BSSRDF, I chose to refactor my material system. As we know that BSSRDF is a generalized version of BXDF. One option would be to derive all BXDF implementations from BSSRDF. This does stick to the truth in reality, but it immediately raised some problems in terms of coding. I don&amp;rsquo;t particularly like the design of this idea for two reasons&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The interface will be fatter with more arguments since we have more dimensions in BSSRDF. It might lead to less performance due to more arguments in call stack and in most cases where there is no subsurface scattering, lots of the arguments are not even used at all.&lt;/li&gt;
&lt;li&gt;The path tracing, bidirectional path tracing algorithms will have to treat everything like a BSSRDF, which won&amp;rsquo;t be efficient in terms of performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Eventually, I ended up with the other solution, both BXDF and BSSRDF derive from what I call scattering unit. All scattering units are held in a structure named scattering event, which is what I used to replace BSDF in my old system. The biggest difference introduced here is that scattering event can also hold multiple BSSRDFs along with BXDFs. A subtle difference here is that instead of holding an array of scattering units, the scattering event data structure chooses to hold two separate arrays, with one for BXDFs and the other for BSSRDFs, since they behave very differently compared with each other. This way it allows me to put the interfaces of BXDF and BSSRDF one level lower than their base class.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/class_diagram.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/class_diagram.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Above is a simplified class diagram that demonstrates what I implemented in my renderer.
It doesn&amp;rsquo;t have everything I have done in it. For example, there are only three BRDFs shown in this diagram, while I have implemented a lot more than this.
However, it gives a good explanation of what the big picture is.&lt;/p&gt;
&lt;p&gt;Again, there are a few adjustments needed to be done in my path tracing algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the third step shown in my pseudo-code, where we used to only evaluate BRDFs. We can choose to randomly pick one scattering unit from scattering event. The scattering unit can be either a BXDF or a BSSRDF. If a BXDF is chosen, we still stick to the old path. In the case of a BSSRDF chosen, the new algorithm will pick sampled positions and evaluate all of them to approximate the contribution from direct illumination.&lt;/li&gt;
&lt;li&gt;Just like what we did for step 3, we need to do similar things for step 4, where we used to only take direction samples from BXDF. Again, we randomly choose to have one scattering unit among all in the scattering event. If a BSSRDF is chosen, we first randomly pick a sampled position based on the BSSRDF model, and then we will recursively evaluate the indirect illumination for this part.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These two steps are what I eventually did in my renderer. But I guess it is also possible to merge the two steps in one by randomly picking between BXDF and BSSRDF once. Once one is picked, we use the type for both evaluation and importance sampling. I haven&amp;rsquo;t tried it in my renderer, it sounds to me it could work. There might be other problems though.&lt;/p&gt;
&lt;p&gt;Ideally, I would like to put the pseudo-code here too.
But in order to avoid a too much longer post, I chose to skip it since it is not that hard to implement.
For anyone, who is interested, &lt;a href=&#34;https://github.com/JiayinCao/SORT/blob/master/src/integrator/pathtracing.cpp&#34;&gt;here&lt;/a&gt; is the implementation in my renderer.&lt;/p&gt;


















&lt;style&gt;
    .ba-sliderade1b6f1ca02a923b999c7d0c1bbb77d {
        position: relative;
        overflow: hidden;
        width: 100%;
         
    }
    
    .ba-sliderade1b6f1ca02a923b999c7d0c1bbb77d img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-sliderade1b6f1ca02a923b999c7d0c1bbb77d .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-sliderade1b6f1ca02a923b999c7d0c1bbb77d .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-sliderade1b6f1ca02a923b999c7d0c1bbb77d .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-containerade1b6f1ca02a923b999c7d0c1bbb77d {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-leftade1b6f1ca02a923b999c7d0c1bbb77d {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-leftade1b6f1ca02a923b999c7d0c1bbb77d {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-rightade1b6f1ca02a923b999c7d0c1bbb77d {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-rightade1b6f1ca02a923b999c7d0c1bbb77d {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centeredade1b6f1ca02a923b999c7d0c1bbb77d {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-sliderade1b6f1ca02a923b999c7d0c1bbb77d&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-containerade1b6f1ca02a923b999c7d0c1bbb77d&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/digital_emily_1.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-rightade1b6f1ca02a923b999c7d0c1bbb77d&#34;&gt;Multiple SSS&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/digital_emily_0.png&gt;
        &lt;div class=&#34;ba-bottom-leftade1b6f1ca02a923b999c7d0c1bbb77d&#34;&gt;Single SSS&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;ade1b6f1ca02a923b999c7d0c1bbb77d&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;Above is an example that uses this feature to implement a skin shader. The comparison is single subsurface scattering versus multiple subsurface scattering.
The difference is very subtle, but it is visible in the dark area on her face.
Also, apart from blending subsurface scattering, this material also blends sss with regular brdf to simulate a bit of specular light on her face.
Below is the shader graph of this material used to render the shot. Click it for an enlarged image if it is not clear enough.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/shader_graph.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/shader_graph.png&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;optimizations&#34;&gt;Optimizations&lt;/h1&gt;
&lt;p&gt;Besides the enhancements that I did in my implementation, I also made a couple of optimizations that purely aim at improving performance. They don&amp;rsquo;t contribute to convergence rate or image quality that much, but making things faster is already a good reason to check them in.&lt;/p&gt;
&lt;h2 id=&#34;intersection-tests&#34;&gt;Intersection tests&lt;/h2&gt;
&lt;p&gt;SSS implementation mentioned in this blog requires finding the intersections with K nearest primitives. A naive way to get it done without introducing more complexity is to find the nearest intersection first and then adjust the ray origin a little bit off the surface along the ray direction, get the next intersection. The process keeps going until either K nearest intersections are found or it exceeds the allowed range of our interest.
This naive implementation is simple in the way that it doesn&amp;rsquo;t require any changes in the spatial acceleration structure. The algorithm to find the K nearest intersection is totally limited in SSS implementation itself. While it does comes at a cost of performing duplicated evaluation multiple times. Imagine in a case, where there are only five primitives in a leaf node, all of which intersect with the ray to be tested, and they are also the first five intersections. If our K is 4 in this case, we will have to perform the ray primitive intersection 20 times just to get the nearest 4 intersections in this leaf. This doesn&amp;rsquo;t even count the workload to iterate the traversal to the leaf node, which is also duplicated by 4 times.&lt;/p&gt;
&lt;p&gt;A slightly advanced approach to solving this problem is to introduce a dedicated interface to return K nearest intersections along the ray. This does require keeping a list of intersection history, which takes a bit of extra memory during intersection evaluation.
But it efficiently avoids all the above-mentioned duplicated computation during finding the K nearest intersections.  In my renderer, I chose to implement the algorithm in all of my spatial data structures, including kd-tree, BVH, OBVH, QBVH, uniform grid, octree.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/cartoon_head.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Above is another example generated in my renderer.
A big proportion of the image is covered by skin, which is essentially this subsurface scattering model.
But apart from these skin pixels, there are also other materials like metal, hair, and even a different primitive type.
In such a scene, this optimization reduces the total intersection by 14%, eventually improves the final rendering performance by 11.5%. For a pure subsurface scattering scene like the monkey head shot, performance gain can be as high as 14%.&lt;/p&gt;
&lt;h2 id=&#34;avoid-mis&#34;&gt;Avoid MIS&lt;/h2&gt;
&lt;p&gt;Whenever we find an intersection to keep iterating, we need to evaluate how much light coming in through that exit point.
Normally, we will take multiple samples from both light sources and the BRDF to avoid bad sampling, which could be caused by either a spiky shape BRDF or a tiny light source.
But we know for a fact that the BRDF here is lambert, there is no need to take samples from BRDF at all.
The &lt;a href=&#34;http://cseweb.ucsd.edu/~viscomp/classes/cse274/wi18/readings/veach_thesis.pdf&#34;&gt;thesis&lt;/a&gt; that introduced multiple importance sampling already made it clear that MIS doesn&amp;rsquo;t necessarily improve quality all the time.
We can simply take advantage of this to avoid taking unnecessary samples.
Taking samples from light sources works simply well enough.&lt;/p&gt;
&lt;p&gt;Ideally, this should get less shadow ray since MIS will require two separate shadow ray to be shot while sampling the light source only needs one. And this should mean that we have less computation to do.
Also, in the case of very small area light sources, MIS may have a slightly worse result than sampling light direction only since sampling from lambert brdf will almost always miss the light source.
However, my experimental data doesn&amp;rsquo;t indicate too much gain from this optimization. I still kept it in my renderer since I can&amp;rsquo;t justify any reason to take samples from brdf in this case and at least it doesn&amp;rsquo;t hurt anything.&lt;/p&gt;
&lt;h2 id=&#34;miscellaneous&#34;&gt;Miscellaneous&lt;/h2&gt;
&lt;p&gt;Apart from the above few optimizations I made, there are also some minor optimizations I did in my renderer.
They are not that important, but it is nice to have since they are fairly easy to implement.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ignore SSS after too many paths&lt;/strong&gt;&lt;br&gt;
Just like we have the maximum bounces of the main path, we can totally have another sperate control purely over subsurface scattering materials. This is just another finer control over the system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Avoid generating SSS when mean free path is 0 in all channels&lt;/strong&gt; &lt;br&gt;
Generally, subsurface scattering evaluation has a different branch in path tracing algorithm, which is more expensive in terms of performance. With the proper transition feature, I would prefer to silently replace them with lambert whenever mean free paths are zero. This is no catch of doing so.&lt;br&gt;
Another further optimization could be partially replacing subsurface scattering with lambert when the corresponding channel has mean free path that is zero. This idea takes advantage of the blending system that I did in my render so that it can blend a lambert with a subsurface scattering together.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-failed-attempt&#34;&gt;A Failed Attempt&lt;/h1&gt;
&lt;p&gt;A more aggressive way is to ignore SSS from all BRDFs without spiky shapes, like lambert.
This might sound like a reasonable optimization at the beginning. But it will destroy some nice things SSS delivers, like the color tint in the shadow of the second shots I showed in this blog.
A more extreme case is like the two shots below.
Though these are not the best demo cases for showing SSS features since both of the spheres look fairly flat. But it does clearly demonstrates the potential problem that could result from such aggressive optimization.
With a concrete wall roughly in the middle of the Cornell box, the sphere with SSS material is pretty much the only way that light can path through from the right side of the Cornell box to the left side.
If we ignore SSS from all non-spiky shape BRDF, we will lose all light contribution on the left side of the Cornell Box. Another slight subtle difference is the top right corner gets a bit stronger bounced light contribution. This is probably because SSS will lose some energy when it fails to find samples on the surface of the same mesh object.
I can&amp;rsquo;t say I care too much about the brighter bounce, but I definitely don&amp;rsquo;t like losing the nice feature of allowing light penetrating through SSS objects. Eventually, I didn&amp;rsquo;t choose to implement this in my renderer.&lt;/p&gt;


















&lt;style&gt;
    .ba-slider5d0af1e7f834029ae9f0b50b5b49da21 {
        position: relative;
        overflow: hidden;
        width: 70%;
         
    }
    
    .ba-slider5d0af1e7f834029ae9f0b50b5b49da21 img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-slider5d0af1e7f834029ae9f0b50b5b49da21 .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-slider5d0af1e7f834029ae9f0b50b5b49da21 .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-slider5d0af1e7f834029ae9f0b50b5b49da21 .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-container5d0af1e7f834029ae9f0b50b5b49da21 {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-left5d0af1e7f834029ae9f0b50b5b49da21 {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-left5d0af1e7f834029ae9f0b50b5b49da21 {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-right5d0af1e7f834029ae9f0b50b5b49da21 {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-right5d0af1e7f834029ae9f0b50b5b49da21 {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centered5d0af1e7f834029ae9f0b50b5b49da21 {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-slider5d0af1e7f834029ae9f0b50b5b49da21&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-container5d0af1e7f834029ae9f0b50b5b49da21&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/cornell_sss_wrong.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-right5d0af1e7f834029ae9f0b50b5b49da21&#34;&gt;No SSS after Lambert&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/cornell_sss.png&gt;
        &lt;div class=&#34;ba-bottom-left5d0af1e7f834029ae9f0b50b5b49da21&#34;&gt;Regular&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;5d0af1e7f834029ae9f0b50b5b49da21&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;Even if I would like to implement it, there is another problem. It is not entirely straightforward to categorize materials based on how rough they are.
PBRT has a hardcoded category for each of its BRDF. However, the ones like measured BRDF, will be very difficult to be hardcoded.
Not to mention there are things like Disney BRDF, which offers lots of flexibility with a gradual transition from being rough to smooth.
An even more extreme case is a rough material with a smooth coating layer on top.
There is just not a clear boundary between rough materials and smooth ones.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this blog, I mentioned my iterations during implementing subsurface scattering.
These iterations allow me to render subsurface scattering in a more efficient way.
Apart from the common subsurface scattering materials, like skin, wax, this algorithm can also be used to approximate juice sometimes, like in the image below. I believe there could be a better way to render this, but it is always good to know more applications of the algorithm.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_tips_for_implementing_subsurface_scattering_in_a_ray_tracer/orange_juice.png&#34; width=&#34;700&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;However, there are still some unsolved problems remained and potential improvements to be done in the future,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A separate spatial structure for each object with subsurface scattering will skip all of the traversal steps from the root all the way to the interior node that holds the object.&lt;/li&gt;
&lt;li&gt;It is not hard to notice that this algorithm will fail to converge to the correct result when the geometry is thin. For example, the head of the dragon shot is a bit dark because most short rays end up finding nothing, while they still have valid pdf. Somehow, Cycles solved the problem, it would be worth investigating.&lt;/li&gt;
&lt;li&gt;Subsurface scattering is not supported in my bidirectional path tracing algorithm. Adding support to it does sound challenging since bdpt is not an easy algorithm to start with. Making sure all MIS weights are correct while adding sss would be difficult.&lt;/li&gt;
&lt;li&gt;There are also numerous other methods for rendering subsurface scattering, like random walk sss. It would be fun to dive into the other algorithms and know the trade-off of these algorithms too.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://graphics.pixar.com/library/ApproxBSSRDF/paper.pdf&#34;&gt;Approximate Reflectance Profiles for Efficient Subsurface Scattering&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://library.imageworks.com/pdfs/imageworks-library-BSSRDF-sampling.pdf&#34;&gt;BSSRDF Importance Sampling&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://pbrt.org/&#34;&gt;Physically based rendering&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://graphics.stanford.edu/papers/bssrdf/bssrdf.pdf&#34;&gt;A Practical Model for Subsurface Light Transport&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://www.iryoku.com/separable-sss/downloads/Separable-Subsurface-Scattering.pdf&#34;&gt;Separable Subsurface Scattering&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;https://developer.nvidia.com/gpugems/gpugems3/part-iii-rendering/chapter-14-advanced-techniques-realistic-real-time-skin&#34;&gt;Advanced Techniques for Realistic Real-Time Skin Rendering&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&#34;https://www.cycles-renderer.org/&#34;&gt;Cycles Renderer&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&#34;https://graphics.stanford.edu/papers/bssrdf/bssrdf.pdf&#34;&gt;A Practical Model for Subsurface Light Transport&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&#34;https://therealmjp.github.io/posts/sss-intro/&#34;&gt;An Introduction To Real-Time Subsurface Scattering&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&#34;http://advances.realtimerendering.com/s2018/Efficient%20screen%20space%20subsurface%20scattering%20Siggraph%202018.pdf&#34;&gt;Efficient Screen-Space Subsurface Scattering Using Burley&amp;rsquo;s Normalization Diffusion in Real-Time&lt;/a&gt;&lt;br&gt;
[11] &lt;a href=&#34;https://agraphicsguynotes.com/posts/basics_about_path_tracing/&#34;&gt;Basics about path tracing&lt;/a&gt;&lt;br&gt;
[12] &lt;a href=&#34;http://cseweb.ucsd.edu/~viscomp/classes/cse274/wi18/readings/veach_thesis.pdf&#34;&gt;Robust Monte Carlo Methods For Light Transport Simulation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic Color Science For Graphics Engineer</title>
      <link>https://agraphicsguynotes.com/posts/basic_color_science_for_graphcis_engineer/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/basic_color_science_for_graphcis_engineer/</guid>
      <description>&lt;p&gt;For more than a decade, we have been doing HDR rendering in our game engines, which means the intermediate render targets won&amp;rsquo;t be limited by the precision of the color formats. It is an even more important concept after the emerging of physically based rendering, which is almost what everyone does these days. However, after so much effort rendering everything in linear color space, it is quite wasteful that we can only display colors with only limited chromaticity and luminance defined by sRGB space due to limitations of LDR monitor and TVs.&lt;/p&gt;
&lt;p&gt;With HDR monitors and TVs become more and more affordable, game industry has never been so serious about colors like it is now. It is almost a standard feature for modern AAA games. Luckily enough, I got a chance to work on supporting HDR monitor/TV for our game &lt;a href=&#34;https://skullandbones.ubisoft.com/game/en-us/home/&#34;&gt;Skull &amp;amp; Bones&lt;/a&gt; these days. With limited knowledge about HDR monitor/TV support, I spent lots of my time this week learning some basic color science, which is almost mandatory for me to understand the whole thing. And I learned lots of interesting stuff by doing some really basic research in this field. Most importantly, I strongly believe that it won&amp;rsquo;t be long before game studios deploy the whole development pipeline in Rec.2020 color space for a wider gamut and better HDR once HDR monitors become more affordable. It is better for me to blog it before I forget everything so that it could be easy for me to pick up in the future.&lt;/p&gt;
&lt;h1 id=&#34;colors-are-way-more-than-rgb&#34;&gt;Colors are way more than RGB&lt;/h1&gt;
&lt;p&gt;Unless we do real spectrum rendering, which is even rare in offline ray tracing, RGB is the most common representation that we use to represent a color. However, colors are way more complex than just three numbers.
In order to understand colors, the first concept that we need to understand is spectral power distribution.&lt;/p&gt;
&lt;p&gt;The light we see every day is composed of lighting signals of different spectrums. Human beings can only see lights with its wavelength ranging from 380 nm to 750 nm, which is a very conservative range.  Rays beyond the range exist, but it is not directly visible to a human being so that we usually don’t care about them in rendering.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/g.jpg&#34; width=&#34;700&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;It is clearly shown the wavelength of visible lights is only a tiny fraction among all wavelengths. Rays like gamma rays, X rays, ultraviolet rays are those with smaller wavelength than the visible range. And infrared, microwaves, radios all have much larger wavelengths. Our focus here is obviously the range starting from 400 nm to 750 nm. Notice, this range is different from what I mentioned before because it was a very conservative range mentioned above, this range is also considered fine for rendering. As a matter of fact, PBRT only considers ranges starting from 400 nm to 700 nm, which is even smaller. So it is not uncommon for you to see ranges with different values in different places, as long as it generally covers the range as a whole, there should be no big problem.&lt;/p&gt;
&lt;p&gt;The concept of spectral color is defined as color that is evoked by a normal human by a single wavelength in the visible spectrum. Basically, it is quite rare for us to see spectral color in our life, almost all colors we see is composed of multiple spectral colors. One way to visualize it in a more intuitive manner is to shoot a white light to a prism. Because the index of refraction is a function of wavelength, the white light will be decomposed into multiple beams of lights.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/prism.jpeg&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;It is not hard to imagine if we project the light luminance signal on different wavelengths, we will have a curve, which reveals the spectral power distribution. This is a very common way to represent a color in a self-explanatory manner. Following is an SPD of a light source.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/spectrum.png&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;cie-chromaticity-diagram&#34;&gt;CIE Chromaticity Diagram&lt;/h1&gt;
&lt;p&gt;Although some companies do full spectrum rendering in the movie industry, it is rare that anyone attempt rendering something with full spectrum in real time rendering because it is very expensive for interactive rendering. To make things easier for us, color scientists have figured out a way to represent color in a very convenient way to represent color without too much loss of precision.&lt;/p&gt;
&lt;p&gt;To see how it works, it is inevitable that we need to look into how human being sees things. Basically, inside our eyes, there are rods and cones on our retina. Rods are responsible for vision at low light levels. Cones are active at higher light levels and capable of color vision. There are three different types of cone receptors in our eyes, with each of them sensitive to light around specific wavelength. One of these types is most sensitive to long wavelengths, around 600 nm, which appear reddish to us. The sensitivity of the second type of cones peaks at the medium wavelength, 550nm, which appear greenish. The last receiptor peaks in the short wavelengths, 450nm, and it appears blueish. Luckily enough, with only three cone receptors in our eye, it is usually good enough to use three numbers to represent colors, the commonly well-known R/G/B.&lt;/p&gt;
&lt;p&gt;A color matching experiment works by picking three primaries first as the basis for the whole color space. These primaries contain single wavelength at 615nm, 525nm, 445nm. And for another light containing a single wavelength, there will be three parameters controlling the intensity of each primary to be tuned so that the combined color by mixing the three primaries with correct intensity appears exactly the same to human beings. Keeping doing it for all colors containing one single wavelength ranging in human being visible wavelengths range, we will get a list of numbers, which we can also plot on a two-dimensional space for better visualization of the data.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/the-color-matching-functions.png&#34; width=&#34;700&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Above is an illustration of what the plot will look like across the whole visible wavelength range. One interesting fact we can easily notice is that there are some regions where we need a negative intensity for the primary. This is not physically plausible. What happened is that there is no way to reproduce the light at those wavelengths by simply adding colors containing the three primaries together. We have to ‘cheat’ by ‘subtracting’ colors to make it happen, since there is no real subtracting, what they did was to add color on the target color side to make it happen.&lt;/p&gt;
&lt;p&gt;Another way to visualize the data is to plot it in three-dimensional space. Imagine we have a normalized orthogonal space, with R/G/B as the three basis. We can plot all the above points in 3D space constructed by RGB. We can easily get something like this,&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Notice that the curve crosses the BR plane, meaning they have some negative values along the ranges. As a matter of fact, they also have some negative value for the green channel, it is just a little bit hard to see from the above image.&lt;/p&gt;
&lt;p&gt;Because sometimes, people only need to care about the hue and saturation of the color, ignoring the intensity, meaning we can reduce the dimensionality by dropping one dimension. This is usually done by projecting the above curve onto the plane r+g+b=1. Another way to think about it is to shoot a ray from the original to every point on the curve, find the intersection set of these rays with the plane r+g+b=1, which is also a curve. Below is an image demonstrating the idea,&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image2.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Since we have the constraint r+g+b = 1, working in 3D space is kind of unnecessarily complex because we can always reconstruct the original signal in 2D space by utilizing the constraint. That said, we can project this curve on RG plane without caring about B, which usually matches intensity. And then we have this curve on 2D space,&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image3.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;This curve enclosure colors with all chromaticity are visible to human beings. However, that is not to say this is displayable with RGB system because some of them have negative value. The displayable area is the grey area. And unfortunately, this is not even the displayable area in most TV/monitors, which is usually smaller than it. We will mention it later.&lt;/p&gt;
&lt;p&gt;The fact we have some negative area will easily cause some trouble. And some really smart people come up with this idea, instead of using the primaries mentioned above, why don’t we use some imaginary color as the basis. Here comes the XYZ axis, which is purely imaginary, not physically plausible. The derivation of XYZ axis is out of the scope of this blog. However, since all visible color range lies in the positive area, XYZ is a pretty common color basis when we talk about color science.&lt;/p&gt;
&lt;p&gt;Although, there is no way to do the same color matching experiment with XYZ primaries because they are not physically plausible. But we can still transform the original data to the new XYZ space and generate the curve in the new space.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image4.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Above is the plot for the color matching result in XYZ space, these are all positive values, which is a very important reason to introduce XYZ space in the first place. The same way we plot the samples in 3D space as above, we can do the same for these samples, the only difference is the basis now are XYZ. Only the final projected color range will be shown here for simplicity. However, it is strongly suggested to check out &lt;a href=&#34;https://graphics.stanford.edu/courses/cs178/applets/threedgamut.html&#34;&gt;this page&lt;/a&gt; for a better feel of this data representation in 3D space.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image5.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;The professional term for the above curve is called ‘chromaticity diagram’. There is one type of &lt;a href=&#34;https://en.wikipedia.org/wiki/CIELUV&#34;&gt;variation&lt;/a&gt; of this chromaticity diagram, but this is the most commonly used one. One good thing about this chromaticity diagram is that it allows people from different industry to talk about color in a unified standard. For example, when people talk about a specific color in the above diagram, it is absolutely independent of their hardware used to display, generate the color.&lt;/p&gt;
&lt;h1 id=&#34;color-spaces&#34;&gt;Color Spaces&lt;/h1&gt;
&lt;p&gt;Since we already have a place to talk about device independent colors, it can well serve the background as discussions about color spaces. We will only mention the tri-stimulus system, that said only color space with three primaries will be considered here.&lt;/p&gt;
&lt;p&gt;There are lots of color spaces out there, sRGB, Adobe RGB, Rec 709, Rec 2020, etc. Since we are only interested in computer graphics, only relevant ones will be mentioned here. But before talking about the color spaces, one important concept is called white point, which deserves our attention here.&lt;/p&gt;
&lt;h2 id=&#34;white-point&#34;&gt;White Point&lt;/h2&gt;
&lt;p&gt;When we define a color space, it is quite common to specific their primaries coordinate in the chromaticity diagram. However, as we mentioned before, the chromaticity diagram is already a diagram of two-dimension by losing the intensity of the original signal. Then defining just the coordinate of the primaries in the chromaticity diagram is not good enough because there is not enough signal to reconstruct the color space. To put it simply, the scale of each primary is still unknown even if the coordinate in the chromaticity diagram is defined. Instead of defining the scaling factor for each of the primaries directly, people choose to define an extra point called ‘white point’ to implicitly define them.&lt;/p&gt;
&lt;p&gt;One important insight to be aware of is that the interpolation on chromaticity diagram is by no means linear after the projections. Just think about the perspective correction in rasterization, it works similarly. Usually, we use (1,1,1) as the white color in the system. However, the white color projection on x+y+z=1 plane is not exactly the centroid ( center of gravity ) of the triangle defined by the primaries in the chromaticity diagram. In order to calculate the chromaticity of the white color, we need the scaling factor for all the three primaries. Just to make it clear, the white point calculation is not our interest here, this is just a clear example to show things. We are more interested in the scaling factor than the white point itself, not to mention in the color space definition, the white point is usually explicitly well defined.&lt;/p&gt;
&lt;p&gt;To calculate the three scaling factor, we need to introduce one constraint, that is the Y channel of R+G+B should be exactly 1. With that in mind, if we already have the white point defined in the color space definition the same way the other three primaries are defined, 2D coordinate in the chromaticity diagram. We can easily project the white point back to the XYZ space by scaling the vector so that the Y coordinate is exactly one. And then we have the following equation,&lt;/p&gt;

$$ S_r * R_{xyz} &amp;#43; S_g * G_{xyz} &amp;#43; S_b * B _{xyz} = \dfrac{W_{xyz}}{W_y} $$
&lt;p&gt;It is very straightforward and easy to solve the equation above,&lt;/p&gt;

$$ \begin{bmatrix} S_r \\ S_g \\ S_b \end{bmatrix} = \begin{bmatrix} \dfrac{W_x}{W_y} \\ 1.0 \\ \dfrac{W_z}{W_y} \end{bmatrix} * { \begin{bmatrix} R_x &amp;amp; R_y &amp;amp; R_z \\ G_x &amp;amp; G_y &amp;amp; G_y \\ B_x &amp;amp; B_y &amp;amp; B_z \end{bmatrix} }^{-1} $$
&lt;h2 id=&#34;rec-709&#34;&gt;Rec. 709&lt;/h2&gt;
&lt;p&gt;Rec. 709, also known as BT. 709, is the standard for HD TVs. The first edition of this standard was approved in 1990. Check this page for futher detail.&lt;/p&gt;
&lt;p&gt;Following are the primaries for Rec. 709,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R – ( 0.64 , 0.33 )&lt;/li&gt;
&lt;li&gt;G – ( 0.30 , 0.60 )&lt;/li&gt;
&lt;li&gt;B – ( 0.15 , 0.06 )&lt;/li&gt;
&lt;li&gt;W – ( 0.3127 , 0.3290 )&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image6.png&#34; width=&#34;450&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;The above image clearly shows the coverage of Rec.709 gamut. All colors inside the triangle are displayable by sRGB standard, LDR devices won’t be able to reproduce the color outside the triangle.
The transformation from the CIE XYZ tristimulus color space into linear Rec.709 space can be calculated by means of a 3×3 matrix,&lt;/p&gt;

$$ \begin{bmatrix} R_{rec709} \\ G_{rec709} \\ B_{rec709} \end{bmatrix} = { \begin{bmatrix} 3.2404542 &amp;amp; -1.5371385 &amp;amp; -0.4985314 \\ -0.9692660 &amp;amp; 1.8760108 &amp;amp; 0.0415560 \\ 0.0556434 &amp;amp; -0.2040259 &amp;amp; 1.0572252 \end{bmatrix}} * \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} $$
&lt;p&gt;On the other side, it is easy to define the reversed transformation to transform color from Rec. 709 space to XYZ space.&lt;/p&gt;

$$ \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = { \begin{bmatrix} 0.4124564 &amp;amp; 0.3575761&amp;amp; 0.1804375 \\ 0.2126729 &amp;amp; 0.7151522 &amp;amp; 0.0721750 \\ 0.0193339 &amp;amp; 0.1191920 &amp;amp; 0.9503041 \end{bmatrix}} * \begin{bmatrix} R_{rec709} \\ G_{rec709} \\ B_{rec709} \end{bmatrix}  $$
&lt;p&gt;Apart from the primaries, the transfer function is also defined by the standard.&lt;/p&gt;

$$ C_{709}&amp;#39;= \begin{cases} 4.5C_{709}&amp;amp;0&amp;lt;=C_{709}&amp;lt;0.0018 \\ 1.099C_{709}^{1/\gamma} - 0.099 &amp;amp; 0.018&amp;lt;=C_{709}&amp;lt;1 \end{cases} $$

$$ \gamma = 2.2 $$
&lt;h2 id=&#34;srgb&#34;&gt;sRGB&lt;/h2&gt;
&lt;p&gt;sRGB is the most commonly used color space in game development. It was created by HP and Microsoft together in 1996 for display on monitors, printer and the internet.&lt;/p&gt;
&lt;p&gt;The primaries of sRGB are exactly the same with Rec.709. So is the transformation matrix between to transform data from XYZ space. However, it has slightly different transfer function comparing with Rec.709.&lt;/p&gt;

$$ C_{sRGB}&amp;#39;= \begin{cases} 12.92C_{sRGB}&amp;amp;0&amp;lt;=C_{sRGB}&amp;lt;0.0031 \\ 1.055C_{sRGB}^{1/\gamma} - 0.055 &amp;amp;0.031&amp;lt;=C_{sRGB}&amp;lt;1 \end{cases} $$

$$ \gamma = 2.4 $$
&lt;p&gt;Sadly, sRGB is the most commonly used color space in gaming, even if it only covers only 35.9% of the whole CIE 1931 Color space. Basically, when we talked about physically based shading, an important concept is linear color space. Color can be linear in several different color spaces, the common linear color space we used in our rendering engine is most likely sRGB. This is mostly because most developers use LDR monitor for their daily work. And sRGB was the industry standard for LDR monitors. Unless everyone, at least relevant people, is geared with HDR monitor, which is obviously an investment for a game studio, linear color space is always in sRGB. Even if there is an HDR monitor for everyone in the studio, transiting the whole rendering engine from sRGB to Rec.2020 still involves some work.&lt;/p&gt;
&lt;p&gt;What most people do is to perform rendering equation evaluation in sRGB linear color space and then convert the data to Rec.2020 for higher dynamic range. Technically speaking, there is no way to generate any color out of sRGB gamut even if they are converted to Rec.2020. But some game engines will perform color grading in Rec.2020, which will make the color more saturated. In this case, there is some possibility to generate color outside sRGB gamut.&lt;/p&gt;
&lt;h2 id=&#34;rec-2020&#34;&gt;Rec. 2020&lt;/h2&gt;
&lt;p&gt;Rec.2020 is the color space of the future. It is designated color space for ultra high TV, or UHDTV. It gets the name from the standards classification: ITU-R Recommendation BT.2020.&lt;/p&gt;
&lt;p&gt;The primaries of this color space are equivalent to monochromatic light sources on the CIE 1931 spectral locus. The wavelength of Rec.2020 primary colors is 630nm for the red primary color, 532nm for the green primary color, and 467nm for the blue primary color. They are defined as follows,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R – ( 0.708 , 0.292 )&lt;/li&gt;
&lt;li&gt;G – ( 0.170 , 0.797 )&lt;/li&gt;
&lt;li&gt;B – ( 0.131 , 0.046 )&lt;/li&gt;
&lt;li&gt;W – ( 0.3127 , 0.3290 )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following is an image demonstrating the gamut of this color space,&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basic_color_science_for_graphcis_engineer/image7.png&#34; width=&#34;450&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;There are two things that are very obvious. They share the same white point. Rec. 2020 covers way bigger area than Rec.709/sRGB, the coverage goes from 35.9% to 75.80%.&lt;/p&gt;
&lt;p&gt;Similiarly, we can easily calculate the transformation matrix transforming color between XYZ space and Rec.2020 space.&lt;/p&gt;

$$ \begin{bmatrix} R_{rec2020} \\ G_{rec2020} \\ B_{rec2020} \end{bmatrix} = { \begin{bmatrix} 1.7166512 &amp;amp; -0.3556708 &amp;amp; -0.2533663 \\ -0.6666844 &amp;amp; 1.6164812 &amp;amp; 0.0157685 \\ 0.0176399 &amp;amp; -0.0427706 &amp;amp; 0.9421031 \end{bmatrix}} * \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} $$

$$ \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = { \begin{bmatrix} 0.6369580 &amp;amp; 0.1446169&amp;amp; 0.1688810 \\ 0.2627002 &amp;amp; 0.6779981 &amp;amp; 0.0593017 \\ 0.0000000 &amp;amp; 0.0280727 &amp;amp; 1.0609851 \end{bmatrix}} * \begin{bmatrix} R_{rec2020} \\ G_{rec2020} \\ B_{rec2020} \end{bmatrix}  $$
&lt;p&gt;Perceptual Quantizer is the very common transfer function used to encode the color before feeding into the display. It is like a new ‘gamma correction’. Please refer to this page for detail of this transfer function.&lt;/p&gt;
&lt;h1 id=&#34;wrap-up&#34;&gt;Wrap Up&lt;/h1&gt;
&lt;p&gt;As HDR monitors are becoming more and more affordable. It is not hard to imagine that in the near future there will be more HDR content available in the future, whether gaming or movie. However, creating those content usually involves understanding these very fundamental knowledge about color spaces.&lt;/p&gt;
&lt;p&gt;The content mentioned in this article is just what I collected in the past few weeks. It barely touches the surface of the whole topic. In order to implement good support for HDR in games, there are way more to do than understanding what is mentioned above. Anyway, I still hope this blog could be helpful to someone who just started the journey like I did a few weeks ago.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://medium.com/hipster-color-science/a-beginners-guide-to-colorimetry-401f1830b65a&#34;&gt;A Beginner’s Guide to (CIE) Colorimetry&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://www.youtube.com/watch?v=LQlJGUcDYy4&#34;&gt;HDR Rendering in Lumberyard&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://www.youtube.com/watch?v=OvLuQliiJlg&amp;amp;t=903s&#34;&gt;HDR Ecosystems for Games&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;http://brucelindbloom.com/index.html?Eqn_RGB_XYZ_Matrix.html&#34;&gt;RGB/XYZ Matrices&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;https://en.wikipedia.org/wiki/Color_balance&#34;&gt;White Balancing&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;https://www.youtube.com/watch?v=iDsrzKDB_tA&#34;&gt;Color Vision 1 Color Basis&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&#34;https://www.youtube.com/watch?v=82ItpxqPP4I&amp;amp;t=634s&#34;&gt;Color Vision 2 Color Matching&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&#34;https://www.youtube.com/watch?v=KDiTxWcD3ZE&amp;amp;t=2s&#34;&gt;Color Vision 3 Color Map&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&#34;https://www.youtube.com/watch?v=V73k_0KuUJo&amp;amp;t=8s&#34;&gt;Color Vision 4 Cones to see color&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&#34;https://developer.nvidia.com/implementing-hdr-rise-tomb-raider&#34;&gt;Implementing HDR in Rise of the Tomb Raider&lt;/a&gt;&lt;br&gt;
[11] &lt;a href=&#34;https://developer.nvidia.com/preparing-real-hdr&#34;&gt;Prepare for Real HDR&lt;/a&gt;&lt;br&gt;
[12] &lt;a href=&#34;https://research.activision.com/t5/Publications/HDR-in-Call-of-Duty/ba-p/10744846&#34;&gt;HDR Display in Call of Duty&lt;/a&gt;&lt;br&gt;
[12] &lt;a href=&#34;http://www.tftcentral.co.uk/articles/pointers_gamut.htm&#34;&gt;The Pointer’s Gamut&lt;/a&gt;&lt;br&gt;
[13] &lt;a href=&#34;https://www.youtube.com/watch?v=l8_fZPHasdo&#34;&gt;How do human being see things&lt;/a&gt;&lt;br&gt;
[14] &lt;a href=&#34;https://graphics.stanford.edu/courses/cs178/applets/locus.html&#34;&gt;Introduction to Color Theory&lt;/a&gt;&lt;br&gt;
[15] &lt;a href=&#34;https://graphics.stanford.edu/courses/cs178/applets/colormatching.html&#34;&gt;Color Matching&lt;/a&gt;&lt;br&gt;
[16] &lt;a href=&#34;https://graphics.stanford.edu/courses/cs178/applets/threedgamut.html&#34;&gt;Chromaticity Diagrams&lt;/a&gt;&lt;br&gt;
[17] &lt;a href=&#34;https://graphics.stanford.edu/courses/cs178/applets/gamutmapping.html&#34;&gt;Gamut Mapping&lt;/a&gt;&lt;br&gt;
[18] &lt;a href=&#34;https://www.image-engineering.de/library/technotes/714-color-spaces-rec-709-vs-srgb&#34;&gt;Color spaces – rec.709 vs sRGB&lt;/a&gt;&lt;br&gt;
[19] &lt;a href=&#34;https://en.wikipedia.org/wiki/SRGB&#34;&gt;sRGB&lt;/a&gt;&lt;br&gt;
[20] &lt;a href=&#34;https://en.wikipedia.org/wiki/Rec._709&#34;&gt;Rec. 709&lt;/a&gt;&lt;br&gt;
[21] &lt;a href=&#34;https://en.wikipedia.org/wiki/CIE_1931_color_space&#34;&gt;CIE 1931 color space&lt;/a&gt;&lt;br&gt;
[22] &lt;a href=&#34;https://wolfcrow.com/say-hello-to-rec-2020-the-color-space-of-the-future/&#34;&gt;Say Hello to Rec.2020, the Color Space of the Future&lt;/a&gt;&lt;br&gt;
[23] &lt;a href=&#34;https://en.wikipedia.org/wiki/High-dynamic-range_video&#34;&gt;High-dynamic-range video&lt;/a&gt;&lt;br&gt;
[24] &lt;a href=&#34;https://en.wikipedia.org/wiki/CIELUV&#34;&gt;CIELUV&lt;/a&gt;&lt;br&gt;
[25] &lt;a href=&#34;http://www.russellcottrell.com/photo/matrixCalculator.htm&#34;&gt;The RGB-XYZ Matrix Calculator&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Anisotropic Microfacet BRDF</title>
      <link>https://agraphicsguynotes.com/posts/sample_anisotropic_microfacet_brdf/</link>
      <pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/sample_anisotropic_microfacet_brdf/</guid>
      <description>&lt;p&gt;I am working on material system in my renderer recently. My old implementation of microfacet models only supports isotropic BRDF, as a result of which, it can&amp;rsquo;t render something like brushed metals in my renderer. After spending three days in my spare time to extend the system to support anisotropic microfacet BRDF, I easily noticed how much mathematics that it needs to understand all the importance sampling methods. The fact that $ \theta $ and $ \phi $ are somewhat correlated makes the importance sampling a lot more complex than isotropic model. For a detailed derivation of isotropic microfacet importance sampling, please check out my previous &lt;a href=&#34;https://agraphicsguynotes.com/posts/sample_microfacet_brdf/&#34;&gt;blog&lt;/a&gt;. I strongly suggest checking it out if the math formula in this blog confuses you because there are a lot of basics that I won&amp;rsquo;t mention in this blog.&lt;/p&gt;
&lt;p&gt;It is gonna be very boring to go through the whole blog. In case you are already bored, here is what you can achieve with the BRDF model. Hopefully, this image can convince you reading through it.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/sample_anisotropic_microfacet_brdf/anisotropic_brdf.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/sample_anisotropic_microfacet_brdf/anisotropic_brdf.png&#34; width=&#34;700&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;sampling-ggx&#34;&gt;Sampling GGX&lt;/h1&gt;
&lt;p&gt;Following is the formula of GGX:&lt;/p&gt;

$$ D(h) = \dfrac{1}{\pi \alpha_u \alpha_v cos^4(\theta) \Big( 1 &amp;#43;tan^2(\theta) \Big( {\frac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\frac {sin^2(\phi) }{\alpha_v^2 }}\Big) \Big) ^ 2 } $$
&lt;p&gt;Then the pdf that we choose to sample this function is:&lt;/p&gt;

$$ p(\theta,\phi) = \dfrac{sin(\theta)}{\pi \alpha_u \alpha_v cos^3(\theta) \Big( 1 &amp;#43; tan^2(\theta) \Big( {\frac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\frac {sin^2(\phi) }{\alpha_v^2 }} \Big) \Big) ^ 2 } $$
&lt;p&gt;Since this is a joint density probability function of $ \phi $ and $ \theta $, we need to take one sample based on the marginal density probability function first and then take the second sample based on the conditional density function. Let&amp;rsquo;s take a look at the CDF for $ \theta $ first.&lt;/p&gt;

$$ P_{\theta}(\theta,\phi) = \int_0^{\theta} p(t,\phi) d(t) = \int_0^{\theta} \dfrac{sin(t)}{\pi \alpha_u \alpha_v cos^3(t) \Big( 1 &amp;#43; tan^2(t) \Big( {\frac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\frac {sin^2(\phi) }{\alpha_v^2 }} \Big)\Big) ^ 2 } d(t)$$
&lt;p&gt;Since we are not integrating $ \phi $, I&amp;rsquo;d like to define a helper term to make the whole derivation shorter.&lt;/p&gt;

$$ A(\phi) = {\dfrac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\dfrac {sin^2(\phi) }{\alpha_v^2 }} $$
&lt;p&gt;And then $ P_{\theta}(\theta,\phi) $ becomes this:&lt;/p&gt;

$ \begin{array} {lcl} P_{\theta}(\theta,\phi) &amp;amp;=&amp;amp; \int_0^{\theta} \dfrac{sin(t)}{\pi \alpha_u \alpha_v cos^3(t) ( 1 &amp;#43; tan^2(t) A(\phi) ) ^ 2 } d(t) \\\\ &amp;amp; = &amp;amp; \dfrac{1}{\pi \alpha_u \alpha_v} \int_0^{\theta} \dfrac{-1}{cos^3(t) ( 1 &amp;#43; tan^2(t) A(\phi) ) ^ 2 } d(cos(t)) \\\\ &amp;amp; = &amp;amp; \dfrac{1}{\pi \alpha_u \alpha_v} \int_0^{\theta} \dfrac{-cos(t)}{cos^4(t) ( 1 &amp;#43; tan^2(t) A(\phi) ) ^ 2 } d(cos(t)) \\\\ &amp;amp; = &amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v} \int_0^{\theta} \dfrac{-1}{ ( cos^2(t) &amp;#43; sin^2(t) A(\phi) ) ^ 2 } d(cos^2(t))  \\\\ &amp;amp;= &amp;amp;  \dfrac{1}{2 \pi \alpha_u \alpha_v} \int_0^{\theta}  \dfrac{-1}{ ( cos^2(t) ( 1 - A(\phi) ) &amp;#43; A(\phi) ) ^ 2 } d(cos^2(t)) \\\\ &amp;amp;= &amp;amp;  \dfrac{1}{2 \pi \alpha_u \alpha_v ( 1 - A(\phi) ) } \int_0^{\theta} \dfrac{-1}{ ( cos^2(t) ( 1 - A(\phi) ) &amp;#43; A(\phi) ) ^ 2 } d( ( 1 - A(\phi) ) cos^2(t)) \\\\ &amp;amp;= &amp;amp;  \dfrac{1}{2 \pi \alpha_u \alpha_v ( 1 - A(\phi) ) } \int_0^{\theta} d \Big( \dfrac{1}{ cos^2(t) ( 1 - A(\phi) ) &amp;#43; A(\phi) } \Big) \\\\ &amp;amp;= &amp;amp;  \dfrac{1}{2 \pi \alpha_u \alpha_v ( 1 - A(\phi) ) } \Big( \dfrac{1}{ cos^2(\theta) ( 1 - A(\phi) ) &amp;#43; A(\phi) } - 1 \Big) \end{array} $
&lt;p&gt;Since there is still $ \phi $ in the above CDF, we can not just use the inversion method here directly. However, this above function is gonna help us defining the marginal probability density function for $ \phi $ and conditional probability density function for $ \theta $. Following is the marginal probability density function for $ \phi $:&lt;/p&gt;

$$ \begin{array} {lcl} p_{\phi}(\phi) &amp;amp;=&amp;amp; \int_0^{0.5 \pi} p(\theta,\phi) d(\theta) \\\\ &amp;amp;=&amp;amp; P_{\theta}( 0.5\pi , \phi ) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v ( 1 - A(\phi) ) } \Big( \dfrac{1}{ cos^2( 0.5 \pi ) ( 1 - A(\phi) ) &amp;#43; A(\phi) } - 1 \Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v A(\phi) } \end{array}$$
&lt;p&gt;By extending A in the above formula, we can easily get the marginal probability density function as following:&lt;/p&gt;
&lt;p&gt;$ p_{\phi}(\phi) = \dfrac{1}{2 \pi \alpha_u \alpha_v \Big( {\frac{cos^2(\phi)}{\alpha_u^2}} + {\frac {sin^2(\phi) }{\alpha_v^2}} \Big) } $&lt;/p&gt;
&lt;p&gt;Then we will use the inversion method. So the CDF of the above function is:&lt;/p&gt;

$$ \begin{array} {lcl}  P_{\phi}(\phi) &amp;amp;=&amp;amp; \int_{0}^{\phi} p_{\phi}(t) dt \\\\ &amp;amp;=&amp;amp; \int_{0}^{\phi} \dfrac{1}{2 \pi \alpha_u \alpha_v \Big( {\frac{cos^2(t)}{\alpha_u^2}} &amp;#43; {\frac {sin^2(t) }{\alpha_v^2}} \Big) } dt \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v} \int_{0}^{\phi} \dfrac{1}{ cos^2(\theta) \Big( {\frac{1}{\alpha_u^2}} &amp;#43; {\frac {tan^2(t) }{\alpha_v^2}} \Big) } dt \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v} \int_{0}^{\phi} \dfrac{1}{ {\frac{1}{\alpha_u^2}} &amp;#43; {\frac {tan^2(t) }{\alpha_v^2}} } d(tan(t)) \\\\ &amp;amp;=&amp;amp; \dfrac{\alpha_u}{2 \pi \alpha_v} \int_{0}^{\phi} \dfrac{1}{ 1 &amp;#43; \Big(\frac { \alpha_u tan(t) }{\alpha_v}\Big)^2 } d(tan(t)) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi} \int_{0}^{\phi} \dfrac{1}{ 1 &amp;#43; \Big(\frac { \alpha_u tan(t) }{\alpha_v}\Big)^2 } d\Big( \frac{\alpha_u tan(t)}{\alpha_v} \Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi} \int_{0}^{\phi} d\Big( arctan\Big( \dfrac{\alpha_u tan(t)}{\alpha_v} \Big) \Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi} arctan\Big( \dfrac{\alpha_u tan(\phi)}{\alpha_v} \Big) \end{array} $$
&lt;p&gt;By setting a random number ranging from 0 to 1 to the CDF, we can easily acquire the following equation:&lt;/p&gt;

$$ \phi = arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) $$
&lt;p&gt;Before we dive into the derivation of $ \theta $, there is one minor situation to handle. Because arctan only gives you ranges between $ -\dfrac{\pi}{2}$ and $ \dfrac{\pi}{2}$, we need to remap some value to get the full range between 0 and $ 2\pi$.&lt;/p&gt;

&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/sample_anisotropic_microfacet_brdf/graph.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/sample_anisotropic_microfacet_brdf/graph.png&#34; width=&#34;350&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Above is a graph that I generated, since our random value goes from 0 to 1, we are only interested in ranges between 0 and 1. The factor $ \dfrac{\alpha_v}{\alpha_v} $ doesn&amp;rsquo;t affect the cycle of this function, it only affects the curve shape. It is fine to sample negative values here since $ \phi $ doesn&amp;rsquo;t always need to be positive, it can be anything as long as it covers the whole range. However, one thing that we can easily notice is that the result goes from $ -\dfrac{\pi}{2}$ and $ \dfrac{\pi}{2}$ and there is even a remap once the random value is larger than 0.5. My solution is to offset each section by an offset.&lt;/p&gt;

$ \phi= \begin{cases} arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;amp;:x\in [0,0.25] \\\\ arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;#43; \pi &amp;amp;:x\in (0.25,0.75) \\\\ arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;#43; 2\pi &amp;amp;:x\in [0.75,1] \end{cases} $
&lt;p&gt;Extra special attention needed here to make sure the value is correctly taken in the rare case where the random value happens to be 0.25 or 0.75 so that the final curve is a continuous one. And the $ 2 \pi $ can be totally ignored since it is the cycle of cos and sin, which is our only usage for $ \phi $. However, I&amp;rsquo;d like to add it here just to keep the result in the range of [0, $ 2 \pi $ ].&lt;/p&gt;
&lt;p&gt;Back to the derivation of $ \theta $, let&amp;rsquo;s look at the CDF of the conditional density function:&lt;/p&gt;

$$ \begin{array} {lcl} P_{\theta}(\theta) &amp;amp;=&amp;amp; \int_0^{\theta} \dfrac{p(t,\phi)}{p_{\phi}(\phi)} d(t) \\\\ &amp;amp;=&amp;amp; \dfrac{\int_0^{\theta} p(t,\phi) d(t)}{p_{\phi}(\phi)} \\\\ &amp;amp;=&amp;amp; \dfrac{ P_{\theta}(\theta,\phi)}{p_{\phi}(\phi)} \\\\ &amp;amp;=&amp;amp; \dfrac{\dfrac{1}{2 \pi \alpha_u \alpha_v ( 1 - A(\phi) ) } \Big( \dfrac{1}{ cos^2(\theta) ( 1 - A(\phi) ) &amp;#43; A(\phi) } - 1 \Big)}{\dfrac{1}{2 \pi \alpha_u \alpha_v A(\phi) }} \\\\ &amp;amp;=&amp;amp; \dfrac{A(\phi)}{ 1 - A(\phi) } \Big( \dfrac{1}{ cos^2(\theta) ( 1 - A(\phi) ) &amp;#43; A(\phi) } - 1 \Big) \end{array} $$
&lt;p&gt;Again let&amp;rsquo;s set a random value from 0 to 1 to the CDF&lt;/p&gt;

$$ \begin{array} {lcl} &amp;amp;&amp;amp; \epsilon = \dfrac{A(\phi)}{ 1 - A(\phi) } \Big( \dfrac{1}{ cos^2(\theta) ( 1 - A(\phi) ) &amp;#43; A(\phi) } - 1 \Big) \\\\ &amp;amp;-&amp;gt;&amp;amp; \dfrac{ \epsilon ( 1 - A(\phi) ) }{ A(\phi) } &amp;#43; 1 = \dfrac{1}{ cos^2(\theta) ( 1 - A(\phi) ) &amp;#43; A(\phi) }  \\\\ &amp;amp;-&amp;gt;&amp;amp; cos^2(\theta) ( 1 - A(\phi) ) &amp;#43; A(\phi) = \dfrac{A(\phi)}{ \epsilon ( 1 - A(\phi) ) &amp;#43; A(\phi) } \\\\ &amp;amp;-&amp;gt;&amp;amp; cos^2(\theta) ( 1 - A(\phi) ) = \dfrac{A(\phi)}{ \epsilon ( 1 - A(\phi) ) &amp;#43; A(\phi) } - A(\phi) \\\\ &amp;amp;-&amp;gt;&amp;amp; cos^2(\theta) ( 1 - A(\phi) ) = \dfrac{A(\phi) ( 1 - A(\phi) ) ( 1 - \epsilon ) }{ \epsilon ( 1 - A(\phi) ) &amp;#43; A(\phi) } \\\\ &amp;amp;-&amp;gt;&amp;amp; cos^2(\theta) = \dfrac{A(\phi)( 1 - \epsilon ) }{ \epsilon ( 1 - A(\phi) ) &amp;#43; A(\phi) } \end{array} $$
&lt;p&gt;Then we easily get the following formula for $ \theta $&lt;/p&gt;

$$ \theta = arccos\Big( \sqrt{ \dfrac{A(\phi)( 1 - \epsilon ) }{ \epsilon ( 1 - A(\phi) ) &amp;#43; A(\phi) } } \Big) $$
&lt;p&gt;A little bit further from the above equation:&lt;/p&gt;

$$ \begin{array} {lcl} &amp;amp;&amp;amp; tan^2(\theta) = \dfrac{1}{cos^2(\theta)} - 1 \\\\ &amp;amp;-&amp;gt;&amp;amp; tan^2(\theta) = \dfrac{ \epsilon ( 1 - A(\phi) ) &amp;#43; A(\phi) }{ A(\phi) ( 1 - \epsilon ) } - 1 \\\\ &amp;amp;-&amp;gt;&amp;amp; tan^2(\theta) = \dfrac{ \epsilon &amp;#43; A( \phi ) ( 1 - \epsilon ) }{ A(\phi) ( 1 - \epsilon ) } - 1 \\\\ &amp;amp;-&amp;gt;&amp;amp; tan^2(\theta) = \dfrac{ \epsilon }{ A(\phi) ( 1 - \epsilon ) } \end{array} $$
&lt;p&gt;Then $ \theta $ can also be calculated this way&lt;/p&gt;

$$ \theta = arctan\Big( \sqrt{ \dfrac{ \epsilon }{ ( 1 - \epsilon ) A(\phi) } } \Big) $$
&lt;p&gt;This is exactly the same thing with the above one except that it has less steps. Before we move forward to the next one, here is the final formula for $ \theta $ and $ \phi $ for importance sampling of GGX&lt;/p&gt;

$$ \phi= \begin{cases} arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;amp;:x\in [0,0.25] \\\\arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;#43; \pi &amp;amp;:x\in (0.25,0.75) \\\\arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;#43; 2\pi &amp;amp;:x\in [0.75,1] \end{cases} $$

$$ \theta = arctan\bigg( \sqrt{ \dfrac{ \epsilon }{ ( 1 - \epsilon ) \Big({\dfrac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\dfrac {sin^2(\phi) }{\alpha_v^2 }}\Big)} } \bigg) $$
&lt;h1 id=&#34;sampling-beckmann&#34;&gt;Sampling Beckmann&lt;/h1&gt;
&lt;p&gt;Several concepts are very similar to the above derivation, which we will skip here for simplicity.&lt;/p&gt;
&lt;p&gt;The formula for Beckmann is as following:&lt;/p&gt;

$$ D(h) = \dfrac{ e^{-tan^2(\theta)\Big( (\frac{cos(\phi)}{\alpha_u})^2 &amp;#43; (\frac{sin(\phi)}{\alpha_v})^2 \Big) } }{\pi \alpha_u \alpha_v cos^4(\theta) } $$
&lt;p&gt;As usual, the PDF that we use to sample Beckmann is defined as following:&lt;/p&gt;

$$ p(\theta,\phi) = \dfrac{ sin(\theta) \Bigg(e^{-tan^2(\theta)\Big( (\frac{cos(\phi)}{\alpha_u})^2 &amp;#43; (\frac{sin(\phi)}{\alpha_v})^2 \Big) } \Bigg)}{\pi \alpha_u \alpha_v cos^3(\theta) } $$
&lt;p&gt;Let&amp;rsquo;s look at the CDF for $ \theta $:&lt;/p&gt;

$$ P_{\theta}(\theta,\phi) = \int_0^{\theta} p(t,\phi) d(t) = \int_0^{\theta} \dfrac{ sin(t) \bigg(e^{-tan^2(t)\Big( (\frac{cos(\phi)}{\alpha_u})^2 &amp;#43; (\frac{sin(\phi)}{\alpha_v})^2 \Big) } \bigg)}{\pi \alpha_u \alpha_v cos^3(t) } d(t) $$
&lt;p&gt;We gonna use the same A term that we defined earlier to simplify the derivation:&lt;/p&gt;

$$ \begin{array} {lcl} P_{\theta}(\theta,\phi) &amp;amp;=&amp;amp; \int_0^{\theta} \dfrac{ sin(t) (e^{-tan^2(t) A(\phi) } )}{\pi \alpha_u \alpha_v cos^3(t) } d(t) \\\\ &amp;amp;=&amp;amp; \dfrac{-1}{\pi \alpha_u \alpha_v} \int_0^{\theta} \dfrac{ e^{-tan^2(t) A(\phi) } }{ cos^3(t) } d(cos(t)) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v} \int_0^{\theta} e^{-tan^2(t) A(\phi) } d\Big(\dfrac{1}{cos^2(t)}\Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v} \int_0^{\theta} e^{ \Big( 1 - \frac{1}{cos^2(t)} \Big) A(\phi) } d\Big(\dfrac{1}{cos^2(t)}\Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v A(\phi) } \int_0^{\theta} e^{ A(\phi) - \frac{A(\phi)}{cos^2(t)} } d\Big(\dfrac{A(\phi)}{cos^2(t)}\Big) \\\\ &amp;amp;=&amp;amp; \dfrac{-1}{2 \pi \alpha_u \alpha_v A(\phi) } \int_0^{\theta} d\Big(e^{ A(\phi) - \frac{A(\phi)}{cos^2(t)} } \Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v A(\phi) } \Big( 1 - e^{ A(\phi) - \frac{A(\phi)}{cos^2(\theta)} } \Big) \end{array}$$
&lt;p&gt;Following is the marginal probability density function for $ \phi $&lt;/p&gt;

$$ \begin{array} {lcl} p_{\phi}(\phi) &amp;amp;=&amp;amp; \lim_{\theta \to {0.5\pi}} \int_0^{\theta} p(\theta,\phi) d(\theta) \\\\ &amp;amp;=&amp;amp; \lim_{\theta \to {0.5\pi}} P_{\theta}( \theta , \phi ) \\\\ &amp;amp;=&amp;amp; \lim_{\theta \to {0.5\pi}} \Big( \dfrac{1}{2 \pi \alpha_u \alpha_v A(\phi) } \Big( 1 - e^{ A(\phi) - \frac{A(\phi)}{cos^2(\theta)} } \Big) \Big) \\\\ &amp;amp;=&amp;amp; \dfrac{1}{2 \pi \alpha_u \alpha_v A(\phi) } \end{array}$$
&lt;p&gt;One minor detail to notice is that since we can&amp;rsquo;t really approach 90 degree angle for $ \theta $, we will not take it into consideration here. Surprisingly, this is exactly the same with GGX&amp;rsquo;s marginal probability density function, so we will take everything we have derived here to avoid the duplicated work so that we can move forward to the CDF for $ \theta $ directly&lt;/p&gt;

$$ \begin{array} {lcl} P_{\theta}(\theta) &amp;amp;=&amp;amp; \int_0^{\theta} \dfrac{p(t,\phi)}{p_{\phi}(\phi)} d(t) \\\\ &amp;amp;=&amp;amp; \dfrac{\int_0^{\theta} p(t,\phi) d(t)}{p_{\phi}(\phi)} \\\\ &amp;amp;=&amp;amp; \dfrac{ P_{\theta}(\theta,\phi)}{p_{\phi}(\phi)} \\\\ &amp;amp;=&amp;amp; \dfrac{\frac{1}{2 \pi \alpha_u \alpha_v A(\phi) } \Big( 1 - e^{ A(\phi) - \frac{A(\phi)}{cos^2(\theta)} }\Big)}{ \frac{1}{2 \pi \alpha_u \alpha_v A(\phi) }} \\\\ &amp;amp;=&amp;amp;  1 - e^{ A(\phi) - \frac{A(\phi)}{cos^2(\theta)} } \\\\ &amp;amp;=&amp;amp;  1 - e^{ - A(\phi) tan^2(\theta) } \end{array} $$
&lt;p&gt;By setting the CDF to $ \epsilon $ it is not hard to get the following equation:&lt;/p&gt;

$$ \theta = arctan\Bigg( \dfrac{ln(1-\epsilon)}{A(\phi)} \Bigg) = arctan\Bigg( \dfrac{ln(1-\epsilon)}{{\frac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\frac {sin^2(\phi) }{\alpha_v^2 }}} \Bigg) $$
&lt;p&gt;Since $ \epsilon $ is randomly chosen between 0 and 1, we can easily replace $ 1 - \epsilon $ with $ \epsilon $ itself, resulting in the final formula for $ \theta $&lt;/p&gt;

$$ \theta = arctan\Bigg( \dfrac{ln(\epsilon)}{{\frac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\frac {sin^2(\phi) }{\alpha_v^2 }}} \Bigg) $$
&lt;p&gt;To summarize, following are the formula we used to importance sample Beckmann:&lt;/p&gt;

$$ \phi= \begin{cases} arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;amp;:x\in [0,0.25] \\\\arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;#43; \pi &amp;amp;:x\in (0.25,0.75) \\\\arctan\Big( \dfrac{\alpha_v}{\alpha_u} tan( 2 \pi \epsilon ) \Big) &amp;#43; 2\pi &amp;amp;:x\in [0.75,1] \end{cases}  $$

$$ \theta = arctan\Bigg( \dfrac{ln(\epsilon)}{{\frac{cos^2(\phi)}{\alpha_u^2 }} &amp;#43; {\frac {sin^2(\phi) }{\alpha_v^2 }}} \Bigg) $$
&lt;h1 id=&#34;sampling-blinn&#34;&gt;Sampling Blinn&lt;/h1&gt;
&lt;p&gt;Here we will talk about the modified Blinn-Phong model, instead of the original one proposed in paper ( &lt;a href=&#34;http://www.irisa.fr/prive/kadi/Lopez/ashikhmin00anisotropic.pdf&#34;&gt;An Anisotropic Phong BRDF Model&lt;/a&gt; ), because it obeys the rule of energy conservation. And the other detail that deserves mentioning is that I will use the exponent term instead of alpha term here:&lt;/p&gt;

$$ e = \dfrac{2.0}{\alpha^4} - 2.0 $$
&lt;p&gt;It goes true along both directions. And the evaluation of Blinn is as following:&lt;/p&gt;

$$ D(h) = \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} cos(\theta) ^ {( cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v ) } $$
&lt;p&gt;Following is the pdf we use to sample Blinn:&lt;/p&gt;

$$ p( \theta , \phi ) = \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} sin(\theta ) cos(\theta) ^ {( cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v &amp;#43; 1 ) } $$
&lt;p&gt;The CDF for $ \theta $ is as following:&lt;/p&gt;

$$ P_{\theta}(\theta,\phi) = \int_0^{\theta} p(t,\phi) d(t) = \int_0^{\theta} \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} sin(t) cos(t) ^ {( cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v &amp;#43; 1 ) } dt $$
&lt;p&gt;Before we move forward, let&amp;rsquo;s define a B term to make the whole derivation shorter:&lt;/p&gt;

$$ B(\phi) = cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v $$
&lt;p&gt;And then the above formula becomes:&lt;/p&gt;

$$ \begin{array} {lcl} P_{\theta}(\theta,\phi) &amp;amp;=&amp;amp; \int_0^{\theta} \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} ( cos(t) ^ {( B(\phi) &amp;#43; 1 ) } ) sin(t) dt \\\\ &amp;amp;=&amp;amp; \dfrac{-\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} \int_0^{\theta} cos(t) ^ {( B(\phi) &amp;#43; 1 ) } d( cos(t) ) \\\\ &amp;amp;=&amp;amp; \dfrac{-\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{ 2\pi ( B(\phi) &amp;#43; 2 ) } \int_0^{\theta} d( cos(t) ^ { {B(\phi) &amp;#43; 2 }} ) \\\\ &amp;amp;=&amp;amp; \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2 \pi ( B(\phi) &amp;#43; 2 ) } ( 1 - cos(\theta) ^ {( B(\phi) &amp;#43; 2 ) } ) \end{array}$$
&lt;p&gt;Following is the marginal probability density function for $ \phi $:&lt;/p&gt;

$$ \begin{array} {lcl} p_{\phi}(\phi) &amp;amp;=&amp;amp; \int_0^{0.5 \pi} p(\theta,\phi) d(\theta) \\\\ &amp;amp;=&amp;amp; P_{\theta}( 0.5\pi , \phi ) \\\\ &amp;amp;=&amp;amp; \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{ 2\pi ( B(\phi) &amp;#43; 2 ) } ( 1 - cos( 0.5 \pi ) ^ {( B(\phi) &amp;#43; 2 ) } ) \\\\ &amp;amp;=&amp;amp; \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{ 2 \pi ( B(\phi) &amp;#43; 2 ) } \\\\ &amp;amp;=&amp;amp; \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{ 2 \pi ( cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v &amp;#43; 2 ) } \end{array}$$
&lt;p&gt;The CDF for $ \phi $ is:&lt;/p&gt;

$$  \begin{array} {lcl} P_{\phi}(\phi) &amp;amp;=&amp;amp; \int_0^{\phi} p_{\phi}(t) d(t) \\\\ &amp;amp;=&amp;amp; \int_0^{\phi}\dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{ 2 \pi ( cos^2(t) * e_u &amp;#43; sin^2(t) * e_v &amp;#43; 2 ) } d(t) \\\\  &amp;amp;=&amp;amp; \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} \int_0^{\phi}\dfrac{1}{ cos^2(t) * e_u &amp;#43; sin^2(t) * e_v &amp;#43; 2 ( cos^2(t) &amp;#43; sin^2(t) ) } d(t) \\\\  &amp;amp;=&amp;amp; \dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2\pi} \int_0^{\phi}\dfrac{1}{  cos^2(t) ( e_u &amp;#43; 2 &amp;#43; tan^2(t) * ( e_v &amp;#43; 2 ) ) } d(t) \\\\  &amp;amp;=&amp;amp; \dfrac{1}{2\pi} \sqrt{\dfrac{ e_v &amp;#43; 2 }{ e_u &amp;#43; 2 }} \int_0^{\phi}\dfrac{1}{  1 &amp;#43; \Big(\sqrt{\frac{ e_v &amp;#43; 2 }{ e_u &amp;#43; 2 }} tan(t) \Big) ^ 2 } d( tan(t) ) \\\\  &amp;amp;=&amp;amp; \dfrac{1}{2\pi} \int_0^{\phi}\dfrac{1}{  1 &amp;#43; \Big( \sqrt{\frac{ e_v &amp;#43; 2 }{ e_u &amp;#43; 2 }} tan(t) \Big) ^ 2 } d\Big( \sqrt{\frac{ e_v &amp;#43; 2 }{ e_u &amp;#43; 2 }} tan(t) \Big) \\\\  &amp;amp;=&amp;amp; \dfrac{1}{2\pi} arctan\Big( \sqrt{\frac{ e_v &amp;#43; 2 }{ e_u &amp;#43; 2 }} tan(t) \Big) \end{array} $$
&lt;p&gt;It is not hard to get the following formula for $ \phi $&lt;/p&gt;

$$ \phi = arctan\Big( \sqrt{\dfrac{ e_u &amp;#43; 2 }{ e_v &amp;#43; 2 }} tan(2 \pi \epsilon) \Big) $$
&lt;p&gt;And we also need to offset this parameter like we did for the previous two sampling method, but the way we do it is almost identical. Last, we need to generate $ \theta $ based on the conditional probability density function:&lt;/p&gt;

$$ \begin{array} {lcl} P_{\theta}(\theta) &amp;amp;=&amp;amp; \int_0^{\theta} \dfrac{p(t,\phi)}{p_{\phi}(\phi)} d(t) \\\\ &amp;amp;=&amp;amp; \dfrac{\int_0^{\theta} p(t,\phi) d(t)}{p_{\phi}(\phi)} \\\\ &amp;amp;=&amp;amp; \dfrac{ P_{\theta}(\theta,\phi)}{p_{\phi}(\phi)} \\\\ &amp;amp;=&amp;amp; \dfrac{\dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{2 \pi ( B(\phi) &amp;#43; 2 ) } ( 1 - cos(\theta) ^ {( B(\phi) &amp;#43; 2 ) } )}{\dfrac{\sqrt{( e_u &amp;#43; 2 ) * ( e_v &amp;#43; 2 ) }}{ 2 \pi ( B(\phi) &amp;#43; 2 ) }} \\\\ &amp;amp;=&amp;amp; 1 - cos(\theta) ^ { B(\phi) &amp;#43; 2 } \end{array} $$
&lt;p&gt;With a random variable equals to the above CDF, we can easily have the following formula:&lt;/p&gt;

$$ \theta = arccos\Big( ( 1 - \epsilon ) ^ { \small \dfrac{1}{cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v &amp;#43; 2} } \Big) $$
&lt;p&gt;And again, we can also flip the $ \epsilon $ because it goes from 0 to 1.&lt;/p&gt;

$$ \theta = arccos\Big( \epsilon ^ { \small \dfrac{1}{cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v &amp;#43; 2} } \Big) $$
&lt;p&gt;Before we sumarize, here is the final formula for importance sampling of Blinn:&lt;/p&gt;

$$ \phi= \begin{cases} arctan\Big( \sqrt{\dfrac{ e_u &amp;#43; 2 }{ e_v &amp;#43; 2 }} tan(2 \pi \epsilon) \Big) &amp;amp;:x\in [0,0.25] \\\\arctan\Big( \sqrt{\dfrac{ e_u &amp;#43; 2 }{ e_v &amp;#43; 2 }} tan(2 \pi \epsilon) \Big) &amp;#43; \pi &amp;amp;:x\in (0.25,0.75) \\\\arctan\Big( \sqrt{\dfrac{ e_u &amp;#43; 2 }{ e_v &amp;#43; 2 }} tan(2 \pi \epsilon) \Big) &amp;#43; 2\pi &amp;amp;:x\in [0.75,1] \end{cases}  $$

$$ \theta = arccos\Big( \epsilon ^ { \small \dfrac{1}{cos^2(\phi) * e_u &amp;#43; sin^2(\phi) * e_v &amp;#43; 2} } \Big) $$
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Importance sampling is always importance for a ray tracer. With the above method, a ray tracer should be able to reach relative noise-free image with reasonalely enough sample taken per pixel.&lt;/p&gt;
&lt;p&gt;There is also some future work improving the efficiency of importance sampling for the microfacet model, like sampling visible normal. And it is also the default method for PBRT microfacet model sampling.&lt;/p&gt;
&lt;p&gt;If someone is interested in the detailed implementation, they can check it out in my github project &lt;a href=&#34;https://github.com/JiayinCao/SORT/blob/master/src/scatteringevent/bsdf/microfacet.cpp&#34;&gt;here&lt;/a&gt; .&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference:&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://www.statlect.com/glossary/marginal-probability-density-function&#34;&gt;Marginal probability density function&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Conditional_probability_distribution&#34;&gt;Conditional probability distribution&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.pbrt.org/&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://hal.inria.fr/hal-00996995v1/document&#34;&gt;Importance Sampling Microfacet-Based BSDFs using the Distribution of Visible Normals&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;https://www.mitsuba-renderer.org/~wenzel/files/visnormal.pdf&#34;&gt;An Improved Visible Normal Sampling Routine for the Beckmann Distribution &lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;http://graphicrants.blogspot.com/2013/08/specular-brdf-reference.html&#34;&gt;Specular BRDF Reference&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&#34;http://www.irisa.fr/prive/kadi/Lopez/ashikhmin00anisotropic.pdf&#34;&gt;An Anisotropic Phong BRDF Model&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&#34;http://simonstechblog.blogspot.com/2011/12/microfacet-brdf.html&#34;&gt;Microfacet BRDF&lt;/a&gt;&lt;br&gt;
[9] &lt;a href=&#34;http://www.farbrausch.de/~fg/stuff/phong.pdf&#34;&gt;Phong Normalization Factor derivation&lt;/a&gt;&lt;br&gt;
[10] &lt;a href=&#34;https://agraphicsguynotes.com/posts/sample_microfacet_brdf/&#34;&gt;Sampling Microfacet BRDF&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How does PBRT verify BXDF</title>
      <link>https://agraphicsguynotes.com/posts/how_does_pbrt_verify_bxdf/</link>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/how_does_pbrt_verify_bxdf/</guid>
      <description>&lt;p&gt;Unit test for BXDF in offline rendering turns out to be way more important than what I thought it would be. I still remember it took me quite a long time when I debugged my bi-directional path tracing algorithm before I noticed there was a little BXDF bug, which easily led to some divergence between BDPT and path tracing. Life will be much easier if we could find any potential BXDF problem at the very beginning.&lt;/p&gt;
&lt;p&gt;Unlike most simple real-time rendering applications, it requires a lot more than just evaluating a BXDF in pixel shader. Usually, in offline rendering, we need to support the following features for each BXDF:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating BXDF given an incoming ray and out-going ray.&lt;/li&gt;
&lt;li&gt;Given a pdf and an incoming ray, we need to sample an out-going ray based on the pdf.&lt;/li&gt;
&lt;li&gt;Given an incoming ray and the same out-going ray generated above, it should return exactly the same PDF given by the above method. Keeping this consistent with the above feature is very important, otherwise, there will most likely be bias introduced in the renderer.&lt;/li&gt;
&lt;li&gt;It is usually hard-coded that which PDF we will use for a specific BXDF. Technically speaking, unlike the above features, one can totally ignore this one. The more similarities between the shape of the two functions, the faster it converges to the ideal result.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this blog, I will talk about how PBRT verifies its BXDF in its &amp;lsquo;bsdftest&amp;rsquo; program.&lt;/p&gt;
&lt;h1 id=&#34;how-the-verification-works&#34;&gt;How the verification works&lt;/h1&gt;
&lt;p&gt;Following is one of the outputs from the verification program&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/how_does_pbrt_verify_bxdf/image1.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;It is fairly clear that it shows results for three BRDFs. If we look at the output information closely, we will notice the following detail.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We have a histogram with 10 x 10 entries in it. The whole sampling hemisphere domain is divided into 100 sub-domains, each of which has a same solid angle.&lt;/li&gt;
&lt;li&gt;Each of the entries should approach 2 * PI.&lt;/li&gt;
&lt;li&gt;The final average should also approach 2 * PI.&lt;/li&gt;
&lt;li&gt;There are two kinds of special samples
&lt;ul&gt;
&lt;li&gt;Bad samples. A sample with invalid data, like invalid radian or pdf.&lt;/li&gt;
&lt;li&gt;Outside samples. Samples that are below the hemisphere.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Evaluated radiance, which is really not important in this program.&lt;/li&gt;
&lt;li&gt;It may also output a warning if PDF evaluated based on the incoming and out-going ray doesn&amp;rsquo;t equal to the PDF we use to generate the out-going ray based on the incoming ray.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One can almost guess most of the things done in this application. Actually what this process does is this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First of all, it creates a sub-set of pbrt&amp;rsquo;s BXDFs, not the full set. That said it doesn&amp;rsquo;t verify every BXDF in the renderer. Since all of the BXDF in the applications are actually BRDF, it only considers samples in the upper hemisphere.&lt;/li&gt;
&lt;li&gt;For each BRDF, it takes 10000000 samples based on importance sampling provided by the BRDF. For each sample, it falls in a bad sample category if it has invalid data, it will be an outside sample if it is under the surface. Otherwise, based on the angle, it will be distributed to an entry in the histogram.&lt;/li&gt;
&lt;li&gt;Once the data in the histogram is generated, the output is generated by some mathematical formula. Basically, we need to pay attention to the error, bad/outside samples, or any invalid PDF error. A BXDF is considered OK if there are no bad samples, no PDF error and everything equals to 2 * PI. The number of outside samples is also important, but it is OK to have some, which means that the efficiency of this importance sampling may not be very good, but it is far from a sign of an existed bug in the renderer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the following sections, we will mainly talk about the mathematical formulas.&lt;/p&gt;
&lt;h1 id=&#34;why-everything-has-to-be-2pi&#34;&gt;Why everything has to be 2*PI&lt;/h1&gt;
&lt;h2 id=&#34;final-average-needs-to-converge-to-2pi&#34;&gt;Final average needs to converge to 2*PI&lt;/h2&gt;
&lt;p&gt;It is easier to explain why the final has to be 2*PI first, which is a pretty good start for this blog.&lt;/p&gt;
&lt;p&gt;First of all, we can easily derive the following formula, which says that the area of a hemisphere where the radius is one is 2 * PI.&lt;/p&gt;

$$ \int_{\Omega} \mathrm{d}w = 2 * PI $$
&lt;p&gt;Although we know it is 2 * PI, but let&amp;rsquo;s assume that we don&amp;rsquo;t so far. In order to evaluate  $ \int_{\Omega} \mathrm{d}w $ 
, we can use Monte Carlo to solve it. The estimator is as follow:&lt;/p&gt;

$$ \dfrac{1}{N} \sum\limits_{i=1}^{N} \dfrac{1}{p({\omega}_i)} $$
&lt;p&gt;If we look at the source code of this application, this is exactly what the verification does to calculate the final average. It is just a little bit less obvious because it accumulates the result in 100 bins first before calculating the final sum, but it is essentially the same math formula under the hood. In a nutshell, the final average is the estimator in this case.&lt;/p&gt;
&lt;p&gt;Since we already know the value of the integral is 2 * PI. It means that the final average should converge to 2 * PI, with 10000000 samples taken, it could very likely indicate a bug if it doesn&amp;rsquo;t converge there.&lt;/p&gt;
&lt;h2 id=&#34;about-each-entry&#34;&gt;About each entry&lt;/h2&gt;
&lt;p&gt;This one is a little bit more complex. But the basic idea is quite similar to the above derivation.&lt;/p&gt;
&lt;p&gt;First of all, we need to pay attention to the way the algorithm divides the hemisphere domain. For phi, it is evenly divided into 10 equal-size region. For theta, it is very important to notice that it is the result of cosine theta that is evenly divided, not theta itself. So that said, theta is divided in the following sub-region.&lt;/p&gt;

$$ [acos(\dfrac{i}{10}), acos(\dfrac{i&amp;#43;1}{10})) $$
&lt;p&gt;In the above formula, i goes from 0 to 9. For the last sub-region, it is fully inclusive. It may be very confused to see it is divided this way. But if we look at the solid angle extended by each sub-region, we will suddenly understand why it works this way.&lt;/p&gt;

$$ \begin{array} {lcl} \int_{{\Omega}_{ij}} \mathrm{d}{\omega}  &amp;amp; = &amp;amp; \int_{2*PI*i/10}^{2*PI*(i&amp;#43;1)/10} \int_{acos({(j&amp;#43;1)/10})}^{acos(j/10)} sin(\theta)\mathrm{d} \theta\mathrm{d} \phi \\\\ &amp;amp; = &amp;amp;\int_{2*PI*i/10}^{2*PI*(i&amp;#43;1)/10}\mathrm{d} \phi \int_{acos(j/10)}^{acos({(j&amp;#43;1)/10})}\mathrm{d}(cos(\theta)) \\\\ &amp;amp; = &amp;amp; 2*PI/100 \end{array} $$
&lt;p&gt;The size of the solid angle is in-dependent of which sub-region it is casted from. The solid angle extended by the sub-region turns out to be same size!&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s define a visibility function so that we can focus on a single entry.&lt;/p&gt;

$$ V_{i j }(\omega) = \begin{cases} 1&amp;amp;:\theta \in [acos((i&amp;#43;1)/10),acos(i/10))  ,  \phi \in [2*PI*j/10,2*PI*(j&amp;#43;1)/10) \\ 0 &amp;amp;:otherwise \end{cases} $$
&lt;p&gt;This visibility function gives you only the visibility of a single entry, leaving the rest of them as zero so that we can easily focus on one single entry. Since we divide the two-dimentional domain into sub-domain, i and j can identify which sub-domain we are focusing on. We can easily derive the following equation:&lt;/p&gt;

$$ \int_{\omega} V_{ij}(\omega) \mathrm{d}(\omega) = \int_{{\Omega}_{ij}} \mathrm{d}{\omega} = 2 * PI / 100 $$
&lt;p&gt;Again, let&amp;rsquo;s look at the Monte Carlo estimator for the left-most integral in the above equation.&lt;/p&gt;

$$ \sum\limits_{k=1}^{N} \dfrac{V_{ij}(\omega)}{p(\omega_k)} $$
&lt;p&gt;Basically, it says that if the sample falls in the subdomain defined by i and j, we count the contribution ( 1 / pdf ), otherwise we simply ignore it. N is the number of total samples taken, instead of just the samples falling in the subdomain. If we connect the two equations, we knows that by multiplying the result of the estimator by 100, we should reach 2 * PI. Which is exactly what the verification process does.&lt;/p&gt;
&lt;h1 id=&#34;what-does-the-process-verify&#34;&gt;What does the process verify&lt;/h1&gt;
&lt;p&gt;Basically, it is most likely to expose a hidden bug in the bxdf implementation if there were something wrong with the following cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A sampling process doesn&amp;rsquo;t generate samples based on given PDF at all.&lt;/li&gt;
&lt;li&gt;The pdf evaluation returns the incorrect value for an incoming and outgoing ray. Put it in another way, it doesn&amp;rsquo;t match the way we sample rays.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are not something easy to find once there is bug. Locating the bug in bxdf during debugging a path tracing algorithm is way more painful than limiting the problem in a specific BXDF system.&lt;/p&gt;
&lt;p&gt;Although it does verify something in bxdf, it doesn&amp;rsquo;t tell how good a PDF is to a bxdf. This is also something can be evaluated in a unit test.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering, second edition&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Volume Rendering in Offline Renderer</title>
      <link>https://agraphicsguynotes.com/posts/volume_rendering_in_offline_renderer/</link>
      <pubDate>Thu, 10 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/volume_rendering_in_offline_renderer/</guid>
      <description>&lt;p&gt;Finally, I have some time reading books, spending several days digesting the volume rendering part of PBRT, there are loads of stuff that interest me. Instead of repeating the theory in it, I decided to put some key points in my blog with some brief introduction and then provide some derivations which are not mentioned in the book.&lt;/p&gt;


&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/volume_rendering_in_offline_renderer/no_volume.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/volume_rendering_in_offline_renderer/no_volume.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/volume_rendering_in_offline_renderer/with_volume.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/volume_rendering_in_offline_renderer/with_volume.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;In case someone is not familiar with what volume rendering is, the above textures are attached for your references. One can easily notice that the fog on the right image greatly enhances the fidelity of the scene to a whole new different level. For a detailed explanation of volume rendering, it is suggested to check out this wiki page. Volume rendering is commonly used as fog, water etc. And mastering the basics behind it is of great importance because it also serves the basic theory of sub-surface scattering, which is very common even in real time 3D applications, like video games.&lt;/p&gt;
&lt;h1 id=&#34;how-light-interacts-with-volume&#34;&gt;How Light Interacts with Volume&lt;/h1&gt;
&lt;p&gt;In a nutshell, volume can change the way light propagates, which is why we care. More specifically, volume consists tons of very small particles that can interact with light rays. Some of them change the direction of light rays, others will absorb some energy from the ray, some even emits energy. There are four types of interaction that we need to consider in an offline renderer:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Absorption&lt;/strong&gt;. Light will attenuate as it travels through a volume.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Emission&lt;/strong&gt;. Some volume may emit energy making itself a &amp;rsquo;light source&#39;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scattering&lt;/strong&gt;. There are two kinds of scattering:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Out-Scattering&lt;/strong&gt;. This happens when the light ray changes its direction. This is why fog makes background dimmer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-Scattering&lt;/strong&gt;. Since light can change its direction, there are chances that some light will come in due to light direction changing. A real case is why it appears grey in a foggy day, because fog bends the sun rays to your eye.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;real-time-solution&#34;&gt;Real-Time Solution&lt;/h1&gt;
&lt;p&gt;Real-time solution for volume rendering is commonly biased. A quite common solution is particle system, meaning generating tons of billboards with alpha blending enabled. Most of the practical or research work aims at how to render a huge number of particles in an efficient manner. I haven&amp;rsquo;t seen any targeting on the unbiased feature of particle rendering, because it is not from the very beginning.&lt;/p&gt;
&lt;p&gt;Of course, particle system can only deliver effects like smog, fire etc. It may not be able to reproduce some other volume like light shaft or water. Although a lot of water simulation work does use particles under the hood, we don&amp;rsquo;t usually call them particle system. Light shaft is commonly a post-processing pass, &lt;a href=&#34;https://developer.nvidia.com/VolumetricLighting&#34;&gt;some work&lt;/a&gt; generate light shaft with real geometry, but I haven&amp;rsquo;t seen any work using particles to simulate it.&lt;/p&gt;
&lt;p&gt;As hardware is getting more and more powerful. Modern AAA games also have lots of volume rendering in them, which is actually way out of the scope of this blog. For a short description and implementation, I strongly suggest walking through the implementation in &lt;a href=&#34;https://www.shadertoy.com/view/XlBSRz&#34;&gt;this shadertoy demo&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;offline-solution&#34;&gt;Offline Solution&lt;/h1&gt;
&lt;p&gt;Comparing with real-time rendering solution, I kind of like offline solution due to its consistence among all situation like fog, light shaft or even water. I am no expert of physics, my take on them is that all of them are a set of particles at a micro level. They are actually the same except that the difference is how particles interact with each other.&lt;/p&gt;
&lt;h2 id=&#34;absorption&#34;&gt;Absorption&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start from absorption. Assuming we have a volume with its particles uniformly distributed inside it, light attenuation due to absorption will be the same at all points along the ray.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/volume_rendering_in_offline_renderer/absorption.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;We have the following equation at a micro level:&lt;/p&gt;

$$ L_o(p,w) - L_i(p,-w) = dL_o(p,w) = -\sigma_a(p,w)L_i(p,-w)dt$$
&lt;p&gt;This may look confusing the first time one sees it because of the subscripts, it adopts the commonly used term Li and Lo in it. Detail explanation is at the page of 286 of the &lt;a href=&#34;https://pbrt.org/&#34;&gt;book&lt;/a&gt; (second edition). After converting all subscript into one, we get the following equation:&lt;/p&gt;

$$ dL_o(p,w) = -\sigma_a(p,w)L_o(p,w)dt $$
&lt;p&gt;And since the subscript is just something that we use to distinguish between incoming and outgoing light, we can drop it in the above equation making it much clearer than it used to be.&lt;/p&gt;

$$ dL(p,w) = -\sigma_a(p,w)L(p,w)dt $$
&lt;p&gt;What it says is that light will attenuate with exactly the same rate at all points along the ray. Comparing with micro level equation, what we care more is a higher level knowledge of light interaction with volume, instead of small particles that are not visible to human vision at all. So the real practical question is how much light will still remain after passing through a uniform volume like the above one.&lt;/p&gt;
&lt;p&gt;First of all, let&amp;rsquo;s convert the equation this way:&lt;/p&gt;

$$ \dfrac{dL(p,w)}{L(p,w)} = -\sigma_a(p,w)dt $$
&lt;p&gt;One detail that we haven&amp;rsquo;t mentioned is the parameter of &amp;rsquo;t&amp;rsquo;. Any point along the ray could be represented by $ p(t)=p(0) + t*w $. In other words, it is the distance between the starting point and ending point that we consider. So we can regard &amp;lsquo;p&amp;rsquo; as a function of t in the above equation. If we take an integral from 0 to d, we have the following equation.&lt;/p&gt;

$$ \int_0^d \dfrac{dL(p(t),w)}{L(p(t),w)} = \int_0^d -\sigma_a(p(t),w)dt $$
&lt;p&gt;It is quite easy to derive the following equation.&lt;/p&gt;

$$ \dfrac{L(p&amp;#43;d*w,w)}{L(p,w)} = e^{ -\int_0^d \sigma_a(p,w)dt} $$
&lt;p&gt;The above equation is the attenuation, which only takes absorption into account, due to the existence of the volume. And it is exactly the second equation mentioned at the page of 578 in PBRT book (second edition).&lt;/p&gt;
&lt;h2 id=&#34;out-scattering&#34;&gt;Out-Scattering&lt;/h2&gt;
&lt;p&gt;Out-scattering is quite similar with absorption. As a matter of fact, there is no difference between them in handling attenuation. So instead of saving absorption factor, we can save attenuation factor which is the summation of absorption and out-scattering. Unless we have a clear reason to save them separately, there is no need to do that.&lt;/p&gt;

$$ \sigma_t=\sigma_a&amp;#43;\sigma_s$$
&lt;p&gt;For our convenience, it is better to define one term, which is commonly named &amp;lsquo;beam transmittance&amp;rsquo; and defines the proportion of light survives the uniform volume due to attenuation (out-scattering and absorption).&lt;/p&gt;

$$ T_r(p(0) \rightarrow p(d) ) = e^{ -\int_0^d \sigma_t(p,w)dt} $$
&lt;h2 id=&#34;emission&#34;&gt;Emission&lt;/h2&gt;
&lt;p&gt;Emission is actually the simplest one among the four. The differential change is just the emission rate.&lt;/p&gt;

$$ dL(p,w) = L_{e}(p,w)dt $$
&lt;h2 id=&#34;in-scattering&#34;&gt;In-Scattering&lt;/h2&gt;
&lt;p&gt;This is the most complex one among the four, it took me a while to figure out the mathematics behind it. I&amp;rsquo;d like to start with two equations in PBRT(2nd) and explain the derivation between them. First equation is the differential changes along the ray direction due to in-scattering.&lt;/p&gt;

$$ \dfrac{dL_o(p,\omega)}{dt} = -\sigma_t(p,\omega) L_i(p,-\omega) &amp;#43; S( p,\omega) $$
&lt;p&gt;At a first glance, it seems weird due to the fact that it involves both $ L_i $ and $ L_o $. As a matter of fact, we can easily convert it to uniform direction by switching $ L_i $ to $ L_o $.&lt;/p&gt;

$$ \dfrac{dL_o(p,\omega)}{dt} = -\sigma_t(p,\omega) L_o(p,\omega) &amp;#43; S( p,\omega) $$
&lt;p&gt;One good feature about it is that since we only have $ L_o $, we can safely drop that subscript making the equation simpler. Although all functions in this equation are functions of both solid angle and position, which is further a function of t, the distance from the original point as mentioned above, it is also safe to drop the solid angle parameter because it doesn&amp;rsquo;t change in the equation at all. We can further simplify the equation to the following state:&lt;/p&gt;

$$ \dfrac{dL(p)}{dt} = -\sigma_t(p) L(p) &amp;#43; S( p ) $$
&lt;p&gt;What it says is really simple, it states the fact that the differential change due to in-scattering along the ray is the amount of differential in-coming scattering radiance S(p) minus the attenuation by absorption and out-scattering. For the simplicity, I won&amp;rsquo;t include the definition of S here. One can always read PBRT(2nd) for further information.&lt;/p&gt;
&lt;p&gt;The other equation in PBRT is the following one, which according to the book can be derived from the above one.&lt;/p&gt;

$$ L_i(p,\omega) = \int_0^{\infty} T_r(p&amp;#39; \rightarrow p) S(p&amp;#39;, -\omega) dt $$
&lt;p&gt;Again, let&amp;rsquo;s get rid of the subscript first.&lt;/p&gt;

$$ L(p,-\omega) = \int_0^{\infty} T_r(p&amp;#39; \rightarrow p) S(p&amp;#39;, -\omega) dt $$
&lt;p&gt;If you look at the picture below, you will find that this description is even better than the previous one, although both make perfect sense. What it says is that the radiance at point p along direction -w due to in-scattering is actually the accumulation of all in-scattering radiance at all points along direction -w attenuated by its corresponding beam transmittance.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/volume_rendering_in_offline_renderer/volume_rendering.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;In order to derive from the first equation to the second one, we need to handle one small issue. Notice that the first equation proceed along the direction w, while the second one goes just in the opposite way. I choose to invert the first equation in my derivation. It becomes this:&lt;/p&gt;

$$ \dfrac{dL(p,-\omega)}{d(-t)} = -\sigma_t(p,-\omega) L(p , -\omega ) &amp;#43; S( p , -\omega ) $$
&lt;p&gt;One subtle difference between this equation and the original one is that the differential denominator is -t instead of t. This is by no means a typo. This is because we invert the direction of the ray along -w. If we take the negative sign out of the denominator, we get the following equation&lt;/p&gt;

$$ \dfrac{dL(p,-\omega)}{d(t)} = \sigma_t(p,-\omega) L(p , -\omega ) - S( p , -\omega ) $$
&lt;p&gt;One can easily notice that this is quite standard first order linear differential equation. The solution is available as the following one:&lt;/p&gt;

$$ L(p(t)) = \dfrac{1}{e^{-\int \sigma (p)dt}}( -\int e^{- \int \sigma (p)dt} S(p) dt &amp;#43; C ) $$
&lt;p&gt;I&amp;rsquo;ll skip the derivation of the solution in this blog, one can find further detail here. And please also notice that the subscript of $ \sigma $ and the $ \omega $ are also dropped because they don&amp;rsquo;t change in the equation, but please notice that the L(p) in the following equation means L(p,-w). To solve the equation, the first thing to do is to determine the constant factor C. We already know that $ L(p( \infty)) $ is zero, dropping it in the equation, we can get C.&lt;/p&gt;

$$ C = \int e^{\int \sigma (p)dt} S(p) dt |_\infty $$
&lt;p&gt;Again, we get the following equation by dropping the value of C in it.&lt;/p&gt;

$$ \begin{array} {lcl} L(p(t)) &amp;amp; = &amp;amp; \dfrac{1}{e^{-\int \sigma (p)dt}} \int_t^{\infty} e^{- \int \sigma (p)dt&amp;#39;} S(p) dt&amp;#39; \\\\ &amp;amp; = &amp;amp; \int_t^{\infty} e^{- \int_{t}^{t&amp;#39;} \sigma (p)dt&amp;#39;&amp;#39;} S(p) dt&amp;#39; \end{array} $$
&lt;p&gt;What we want is actually L(p(0)), letting t equals to zero, will give you the following equation:&lt;/p&gt;

$$  L(p(0)) = \int_0^{\infty} e^{- \int_{0}^{t} \sigma (p)dt&amp;#39;} S(p) = \int_0^{\infty} T_r(p(t) \rightarrow p(0)) S(p(t)) dt $$
&lt;p&gt;And with the above equation, we finished the derivation.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering, second edition&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://www.math.ucdavis.edu/~thomases/W11_16C1_lec_1_7_11.pdf&#34;&gt;UCDavis, Mathematics. First order linear differential equation&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://www.shadertoy.com/view/XlBSRz#&#34;&gt;Volumetric Integration&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image Based Lighting in Offline and Real-time Rendering</title>
      <link>https://agraphicsguynotes.com/posts/image_based_lighting_in_offline_and_realtime_rendering/</link>
      <pubDate>Wed, 07 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/image_based_lighting_in_offline_and_realtime_rendering/</guid>
      <description>&lt;p&gt;Image-based lighting is a practical way to enhance the visual quality of computer graphics. I used to be confused by it until I read the book &amp;ldquo;&lt;a href=&#34;https://books.google.com/books?hl=zh-CN&amp;amp;lr=&amp;amp;id=w1i_1kejoYcC&amp;amp;oi=fnd&amp;amp;pg=PP2&amp;amp;dq=high+dynamic+range+imaging+2nd+edition&amp;amp;ots=4i3Y3AMLtz&amp;amp;sig=qsyJ3UxHv4XuxRKGFauG2G-kOXY#v=onepage&amp;amp;q=high%20dynamic%20range%20imaging%202nd%20edition&amp;amp;f=false&#34;&gt;High Dynamic Range Imaging&lt;/a&gt;&amp;rdquo;, which provides a very clear explanation about IBL. And I actually have implemented the algorithm in my offline renderer before, it was just that I didn&amp;rsquo;t know it is IBL. The book &lt;a href=&#34;https://pbrt.org/&#34;&gt;PBRT&lt;/a&gt; has some materials talking about it without explicitly mentioning the term. Following images are generated with IBL in my renderer, except that the last one uses a single directional light.&lt;/p&gt;
&lt;p&gt;

&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ibl1.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ibl1.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ibl3.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ibl3.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;



&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ibl2.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ibl2.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/non_ibl.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/non_ibl.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;As we can see from the above images, IBL generated images looks way more promising than the one with a directional light. The real beauty of it is that with physically based shading everything looks great in different lighting environments without the need to change material parameters at all.&lt;/p&gt;
&lt;h1 id=&#34;ibl-in-offline-rendering&#34;&gt;IBL in Offline Rendering&lt;/h1&gt;
&lt;p&gt;Unlike IBL in real-time rendering, offline ray tracers can easily achieve unbiased result with IBL without compromising anything. The techs involved in generating the above images are introduced in my previous blogs, like
&lt;a href=&#34;https://agraphicsguynotes.com/posts/basics_about_path_tracing/&#34;&gt;path tracing&lt;/a&gt;,
&lt;a href=&#34;https://agraphicsguynotes.com/posts/monte_carlo_integral_with_multiple_importance_sampling/&#34;&gt;Monte-Carlo Integration&lt;/a&gt;,
&lt;a href=&#34;https://agraphicsguynotes.com/posts/sample_microfacet_brdf/&#34;&gt;Importance sampling for microfacet BRDF&lt;/a&gt;.
I won&amp;rsquo;t repeat them again in this blog.&lt;/p&gt;
&lt;p&gt;First thing first, what is image based lighting. My take on it is that IBL uses a special HDR map recording all incoming radiance from different solid angles to simulate complex lighting environment. It is pretty much like the spherical reflection map we used to render reflective materials in the old days. For old-school reflective materials in real time rendering, only one texture sampling is necessary, which makes the algorithm feasible even in fixed function pipe. Since not all materials need only one texture sampling, using the HDR image to render other materials is a challenging task. The essential difference between the two lies in the fact that pure reflective BRDF is actually delta function, which will reduce the dimension of the integral. In offline rendering, since speed is not the most sensitive thing, one can always use Monte-Carlo method to evaluate the integral of rendering equation. So, in a nut-shell, IBL is a technique that uses HDR environment light to light 3d objects in the scene.&lt;/p&gt;
&lt;p&gt;Below is an HDR image used in the first images shown above. Of course, it has been converted into LDR since majority displays on the market doesn&amp;rsquo;t support HDR.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/glacier_latlong.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/glacier_latlong.png&#34; width=&#34;700&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;The only important thing that is not mentioned in my previous blog is how to do importance sampling with the HDR image. It is actually quite simple, just a discretized inversion method. Please refer to the book &lt;a href=&#34;https://pbrt.org/&#34;&gt;PBRT&lt;/a&gt; for further detail implementation of it.&lt;/p&gt;
&lt;h1 id=&#34;ibl-in-real-time-rending-ue4-implementation&#34;&gt;IBL in Real Time Rending (UE4 implementation)&lt;/h1&gt;
&lt;p&gt;UE4&amp;rsquo;s implementation of IBL is really impressive. Usually, 1024 samples may be needed to generate a noise-free image. However, 1024 times sampling is nothing practical in a real-time rendering engine, it will make the rendering application heavily bandwidth bounded. UE4 uses a tricky way to resolve the issue of the excessive number of samplings, reducing it from a thousand to two samplings and making IBL a practical algorithm in it. Of course, it is an estimation of the rendering equation. That said it is far from unbiased. However, truth is there are biases everywhere in real time rendering, one extra source of bias doesn&amp;rsquo;t even hurt at all.&lt;/p&gt;
&lt;p&gt;The slide is here. However, one can easily get lost like I did during the introduction of IBL in this slide. Luckily, the note explains everything in a very clear manner.&lt;/p&gt;
&lt;h1 id=&#34;split-sum-approximation&#34;&gt;Split Sum Approximation&lt;/h1&gt;
&lt;p&gt;I will skip the original rendering equation and its Monte Carlo integration. We need to evaluate the following equation in a very efficient manner.&lt;/p&gt;

$$ \dfrac{1}{N} \sum_{k=1}^{N} \dfrac{L_i(l_k) f(l_k,v) cos\theta_{l_k}}{p(l_k,v)} \approx (\dfrac{1}{N} \sum_{k=1}^{N} L_i(l_k) ) (\dfrac{1}{N} \sum_{k=1}^{N} \dfrac{f(l_k,v) cos\theta_{l_k}}{p(l_k,v)} ) $$
&lt;p&gt;The approximation is not exactly 100% accurate. However, the bias introduced in a real rendering scene can be barely noticed.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/s2013_pbs_epic_slides-17-dragged.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/s2013_pbs_epic_slides-17-dragged.png&#34; width=&#34;700&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;The above slide comes from UE4&amp;rsquo;s presentation in Siggraph 2013, I can barely tell the difference between the balls in the first two rows. Clearly, the split sum is an acceptable approximation in real time rendering. However, the goal here is to reduce the number of sampling in IBL tech, splitting the equation is only the beginning, it doesn&amp;rsquo;t reduce anything yet at all. The golden idea here is to pre-compute some computation heavy task first before real-time rendering happens.&lt;/p&gt;
&lt;h1 id=&#34;pre-filter-environment-map&#34;&gt;Pre-Filter Environment Map&lt;/h1&gt;
&lt;p&gt;Following is the first part of the split sum, it is relatively easy to pre-compute an estimation.&lt;/p&gt;

$$ \dfrac{1}{N} \sum_{k=1}^{N} L_i(l_k) $$
&lt;p&gt;Averaging the radiance for different roughness will yield a good result and we can actually store textures for each roughness value into different mipmap levels. However, UE4 convolves the environment map with the GGX distribution of their shading model using importance sampling. One minor detail is that since microfacet BRDF is also a function of incoming angle, they actually made an assumption that the incoming angle is actually zero, in other words, the incoming direction is exactly the opposite of the surface normal. Of course, it introduces another source of bias here.&lt;/p&gt;
&lt;h1 id=&#34;environment-brdf&#34;&gt;Environment BRDF&lt;/h1&gt;
&lt;p&gt;This is actually where the most magic happens. Let&amp;rsquo;s first look at the original integral form of the equation.&lt;/p&gt;

$$ \int_H f(w_i,w_o) cos\theta_{w_i} dw_i $$
&lt;p&gt;By assuming the BRDF is actually microfacet model, we have the form of BRDF like this:&lt;/p&gt;

$$ f(\omega_i,\omega_o,x) = \dfrac{F(\omega_i , h) G(\omega_i,\omega_o,h) D(h)}{4 cos(\theta_i) cos(\theta_o)} $$
&lt;p&gt;Schlick&amp;rsquo;s approximation of Fresnel is used in it:&lt;/p&gt;

$$ F(w_i,h) = F_0 &amp;#43; ( 1 - F_0 ) ( 1 - w_i h )^5 $$
&lt;p&gt;In order to pre-compute what we need in shading, we need to know how many parameters there are in the BRDF. The obvious ones are the incoming direction, outgoing direction, roughness, albedo (or direction-hemisphere reflectance). Since what we care is how to compute the integral efficiently, the incoming direction is not our concern at all, leaving us five dimensions, out going direction, roughness and albedo ( three dimensions here because it is a color ). One may wonder here that isn&amp;rsquo;t outgoing direction two-dimensional? The truth is only the cosine factor of the angle between it and the normal is relevant in a D-term, thus reducing it to one single dimension. Sadly a five dimensional data is really hard to pre-computed before-hand.&lt;/p&gt;
&lt;p&gt;As a matter of fact, a simple math trick will factor albedo out of the equation:&lt;/p&gt;

$$ \int_H f(w_i,w_o) cos\theta_{w_i} dw_i = F_0 \int_H \frac{f(w_i,w_o) cos\theta_{w_i}}{F( w_i, h )} ( 1 - ( 1 - w_i h )^5 ) d w_i &amp;#43;\int_H \frac{f(w_i,w_o) cos\theta_{w_i}}{F( w_i, h )} ( 1 - w_i h )^5 d w_i $$
&lt;p&gt;Don&amp;rsquo;t be scared by the complex equation, it is actually clearer than it used to be before. Now, we only need to evaluate the first and second integral on the right side before-hand, leaving the F0 or albedo evaluation in the real time rendering.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/image_based_lighting_in_offline_and_realtime_rendering/env_brdf.png&#34; width=&#34;300&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;With only two dimensions in the equation, we can save the pre-computed information in one single texture. And the above texture is one example of it. During pixel shading, one can simply estimate the integral with the following calculation:&lt;/p&gt;

$$ EnvBRDF = Color * tex2d( cos\theta_o , roughness ).x &amp;#43; tex2d( cos\theta_o , roughness ).y $$
&lt;p&gt;It looks simpler enough to me. Only one texture sampling to evaluate the second integral of the split sum!&lt;/p&gt;










    
    &lt;link href=&#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css&#34; rel=&#34;stylesheet&#34;&gt;



    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;



&lt;style&gt;
#slide-window {
    position: relative;
    width: 750px;
    height: 421px;
    overflow: hidden;
    margin-left: auto;
    margin-right: auto;
}

#slides-list {
    width: 750px;
    height: 421px;
    position: absolute;
    margin: 0px;
    padding: 0px;
    -webkit-transform: translate3d(0px, 0px, 0px);
    transform: translate3d(0px, 0px, 0px);
    transition: all 0.66s ease;
    -webkit-transition: all 0.66s ease;
}

.slide {
    list-style: none;
    position: relative;
    float: left;
    margin: 0;
    padding: 0;
    width: 750px;
    height: 421px;
    background: #ccc;
    text-align: center;
    line-height: 100%;
    background-size: cover;
    background-position: 50% 50%;
    color: #fff;
    -webkit-transform: translate3d(0px, 0px, 0px);
    -webkit-transform-style: preserve-3d;
}

.nav {
    position: relative;
    z-index: 9;
    top: 45%;
    cursor: pointer;
    color: #fff;
    opacity: 0.7;
    transition: all 0.66s ease;
    -webkit-transition: all 0.66s ease;
}

.nav:hover {
    opacity: 1.0;
}

#left {
    left: 3%;
    float: left;
}

#right {
    right: 3%;
    float: right;
}
&lt;/style&gt;

&lt;div id=&#34;slide-window&#34;&gt;
    &lt;ol id=&#34;slides-list&#34;&gt;
        
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ca.png);&#34;&gt;&lt;/li&gt;
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ssa.png);&#34;&gt;&lt;/li&gt;
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/image_based_lighting_in_offline_and_realtime_rendering/ref.png);&#34;&gt;&lt;/li&gt;
        
    &lt;/ol&gt;
    &lt;span class=&#34;nav fa fa-chevron-left fa-3x&#34; id=&#34;left&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;nav fa fa-chevron-right fa-3x&#34; id=&#34;right&#34;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;script&gt;
sliderJQuery = jQuery.noConflict();
sliderJQuery(function( $ ) {
    $.global = new Object();
    $.global.total = 0;

    $(document).ready(function () {
        var slideWindowWidth = $(&#39;#slide-window&#39;).width();
        var slideCount = $(&#39;#slides-list li&#39;).length;
        var totalSlidesWidth = slideCount * slideWindowWidth;

        $.global.item = 0;
        $.global.total = slideCount;

        $(&#39;.slide&#39;).css(&#39;width&#39;, slideWindowWidth + &#39;px&#39;);
        $(&#39;#slides-list&#39;).css(&#39;width&#39;, totalSlidesWidth + &#39;px&#39;);

        $(&#39;#left&#39;).click(function () {
            resetAutoSlide();
            performSlide(&#39;back&#39;);
        });

        $(&#39;#right&#39;).click(function () {
            resetAutoSlide();
            performSlide(&#39;forward&#39;);
        });

    });

    function performSlide(direction) {
        if (direction == &#39;back&#39;) {
            var nextSlideId = $.global.item - 1;
        }
        if (direction == &#39;forward&#39;) {
            var nextSlideId = $.global.item + 1;
        }

        if (nextSlideId == -1) {
             
            moveCss($.global.total - 1);
        } else if (nextSlideId == $.global.total) {
             
            moveCss(0);
        } else {
             
            moveCss(nextSlideId);
        }
    }

    function moveCss(nextSlideId) {
        var slideWindowWidth = $(&#39;#slide-window&#39;).width();
        var margin = slideWindowWidth * nextSlideId;

        $(&#39;#slides-list&#39;).css(&#39;transform&#39;, &#39;translate3d(-&#39; + margin + &#39;px,0px,0px)&#39;);

        $.global.item = nextSlideId;
    }

    
      var autoSlide = parseInt(&#34;0&#34;, 10);
      var autoSlideInterval;
      function resetAutoSlide(){
        if(autoSlide) {
          if(autoSlideInterval) {
            clearInterval(autoSlideInterval);
          }
          autoSlideInterval = setInterval(function(){
            performSlide(&#39;forward&#39;);
          }, autoSlide)
        }
      }
      resetAutoSlide();
});
&lt;/script&gt;
&lt;p&gt;Above is a comparison between the reference, split sum and the simplified method in real time rendering. Pretty cool approximation.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] High Dynamic Range Imaging&lt;br&gt;
[2] &lt;a href=&#34;http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_slides.pdf&#34;&gt;Real Shading in UE4&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Shading&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instant Radiosity in my Renderer</title>
      <link>https://agraphicsguynotes.com/posts/instant_radiosity_in_my_renderer/</link>
      <pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/instant_radiosity_in_my_renderer/</guid>
      <description>&lt;p&gt;I read about this instant radiosity algorithm in the book physically based rendering 3rd these days. It is mentioned as instant global illumination though, they are actually the same thing. I thought it should be a good algorithm until I have implemented in renderer, I&amp;rsquo;m afraid that it is not quite an efficient one. Although it is also unbiased like path tracing and bidirectional path tracing, the convergence speed is just terribly low comparing with the others. It can barely show pure specular materials objects, it definitely needs special handling on delta bsdf. Since it is already implemented, I&amp;rsquo;ll put some notes on it.&lt;/p&gt;
&lt;h1 id=&#34;basic-idea&#34;&gt;Basic Idea&lt;/h1&gt;
&lt;p&gt;Instant Radiosity is pretty close to light tracing. Both of the algorithms trace rays from light sources instead of camera. The only difference between the two algorithm is where they connect vertices. In a light tracing algorithm, vertices along the light path are connected to camera directly. However in an instant radiosity algorithm, primary rays are still generated. Light path vertices are connected to the primary ray intersections then. The two algorithms are subset of bidirectional path tracing. Light tracing counts the path with only one vertex (eye vertex) in the eye path and instant radiosity only takes two-vertices length eye path into account.&lt;/p&gt;
&lt;p&gt;The other big difference is that the light path is not for per-sample any more. With vertices in a number of light paths pre-calculated, all of the pixel samples use the same set of vertices instead of generating them during their radiance evaluation. I think that is where it differs most from other algorithms. In some senses, it can be explained this way, many virtual point light sources are distributed in the scene before evaluating each pixel value. After these virtual lights are well distributed, further evaluation of radiance can only consider &amp;lsquo;direct illumination&amp;rsquo; w.r.t to both of real light sources and virtual light sources.&lt;/p&gt;
&lt;p&gt;An interesting fact of this algorithm is that it converges to the correct result in a quite different way comparing with other algorithms, like path tracing. In a path tracer, if you have less number of samples, you usually get noisy results. While the results with less number of light paths in instant radiosity look more likely to be smoothly illuminated with a couple of light sources, just matches the above explanation. See the following images generated with roughly same amount of time by instant radiosity and path tracing, the left one calculated by instant radiosity gets quite smoother shading, while the right one is pretty noisy. I can&amp;rsquo;t say smoother is better, we definitely have some noticeable artifacts on the left result. First, the virtual shadows easily catch our attention even if there is only one real point light in the cornell box. Second, it is totally black on this right mirror like box. Third, we have some hotspot around the corner inside this cornell box.&lt;/p&gt;


&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/ir.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/ir.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/pt.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/pt.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;Of course, that is not to say it is a biased algorithm, it is just because we have only limited number of light paths generated in preprocess stage. With more light paths generated, those artifacts should be gone eventually. However, only the first issue can be hidden in a reasonable speed. The second issue usually needs explicit handling, like the way we handle it in a whitted ray tracer. We&amp;rsquo;ll talk about how to handle the last one later.&lt;/p&gt;
&lt;h1 id=&#34;math-behind-it&#34;&gt;Math Behind It&lt;/h1&gt;
&lt;p&gt;The math behind it is much less complex than MIS bidirectional path tracer mentioned on my previous post. And since it is necessary to trace primary rays too, we don&amp;rsquo;t need to consider primary ray pdf because it is cancelled with importance function. It leaves us really simple mathematics behind the algorithm. We just need to evaluate the following equation correctly:&lt;/p&gt;

$$ L(p,w_i) = {\int...\int} \prod_{i=1}^{k-1}(f_r(x_{i&amp;#43;1} \rightarrow x_i \rightarrow x_{i-1} )G(x_i\longleftrightarrow x_{i&amp;#43;1})) L_e(x_k\rightarrow x_{k-1}) d(x_2) d(x_3) ... d(x_k) $$
&lt;p&gt;This is not the whole rendering equation, it just stands for radiance value contributed by k+1 vertices length path, including both camera vertex and light vertex. For an unbiased renderer, it needs to evaluate the above equation for every possible k value, where k usually starts from 2 in the case of direct visible light. For a Monte Carlo estimator, it uses the following equation to approach the integral:&lt;/p&gt;

$$ L(p,w_i) = \dfrac{1}{N}\Sigma_{i=1}^N(\dfrac{\prod_{i=1}^{k-1}(f_r(x_{i&amp;#43;1}\rightarrow x_i\rightarrow x_{i-1})G(x_i\longleftrightarrow x_{i&amp;#43;1}))L_e(x_k\rightarrow x_{k-1})}{\prod_{i=2}^{k} p_a(x_i)}) $$
&lt;p&gt;Since there are two stages in this algorithm, we can only evaluate part of the equation in the first stage, it is the part that doesn&amp;rsquo;t involve the x1 and x0, they should be two bsdf evaluation and a gterm.&lt;/p&gt;

$$ \begin{array} {lcl} L(p,w_i) &amp;amp;=&amp;amp; \dfrac{1}{N}\Sigma_{i=1}^N(\underbrace{f_r(x_2\rightarrow x_1 \rightarrow x_0)G(x_1\longleftrightarrow x_2)f_r(x_3\rightarrow x_2 \rightarrow x_1)}_{sample\, radiance\, evaluation\, in\, stage2} \\\\ &amp;amp;=&amp;amp; \prod_{i=3}^{k-1}(\underbrace{\dfrac{f_r(x_{i&amp;#43;1}\rightarrow x_i\rightarrow x_{i-1})G(x_i\longleftrightarrow x_{i-1})}{p_a(x_{i-1})}}_{stored\,in\,vertex\, during\, light\, path\, tracing\, in\, stage1})\dfrac{G(x_k\longleftrightarrow x_{k-1})L_e(x_k\rightarrow x_{k-1})}{p_a(x_{k-1})p_a(x_k)}) \end{array} $$
&lt;p&gt;Although it may look more complex by a first look, however it is actually more clear for implementing the algorithm. As we can see from the above equation, the first two brdf and g-term are to be evaluated in radiance sampling in the second stage. While the rest of the equation should be done in the pre-process stage 1, which distributes virtual point lights around the whole scene. And it can be done in an incremental way, just like we trace rays in a path tracer. To be simpler, the above equation can be further simplified since we don&amp;rsquo;t actually pick samples w.r.t area, we sample new vertex by bsdf pdf w.r.t to the solid angle.&lt;/p&gt;

$$ \begin{array} {lcl} L(p,w_i) &amp;amp;=&amp;amp; \dfrac{1}{N}\Sigma_{i=1}^N(\underbrace{f_r(x_2\rightarrow x_1 \rightarrow x_0)G(x_1\longleftrightarrow x_2)f_r(x_3\rightarrow x_2 \rightarrow x_1)}_{sample\, radiance\, evaluation\, in\, stage2} \\\\ &amp;amp; &amp;amp; \prod_{i=3}^{k-1}(\underbrace{\dfrac{f_r(x_{i&amp;#43;1}\rightarrow x_i\rightarrow x_{i-1})cos\theta_{i\rightarrow i-1}}{p_w(x_i\rightarrow x_{i-1})}}_{stored\,in\,vertex\, during\, light\, path\, tracing\, in\, stage1})\dfrac{cos\theta_{k\rightarrow k-1}L_e(x_k\rightarrow x_{k-1})}{p_w(x_k \rightarrow x_{k-1})p_a(x_k)}) \end{array} $$
&lt;p&gt;This is pretty clear for implementing the algorithm. After the first stage is done, we only need to trace one ray segment for evaluate each virtual point light&amp;rsquo;s contribution, no matter how long the path actually is.&lt;/p&gt;
&lt;h1 id=&#34;handling-the-artifacts&#34;&gt;Handling the Artifacts&lt;/h1&gt;
&lt;p&gt;As we can see from the above comparison, we can&amp;rsquo;t get anything from the mirror like box because it is almost delta function. In a practical ray tracer, it definitely needs special treatment for delta bsdf. Sadly, there is no delta bsdf support in my renderer, the mirror like material is actually a microfacet bsdf with 0 as roughness value. I&amp;rsquo;m not sure if there is a practical way to handle materials like this.&lt;/p&gt;
&lt;p&gt;The other artifact that we can handle is those hot spots at the corner of the Cornell box. Those high lighted area is caused by connecting quite near vertices, resulting in a very large g-term value. According to the book, advanced global illumination, we can add a very small bias in the denominator of gterm to avoid those large values so that we can remove the hot spots by introducing bias into the method, which is unnoticeable. I can&amp;rsquo;t say that I agree with the idea, it is totally visible to me. Pbrt book introduces a great way of removing those ugly hot spot, it works pretty well to me. However I made a tiny change in the original method. My method is to clamp the inverse of the squared distance in the gterm when connecting vertices. Clamping it will definitely introduce a bias which is not something that I&amp;rsquo;d like to see, in order to get our unbiased feature back, we&amp;rsquo;ll add more code to handle it.&lt;/p&gt;

$$ \begin{array} {lcl} G(x_1,x_2) &amp;amp;=&amp;amp; \dfrac{cos\theta_{1\rightarrow 2} cos\theta_{2\rightarrow 1}}{|x_1 - x_2|} \\ &amp;amp;=&amp;amp; cos\theta_{1\rightarrow 2} cos\theta_{2\rightarrow 1} ( min( \dfrac{1}{|x_1-x_2|} , g_{clamp} ) &amp;#43; max( \dfrac{1}{|x_1-x_2|} - g_{clamp} , 0 ) ) \\ &amp;amp;=&amp;amp; \underbrace{cos\theta_{1\rightarrow 2} cos\theta_{2\rightarrow 1} min( \dfrac{1}{|x_1-x_2|} , g_{clamp} )}_{evaluate\, the\, same\, way} &amp;#43; \underbrace{cos\theta_{1\rightarrow 2} cos\theta_{2\rightarrow 1} max( \dfrac{1}{|x_1-x_2|} - g_{clamp} , 0 )}_{evaluate\, recursively} \end{array} $$

$$ G_0(x_1,x_2)=cos\theta_{1\rightarrow 2} cos\theta_{2\rightarrow 1} min(\dfrac{1}{|x_1-x_2|},g_{clamp}) $$

$$ G_1(x_1,x_2)=cos\theta_{1\rightarrow 2} cos\theta_{2\rightarrow 1} max(\dfrac{1}{|x_1-x_2|}-g_{clamp},0) $$

$$ G(x_1,x_2)=G_0(x_1,x_2)&amp;#43;G_1(x_1,x_2) $$
&lt;p&gt;Now we&amp;rsquo;ve split the equation into two different parts and it equals to the original equation. We can them divide the extended rendering equation into two respectively.&lt;/p&gt;

$$ L(p,w_i) = L_0(p,w_i) &amp;#43; L_1(p,w_i) $$

$$ L_0(p,w_i) = {\int...\int} G_0(x_1,x_2) f_r(x_2 \rightarrow x_1 \rightarrow x_0) \prod_{i=2}^{k-1}(f_r(x_{i&amp;#43;1} \rightarrow x_i \rightarrow x_{i-1} )G(x_i\longleftrightarrow x_{i&amp;#43;1})) L_e(x_k\rightarrow x_{k-1}) d(x_2) d(x_3) … d(x_k) $$

$$ L_1(p,w_i) = {\int...\int} G_1(x_1,x_2) f_r(x_2 \rightarrow x_1 \rightarrow x_0) \prod_{i=2}^{k-1}(f_r(x_{i&amp;#43;1} \rightarrow x_i \rightarrow x_{i-1} )G(x_i\longleftrightarrow x_{i&amp;#43;1})) L_e(x_k\rightarrow x_{k-1}) d(x_2) d(x_3) ... d(x_k) $$
&lt;p&gt;The first part clamps the value to a maximum limit in order to avoid high radiance value by connecting two near vertices. Really simple, there is nothing to say about it.&lt;/p&gt;
&lt;p&gt;The second part is where the trick is. It would make no difference if we evaluate the equation the same way, because there is no upper limit on the inverse squared distance term. Instead of connecting the primary ray intersection with light path vertices, we sample a new ray based on the bsdf pdf, exactly like the way we do in a path tracer and then evaluate the radiance value recursively so that we can skip the super near vertex connection. Here is the math proof why it eliminates the near connection, only relative parts are shown:&lt;/p&gt;

$$ max( \dfrac{1}{|x_1-x_2|} - g_{clamp} , 0 ) dA(x_2) = max( 1 - \dfrac{g_{clamp} | x_2 - x_1 |}{cos\theta_{1 \rightarrow 2}}) dW(x_2) $$
&lt;p&gt;The squared distance switches from denominator to the numerator, that&amp;rsquo;s why we won&amp;rsquo;t be affected by short distance vertices connection. A detail here is that during the recursive radiance evaluation, we treat the secondary ray as fake primary ray and the virtual light source which is very near to it won&amp;rsquo;t connect to it again because virtual light sources don&amp;rsquo;t affect directly illumination in the algorithm. A interesting fact here is that if g_clamp is 0, it switches from instant radiosity to path tracing.&lt;/p&gt;
&lt;h1 id=&#34;limitation-of-thealgorithm&#34;&gt;Limitation of the Algorithm&lt;/h1&gt;
&lt;p&gt;To tell the truth, I can&amp;rsquo;t justify many reasons to use the algorithm comparing with others, it converges quite slowly for mirror like materials and shows high light at corners. Even with the above trick, it is still hard to get similar result with other methods with only limited number of light paths, by limited number of light paths, I am talking about thousands. I tried 1024 light paths generated in the preprocess, it still can&amp;rsquo;t eliminate those artifacts. See the below images, left result is from instant radiosity, right one uses MIS bidirectional path tracing. The first one uses almost double time than the bdpt result. And that already gives me enough reason to switch to other more robuse algorithms like path tracing, bidirectional path tracing.&lt;/p&gt;


&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/ir_1024.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/ir_1024.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/bdpt_64.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/instant_radiosity_in_my_renderer/bdpt_64.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering, 3rd&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.advancedglobalillumination.com/&#34;&gt;Advanced Global Illumination&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.cs.cornell.edu/courses/cs6630/2012sp/slides/Boyadzhiev-Matzen-InstantRadiosity.pdf&#34;&gt;Instant Radiosity&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Missing Primary Ray PDF in Path Tracing</title>
      <link>https://agraphicsguynotes.com/posts/the_missing_primary_ray_pdf_in_path_tracing/</link>
      <pubDate>Thu, 04 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/the_missing_primary_ray_pdf_in_path_tracing/</guid>
      <description>&lt;p&gt;I was always wondering why don&amp;rsquo;t we take the PDF of primary ray into account in a path tracer. Sadly there aren&amp;rsquo;t many resources available explaining it. I guess the book Physically based rendering 3rd will provide some explanation, however it is not released yet. After some searching on the internet, I finally got something to explain it. It actually gets cancelled with the terms in importance function and LTE. It gets cancelled in a very elegant way that we don&amp;rsquo;t need to put any resources on it at all, which is why many open-source ray tracer don&amp;rsquo;t consider it in the first place. In this blog, I&amp;rsquo;m gonna explain the detailed math behind the whole theory.&lt;/p&gt;
&lt;h1 id=&#34;primary-ray-pdf&#34;&gt;Primary Ray PDF&lt;/h1&gt;
&lt;p&gt;First thing first, what is the pdf of primary ray.
Assuming we have an imaginary image plane placing at a specific distance from the camera point so that area of each pixel in this image plane is exactly one. Since we already know the resolution of the image, we can calculate the distance with the following equation:&lt;/p&gt;

$$ d = \dfrac{res_h}{tan(fov_y)} $$
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/the_missing_primary_ray_pdf_in_path_tracing/1147198.jpg&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Here is a ray generating through a pixel, we can also get the distance from the eye point and the intersection between the ray and the target pixel.&lt;/p&gt;

$$ r = \dfrac{d}{cos\theta} $$
&lt;p&gt;Since the area of the pixel is exactly one, the pdf w.r.t area of sampling a point on the pixel is also one. So the pdf w.r.t solid angle of sampling that point from eye point is:&lt;/p&gt;

$$ pdf_w = \dfrac{r^2}{cos\theta} $$
&lt;p&gt;Putting them all together, you will get the following equation:&lt;/p&gt;

$$ pdf_w = (\dfrac{res_h}{tan(fov_y)})^2 \dfrac{1}{cos^3 \theta} $$
&lt;p&gt;The $ (\dfrac{res_h}{tan(fov_y)})^2 $ won&amp;rsquo;t change if image resolution is a fixed number, which is very common most of the time.&lt;/p&gt;
&lt;p&gt;Besides the pdf w.r.t solid angle, we also need to pick a single point on the virtual aperture for simulating depth of field effect. The pdf w.r.t area of sampling this point is very simple, just the inverse of the aperture area.&lt;/p&gt;

$$ pdf_a = \dfrac{1}{area_{aperture}} $$
&lt;h1 id=&#34;measurement-equation&#34;&gt;Measurement Equation&lt;/h1&gt;
&lt;p&gt;The second thing we need to be clear with is what is stored in a pixel. In a very naive ray tracer, it is usually the generated radiance. However it is much more complex if we consider more. Here is a precise equation showing pixel information:&lt;/p&gt;

$$ I = \int_{A_{pixel}}\int_A W_e(p_0\rightarrow p_1)L(p_1\rightarrow p_0) G( p_0 \longleftrightarrow p_1 ) dA(p_0) dA(p_1) $$
&lt;p&gt;I won&amp;rsquo;t show derivation of the equation since it is well out of the scope of this post, I suggest the book Physically based rendering 2rd for further detail on it. However in this book, &amp;lsquo;We&amp;rsquo; is assumed to be a dirac delta function, which is actually not. And we are focusing on things before the first intersection, so there is no need to expend the radiance term, we&amp;rsquo;ll just assume that it is already known. How to get the radiance value correctly is already mentioned in my previous &lt;a href=&#34;https://agraphicsguynotes.com/posts/basics_about_path_tracing/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Understanding this equation is very important for implementing something like path tracing and light tracing, especially when DOF is also considered. We&amp;rsquo;ve already been familiar with the radiance, G term and primary ray pdf, figuring out what &amp;lsquo;We&amp;rsquo; is is all left to do.&lt;/p&gt;
&lt;h2 id=&#34;importance-function&#34;&gt;Importance Function&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;We&amp;rsquo; term is also called importance function. Unfortunately, I&amp;rsquo;m not 100% sure that my derivation is a correct one. Since it works pretty well in my renderer, so I decided to go with it until I get a better understanding on it from somewhere else.&lt;/p&gt;
&lt;p&gt;First thing we should know about this importance function is that it has illumination fall off by a factor of $ cos^4 \theta $, where $ \theta $ is the angle between the primary ray and the camera forward direction. This thing usually manifest itself as Vignetting, where the corner of the image is a little bit darker than the center.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/the_missing_primary_ray_pdf_in_path_tracing/vignetting.jpg&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Here is a demonstration, as we can notice from the image, pixels at corners appear much more dimmer than the center ones. For a better explanation of this fall off thing, I found it kind of helpful.&lt;/p&gt;
&lt;p&gt;And in order to avoid the vignetting effect in our offline rendering, we can change the importance function, making it proportional the inverse of this $ cos^4 \theta$ so that we can cancel it in the first place.&lt;/p&gt;
&lt;p&gt;Besides this, we also need to consider sampling a point on the virtual aperture. The larger the aperture is, the more DOF effect we will get. In a real world scene, the larger your aperture is, the more light will feed your image sensor. For example, if you aperture is 4 times larger than before, you will have 4 times flux at your image sensor. Image will be 4 times brighter if nothing is done. Usually we need to reduce the shuttering window to 25% so that the total flux at the image sensor is roughly the same, producing images with similar average brightness. When we are doing offline rendering, we are actually capturing an image at a single time point instead of a window. So we are gonna have to put the reduction at somewhere else, that is the importance function we are talking about. That said, if your aperture is 4 times larger, importance function should be 4 times smaller so that we can get results with roughly the same amount of light.&lt;/p&gt;

$$ W_e=\dfrac{c}{area_{aperture} cos^4\theta} $$
&lt;p&gt;With the above equation, we need to figure out what c is. Let&amp;rsquo;s hold it for a while, we&amp;rsquo;ll just assume it is a constant so far.&lt;/p&gt;
&lt;h2 id=&#34;monte-carlo-evaluation&#34;&gt;Monte Carlo Evaluation&lt;/h2&gt;
&lt;p&gt;With Monte Carlo method, we can get the value of importance function mentioned above this way:&lt;/p&gt;

$$ \begin{array}{lcl}I&amp;amp;=&amp;amp;\dfrac{W_e(p_0\rightarrow p_1)L(p_1\rightarrow p_0)G(p_0\longleftrightarrow p_1)}{P_a(p_0)P_a(p_1)} \\\\ &amp;amp;=&amp;amp; \dfrac{W_e(p_0\rightarrow p_1)L(p_1\rightarrow p_0)cos\theta_0}{P_a(p_0)P_w(p_0\rightarrow p_1)} \\\\ &amp;amp;=&amp;amp; \dfrac{c * tan^2(fov_y)}{res_h^2}L(p_1\rightarrow p_0) \end{array} $$
&lt;p&gt;If we let c equals to $ \dfrac{res_h^2}{tan^2(fov_y)} $, we can easily cancel the prefix in this equation, making the evaluation exactly equals to the radiance value, which is exactly what we want in the first place. And it also matches the behavior in a naive ray tracer, storing radiance value directly in the result.&lt;/p&gt;
&lt;p&gt;In a short conclusion, if we make importance function this way:&lt;/p&gt;

$$ W_e=\dfrac{res_h^2}{tan^2(fov_y) area_{aperture} cos^4\theta} $$
&lt;p&gt;The Monte Carlo evaluation of the measurement equation is simply the radiance value:&lt;/p&gt;

$$ \begin{array}{lcl}I&amp;amp;=&amp;amp;\dfrac{W_e(p_0\rightarrow p_1)G(p_0\longleftrightarrow p_1)}{P_a(p_0)P_a(p_1)} L(p_1\rightarrow p_0) \\\\ &amp;amp;=&amp;amp; L(p_1\rightarrow p_0) \end{array} $$
&lt;p&gt;And that is why we don&amp;rsquo;t need to consider primary ray pdf in the first place because it gets cancelled before the first intersection. In a practical path tracer or others, there is totally no need to evaluate the complex equation since we already know that the throughput equals to one before we consider the radiance.&lt;/p&gt;
&lt;h1 id=&#34;why-bother-to-figure-it-out&#34;&gt;Why Bother to Figure it Out?&lt;/h1&gt;
&lt;p&gt;We came all the way to a value equals to one, why bother? The answer is simple, to make things more transparent. We can&amp;rsquo;t just store radiance value into images without knowing why we keep doing it. And the other very important reason is to perform correct calculation in a light tracing algorithm, which doesn&amp;rsquo;t trace primary ray at all. Without knowing the importance function, it will quickly confuse anyone who is trying to do it in the first time.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://dougkerr.net/Pumpkin/articles/Cosine_Fourth_Falloff.pdf&#34;&gt;Derivation of the “Cosine Fourth” Law for Falloff of Illuminance Across a Camera Image&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.pbrt.org/&#34;&gt;Physically based Rendering, 2rd&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Practical implementation of MIS in Bidirectional Path Tracing</title>
      <link>https://agraphicsguynotes.com/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/</link>
      <pubDate>Sat, 16 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/</guid>
      <description>&lt;p&gt;In my previous &lt;a href=&#34;https://agraphicsguynotes.com/posts/naive_bidirectional_path_tracing/&#34;&gt;post&lt;/a&gt;, I talked some basic stuff about naive bidirectional path tracing. However it is hard to show any real value since there are always too much noise comparing with best solutions depending on the scene to be rendered. That is because the contribution of each specific path is not properly weighted. And multiple importance sampling can be the key to the issue, the following comparison shows big difference between different methods. All of the images are generated with my open-source &lt;a href=&#34;https://sort-renderer.com&#34;&gt;renderer&lt;/a&gt;.&lt;/p&gt;










    
    &lt;link href=&#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css&#34; rel=&#34;stylesheet&#34;&gt;



    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;



&lt;style&gt;
#slide-window {
    position: relative;
    width: 600px;
    height: 600px;
    overflow: hidden;
    margin-left: auto;
    margin-right: auto;
}

#slides-list {
    width: 600px;
    height: 600px;
    position: absolute;
    margin: 0px;
    padding: 0px;
    -webkit-transform: translate3d(0px, 0px, 0px);
    transform: translate3d(0px, 0px, 0px);
    transition: all 0.66s ease;
    -webkit-transition: all 0.66s ease;
}

.slide {
    list-style: none;
    position: relative;
    float: left;
    margin: 0;
    padding: 0;
    width: 600px;
    height: 600px;
    background: #ccc;
    text-align: center;
    line-height: 100%;
    background-size: cover;
    background-position: 50% 50%;
    color: #fff;
    -webkit-transform: translate3d(0px, 0px, 0px);
    -webkit-transform-style: preserve-3d;
}

.nav {
    position: relative;
    z-index: 9;
    top: 45%;
    cursor: pointer;
    color: #fff;
    opacity: 0.7;
    transition: all 0.66s ease;
    -webkit-transition: all 0.66s ease;
}

.nav:hover {
    opacity: 1.0;
}

#left {
    left: 3%;
    float: left;
}

#right {
    right: 3%;
    float: right;
}
&lt;/style&gt;

&lt;div id=&#34;slide-window&#34;&gt;
    &lt;ol id=&#34;slides-list&#34;&gt;
        
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/bdpt.png);&#34;&gt;Bidirectional Path Tracing&lt;/li&gt;
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/nbdpt.png);&#34;&gt;Naive Bidirectional Path Tracing&lt;/li&gt;
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/pt.png);&#34;&gt;Path Tracing&lt;/li&gt;
        
            
            
            &lt;li class=&#34;slide&#34; style=&#34;background-image:url(/img/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/lt.png);&#34;&gt;Light Tracing&lt;/li&gt;
        
    &lt;/ol&gt;
    &lt;span class=&#34;nav fa fa-chevron-left fa-3x&#34; id=&#34;left&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;nav fa fa-chevron-right fa-3x&#34; id=&#34;right&#34;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;script&gt;
sliderJQuery = jQuery.noConflict();
sliderJQuery(function( $ ) {
    $.global = new Object();
    $.global.total = 0;

    $(document).ready(function () {
        var slideWindowWidth = $(&#39;#slide-window&#39;).width();
        var slideCount = $(&#39;#slides-list li&#39;).length;
        var totalSlidesWidth = slideCount * slideWindowWidth;

        $.global.item = 0;
        $.global.total = slideCount;

        $(&#39;.slide&#39;).css(&#39;width&#39;, slideWindowWidth + &#39;px&#39;);
        $(&#39;#slides-list&#39;).css(&#39;width&#39;, totalSlidesWidth + &#39;px&#39;);

        $(&#39;#left&#39;).click(function () {
            resetAutoSlide();
            performSlide(&#39;back&#39;);
        });

        $(&#39;#right&#39;).click(function () {
            resetAutoSlide();
            performSlide(&#39;forward&#39;);
        });

    });

    function performSlide(direction) {
        if (direction == &#39;back&#39;) {
            var nextSlideId = $.global.item - 1;
        }
        if (direction == &#39;forward&#39;) {
            var nextSlideId = $.global.item + 1;
        }

        if (nextSlideId == -1) {
             
            moveCss($.global.total - 1);
        } else if (nextSlideId == $.global.total) {
             
            moveCss(0);
        } else {
             
            moveCss(nextSlideId);
        }
    }

    function moveCss(nextSlideId) {
        var slideWindowWidth = $(&#39;#slide-window&#39;).width();
        var margin = slideWindowWidth * nextSlideId;

        $(&#39;#slides-list&#39;).css(&#39;transform&#39;, &#39;translate3d(-&#39; + margin + &#39;px,0px,0px)&#39;);

        $.global.item = nextSlideId;
    }

    
      var autoSlide = parseInt(&#34;0&#34;, 10);
      var autoSlideInterval;
      function resetAutoSlide(){
        if(autoSlide) {
          if(autoSlideInterval) {
            clearInterval(autoSlideInterval);
          }
          autoSlideInterval = setInterval(function(){
            performSlide(&#39;forward&#39;);
          }, autoSlide)
        }
      }
      resetAutoSlide();
});
&lt;/script&gt;
&lt;p&gt;Those images are generated with rough the same amount of time. No doubt about it, MIS BDPT dominates among all those results. It is less noisy and shows good caustics. Although light tracing can also shows good caustics, it is far from a practical algorithm due to the noise in the rest of the scene, not to mention it almost failed to show any radiance value on the glass monkey head. Traditional path tracing algorithm shows no caustics at all, not because it is a biased algorithm. It is unbiased for sure, however it just converges to the correct caustics in a unreasonable speed. Naive bidirectional path tracing also has roughly same amount of noise, however it also has dimmer monkey head because light tracing doesn&amp;rsquo;t do a good job on it. In other words, bidirectional path tracing barely reveals any value without MIS.&lt;/p&gt;
&lt;p&gt;I searched a lot of materials about MIS in BDPT, however there are only quite limited materials on the internet. Although some open source renderers, like luxrender, give detailed implementation, most of them doesn&amp;rsquo;t give any insight in the math behind it, without which one can be quickly confused by its code. &lt;a href=&#34;http://www.smallvcm.com/&#34;&gt;SmallVCM&lt;/a&gt; expends the algorithm further, offering a better solution over MIS BDPT and it has detailed paper on the math. However it is a little complex for someone who just wants to figure out how to do MIS BDPT. Eric Veach&amp;rsquo;s thesis gives the best explanation on MIS in BPDT,  sadly it doesn&amp;rsquo;t go further with MIS implementation. In this blog, I&amp;rsquo;m gonna talk something about MIS in bidirectional path tracing. Most of the theory comes from this &lt;a href=&#34;https://www.semanticscholar.org/paper/Implementing-Vertex-Connection-and-Merging-Georgiev/383a98f474dc482b9c1427d9408cf85a22e01dc0&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;pdf-of-a-specific-path&#34;&gt;PDF of a Specific Path&lt;/h1&gt;
&lt;p&gt;To use MIS in BDPT, it is very important to evaluate pdf of a specific path accurately. In my previous &lt;a href=&#34;https://agraphicsguynotes.com/posts/naive_bidirectional_path_tracing/&#34;&gt;post&lt;/a&gt;, I mentioned the path pdf in path tracing. In a bidirectional path tracing algorithm, it is more complex since the path starts from both sides. However the theory stays similar, we just need to be careful with marginal conditions.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m gonna use the term in the SmallVCM paper because it is more elegant and easy to read. For a path starting from one side, we define the forward pdf term:&lt;/p&gt;

$$ \tag{1} \overrightarrow{p_i}(\bar{y}) = \begin{cases} p_a(\bar{y})&amp;amp;:i=0 \\ \overrightarrow{p_{\sigma , i}}(\bar{y})\overrightarrow{g_i}(\bar{y})&amp;amp;:otherwise \end{cases} $$

$$ \tag{2} \overrightarrow{p_{\sigma , i}}(\bar{y}) =\begin{cases} {p_{\sigma}}(p_0\rightarrow p_1)&amp;amp;:i=1 \\ {p_{\sigma}}(p_{i-2}\rightarrow p_{i-1} \rightarrow p_{i})&amp;amp;:i\ge 2 \end{cases}$$

$$ \tag{3} \overrightarrow{g_i}(\bar{y}) = \dfrac{cos \theta_{i\rightarrow i-1}}{\|y_i - y_{i-1} \|^2}$$
&lt;p&gt; $ p_{\sigma} $ 
 denotes the pdf w.r.t solid angle,  $ p_a $ 
 represents the pdf w.r.t area. For the first  $ p_{\sigma} $ 
 , it is defined the type of light sources or camera sensor and brdf defines the rest of them. The  $ \overrightarrow{g} $ 
  term converts the pdf measured from solid angle to area. With these terms defined, we have the pdf of a specific path starting from one side with s vertices, including the vertex on light sources or camera sensor, as the following one:&lt;/p&gt;

$$ \tag{4}  p_s(\bar{y}) = \prod_{i=0}^{s-1}\overrightarrow{p_i}(\bar{y})$$
&lt;p&gt;The reverse pdf of sub-path starting from the other side is quite similar with the above ones. For simplicity, they are listed here:&lt;/p&gt;

$$ \tag{5} \overleftarrow{p_i}(\bar{y}) = \begin{cases} p_a(\bar{y})&amp;amp;:i=k \\ \overleftarrow{p_{\sigma , i}}(\bar{y})\overleftarrow{g_i}(\bar{y})&amp;amp;:otherwise \end{cases}$$

$$ \tag{6} \overleftarrow{p_{\sigma , i}}(\bar{y}) = \begin{cases} {lr}{p_{\sigma}}(p_{k-1}\leftarrow p_k)&amp;amp;:i={k-1} \\ {p_{\sigma}}(p_{i}\leftarrow p_{i&amp;#43;1} \leftarrow p_{i&amp;#43;2})&amp;amp;:i\le{k - 2} \end{cases} $$

$$ \tag{7} \overleftarrow{g_i}(\bar{y}) = \dfrac{cos \theta_{i\rightarrow i&amp;#43;1}}{\|y_i - y_{i&amp;#43;1} \|^2} $$

$$ \tag{8} p_t(\bar{y}) = \prod_{i=k&amp;#43;1-t}^{k}\overleftarrow{p_i}(\bar{y}) $$
&lt;p&gt;k is one less than the total number of vertices (including eye and light vertices) in the path, counting the two sub-path of light path and eye path. The different direction denotes that path is traced from a different direction other than the above one. Now we have the pdf of connecting two separate paths generated from each direction:&lt;/p&gt;

$$ \tag{9} p_{s,t}=p_s(\bar{y})p_t(\bar{y}) $$
&lt;p&gt;Calculating this value in an accurate manner is not only particularly important for evaluating the Monte Carlo integral, but also a must have for multiple importance sampling factor evaluation.&lt;/p&gt;
&lt;h1 id=&#34;mis-weight&#34;&gt;MIS weight&lt;/h1&gt;
&lt;p&gt;For a naive bidirectional path tracer, the weight for connecting two sub-paths is simply 1/(s+t+1) where s is the number of vertices in the eye sub-path and t is the number of vertices in the light sub-path, both counting the first vertex sampled on light source or camera sensor. Since we rarely consider the situation of path hitting the camera sensor, it is usually 1/(s+t). The exact form of this weight is dependent on how many cases we take into consideration during bidirectional path tracing. For example, if it there is only point (delta) light in the scene, it would be 1/(s+t-1) since it is impossible to find a ray hitting the light source, reducing the number of cases by one. Naive bpdt weight is very simple, however it rarely delivers good quality comparing with other methods. As we can see from the above comparison, apart from its noisy result image, things with glass material is much dimmer due to the super low convergence rate of light tracing on such material.&lt;/p&gt;
&lt;p&gt;The short conclusion here is that bidirectional path tracing provides no extra value with uniform weights. That is why we need to blend these intermediate results in a better way and it is multiple importance sampling that can be used to solve the very issue.&lt;/p&gt;
&lt;p&gt;The original form of MIS weight factor introduced in Eric Veach&amp;rsquo;s thesis is like this:&lt;/p&gt;

$$ \tag{10} \begin{array} {lcl} w_{s,t} &amp;amp; = &amp;amp; \dfrac{p_{s,t}^2}{\Sigma_{i=0}^{s&amp;#43;t} (p_{i,s&amp;#43;t-i}^2)} \\ &amp;amp; = &amp;amp; \dfrac{1}{\Sigma_{i=0}^{s&amp;#43;t}((p_{i,s&amp;#43;t-i}/p_{s,t})^2)} \end{array} $$
&lt;p&gt;Calculating all these pdfs directly is pretty boring and can easily introduce some subtle bugs which is very hard to be found. Instead of doing so, we simplify the equation first before actually implementing these weight evaluation.&lt;/p&gt;
&lt;p&gt;Since most of the components in  $ p_{i,t+s-i} 
$ and  $ p_{i+1,t+s-i-1} $ 
 are similar except those around the connection edge, the ratio is much simpler comparing with evaluating those two pdfs first and doing the divide later.&lt;/p&gt;

$$ \tag{11} \dfrac{p_{i&amp;#43;1,s&amp;#43;t-i-1}}{p_{i,s&amp;#43;t-i}} = \dfrac{\overrightarrow{p_i}(\bar{y})}{\overleftarrow{p_i}(\bar{y})} $$
&lt;p&gt;Suppose vertex 0 is on the camera sensor. By defining the following two terms, we can put equation 10 in a more simpler form.&lt;/p&gt;

$$ \tag{12} w_{camera,s-1}=\Sigma_{i=0}^{s-1}(p_{i,s&amp;#43;t-i}/p_{s,t})^2 $$

$$ \tag{13} w_{light,t-1}=\Sigma_{i=s&amp;#43;1}^{s&amp;#43;t}(p_{i,s&amp;#43;t-i}/p_{s,t})^2 $$

$$ \tag{14} \begin{array} {lcl} w_{s,t} &amp;amp; = &amp;amp; \dfrac{p_{s,t}^2}{\Sigma_{i=0}^{s&amp;#43;t} (p_{i,s&amp;#43;t-i}^2)} \\ &amp;amp; = &amp;amp; \dfrac{1}{\Sigma_{i=0}^{s&amp;#43;t}((p_{i,s&amp;#43;t-i}/p_{s,t})^2)} \\ &amp;amp; = &amp;amp;\dfrac{1}{\Sigma_{i=0}^{s-1}((p_{i,s&amp;#43;t-i}/p_{s,t})^2) &amp;#43; 1 &amp;#43; \Sigma_{i=s&amp;#43;1}^{s&amp;#43;t}((p_{i,s&amp;#43;t-i}/p_{s,t})^2) } \\ &amp;amp; = &amp;amp; \dfrac{1}{w_{camera,s-1}&amp;#43;1&amp;#43;w_{light,t-1}} \end{array} $$
&lt;p&gt;The evaluation of MIS weight is to evaluate these w terms given two specific sub-paths. Luckily, these two terms can be recorded progressively as we trace rays in the scene.&lt;/p&gt;

$$ \tag{15} w_{camera,i}=\dfrac{\overleftarrow{p_i}(\bar{y})}{\overrightarrow{p_i}(\bar{y})}(w_{camera,i-1}&amp;#43;1) $$

$$ \tag{16} w_{light,i}=\dfrac{\overrightarrow{p_{s&amp;#43;t-i}}(\bar{y})}{\overleftarrow{p_{s&amp;#43;t-i}}(\bar{y})}(w_{light,i-1}&amp;#43;1) $$
&lt;p&gt;At a first look, they seem pretty different to each other. Care needs to be paid for each path differently. However they are actually the same. If the rays are generated from light sources instead of camera, we actually have the following equation 17:&lt;/p&gt;

$$ \tag{17} w_{light,i}=\dfrac{\overleftarrow{p_i}(\bar{y})}{\overrightarrow{p_i}(\bar{y})}(w_{light,i-1}&amp;#43;1) $$
&lt;p&gt;A subtle difference is the pdf evaluation. With regard to the camera weight, forward pdf means pdf of sampling a direction from vertex, which is nearer to the camera vertex in term of path, to the next vertex. Vice versa. However for the light weight, forward pdf means pdf of sampling a direction from vertex, which is nearer to the light, instead of the camera vertex, to the next vertex. And what is also inverse is the index of vertex, vertex 0 means the light vertex in the former case, while it is camera sensor vertex in the latter case. Since they obey exact the same rule from their own perspective, I&amp;rsquo;m gonna drop the subscript.&lt;/p&gt;

$$ \tag{18} w_{i}=\dfrac{\overleftarrow{p_i}(\bar{y})}{\overrightarrow{p_i}(\bar{y})}(w_{i-1}&amp;#43;1) $$
&lt;p&gt;There are two issues blocking us from evaluating the term directly, the  $ \overleftarrow{p_i}(\bar{y})$ 
 is unknown because next vertex is not traced yet, neither the pdf w.r.t solid angle, nor the directional g-term can be calculated before we have a clue of what the next vertex is. Another subtle issue is hidden in the  $ w_{i-1} $ 
 term, to evaluate this term, we need to know the pdf w.r.t solid angle from vertex i to i-1, sadly it is also determined by vertex i+1 because that&amp;rsquo;s where the reverse input direction is for the brdf. In order to avoid those terms, it is defined the following way:&lt;/p&gt;

$$ \tag{19} w_{i}=\overleftarrow{p_i}(\bar{y})(\underbrace{\dfrac{w_{i-1}}{\overrightarrow{p_i}(\bar{y})}}_{\overleftarrow{p_{\sigma,i-1}} \boxed{vc_i}}&amp;#43;\underbrace{\dfrac{1}{\overrightarrow{p_i}(\bar{y})}}_{\boxed{vcm_i}}) $$
&lt;p&gt;From this equation, we can see that both VC and VCM can be calculated for the current vertex because those which can&amp;rsquo;t be calculated are avoided in the equation. This simply give us the following equation:&lt;/p&gt;

$$ \tag{20} vcm_i = \dfrac{1}{\overrightarrow{p_i}} $$

$$ \tag{21} vc_i=\dfrac{\overleftarrow{g_{i-1}}}{\overrightarrow{p_i}}(vcm_{i-1}&amp;#43;\overleftarrow{p_{\sigma,i-2}}vc_{i-1}) $$
&lt;p&gt;These two terms can be recorded once we find a vertex along the path for each vertex. For the very first vertex on the light source and camera sensor, we define them the following way:&lt;/p&gt;

$$ \tag{22} vcm_{camera,1} = \dfrac{n_{samples}}{\overrightarrow{p_1}} $$

$$ \tag{23} vc_{camera,1} = 0 $$

$$ \tag{24} vcm_{light,1} = \dfrac{1}{\overrightarrow{p_1}} $$

$$ \tag{25} vc_{light,1} = \dfrac{\overleftarrow{g_0}}{\overrightarrow{p_0}\overrightarrow{p_1}} $$
&lt;p&gt;We have the initial values for those terms, we only need to evaluate them progressively as we trace along the path. The only thing left is how we use them to evaluate the MIS weight. There are four cases to be consider:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1: s &amp;gt; 1 and t &amp;gt; 1&lt;/strong&gt;. This is the most common case in bidirectional path tracing, it actually connects two traced vertices from each side.&lt;/p&gt;

$$ \tag{26} w_{camera,s-1}=\overleftarrow{p_{s-1}}(vcm_{s-1}&amp;#43;\overleftarrow{p_{\sigma,s-2}}vc_{s-1}) $$

$$ \tag{27} w_{light,t-1}=\overleftarrow{p_{t-1}}(vcm_{t-1}&amp;#43;\overleftarrow{p_{\sigma,t-2}}vc_{t-1}) $$
&lt;p&gt;&lt;strong&gt;Case 2: t = 0&lt;/strong&gt;. This is the case where eye path hits area light source.&lt;/p&gt;

$$ \tag{28} w_{camera,s-1}= \overleftarrow{p_{s-1}}( vcm_{s-1}&amp;#43;\overleftarrow{p_{\sigma,s-2}}vc_{s-1}) $$

$$ \tag{29} w_{light,t-1}=0 $$
&lt;p&gt;&lt;strong&gt;Case 3: t = 1&lt;/strong&gt;. This is the case only one vertex is in the light sub-path.&lt;/p&gt;

$$ \tag{30} w_{camera,s-1}=\overleftarrow{p_{s-1}}(vcm_{s-1}&amp;#43;\overleftarrow{p_{\sigma,s-2}}vc_{s-1}) $$

$$ \tag{31} w_{light,t-1}=\dfrac{\overleftarrow{p_0}}{\overrightarrow{p_0}} $$
&lt;p&gt;&lt;strong&gt;Case 4: s = 1&lt;/strong&gt;. This is the light tracing case.&lt;/p&gt;

$$ \tag{32} w_{camera,s-1}=0 $$

$$ \tag{33} w_{light,t-1}=\dfrac{\overleftarrow{p_{t-1}}}{n_{sample}}(vcm_{t-1}&amp;#43;\overleftarrow{p_{\sigma,t-2}}vc_{t-1}) $$
&lt;p&gt;So far we&amp;rsquo;ve already knew how to calculate these complex MIS weight in an elegant way, which saves much time with little memory overhead of two float numbers at each vertex. Since we only need to store a whole sub-path from one side, the two floating number memory overhead is barely noticeable.&lt;/p&gt;
&lt;h1 id=&#34;special-handling-for-light&#34;&gt;Special Handling for Light&lt;/h1&gt;
&lt;p&gt;I have five typical type of light source in my renderer, point light, spot light, directional light, area light and sky light. Each has different properties and some of them need special treatment in a bidirectional path tracer.&lt;/p&gt;
&lt;h2 id=&#34;delta-light&#34;&gt;Delta light&lt;/h2&gt;
&lt;p&gt;In term of light size, point light, spot light and distant light has no physical surface at all. Although these lights are not real at all in the real world, they are pretty useful in computer graphics. Note skylight has a fake surface of an sphere facing inside the world, so it can&amp;rsquo;t be count as delta light. Delta light has one very special property, it can&amp;rsquo;t be hit by random ray traced from anywhere, even if it actually hits the exact point, not to mention how small the chance is.&lt;/p&gt;
&lt;p&gt;In this very case, we are losing the  $ \overleftarrow{p_0} $ 
 term when tracing from delta light sources. That give the following condition for delta light source:&lt;/p&gt;

$$ \tag{34} vc_i = 0 $$
&lt;h2 id=&#34;infinite-light&#34;&gt;Infinite light&lt;/h2&gt;
&lt;p&gt;In term of distance between light source and target vertex, distant light and sky light need special handling because there is actually no real surface for these light sources and vertices sampled on these light sources can be arbitrarily far.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start from sky light first. A sky sphere is used in my render to simulate the radiance distribution of the whole sky. Even for a physical based renderer, a sky light is actually not 100% accurate. For each point to be shaded, it is always the center of the whole skysphere, no matter where the vertex is. Since we are simulating the radiance far away from the whole scene, it introduces little visual artifact that is noticeable.&lt;/p&gt;
&lt;p&gt;To sample a point on a sky light source, we first sample a direction from the target vertex, which is always (0,0,0) in sky sphere&amp;rsquo;s space. After a direction is sampled, we can evaluate the radiance along the inverse direction. &lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/practical_implementation_of_mis_in_bidirectional_path_tracing/infinite.png&#34; width=&#34;500&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Then we need to sample a point from a plane that is perpendicular to the direction and also the tangent plane of the bounding sphere of the scene. What we only need to know is the radius of the sphere since it can be arbitrary.&lt;/p&gt;
&lt;p&gt;Luckily we have a very nice property for sky light, as I mentioned each point to be shaded is at the center of the sky sphere. That said the distance in directional g-term is exactly the same no matter which direction it is. With the following hack, we can ignore the radius value in MIS weight.&lt;/p&gt;

$$ \tag{35} \overrightarrow{p_0} = \overleftarrow{p_{\sigma,origin\rightarrow y_0}} $$

$$ \tag{36} \overleftarrow{p_0}=\overleftarrow{p_{\sigma,0}} $$

$$ \tag{37} \overleftarrow{g_0}=1 $$

$$ \tag{38} \overrightarrow{p_1}=p_a(\hat{y_1}) cos\theta_{1\rightarrow 0} $$
&lt;p&gt;That works because the  $ \overleftarrow{p_0} $ 
  and  $ \overrightarrow{p_0} $ 
 always appear in pair and the g-terms just get canceled with each other.  After a direction is picked, there is really no g-term available in the pdf (  $ \overrightarrow{p_1} $ 
 ) because the direction is fixed. And the radius value totally gets vanished. Since we need to calculate shadow, we still need to setup a number for this radius value, however it doesn&amp;rsquo;t have anything to do with MIS weight and Monte Carlo estimation. Any number larger than the radius of scene bounding sphere can be fine.&lt;/p&gt;
&lt;p&gt;Direction light is very similar except the fact that we don&amp;rsquo;t need to sample a direction in the first place because it is fixed. The others are totally the same.&lt;/p&gt;
&lt;h2 id=&#34;implementing-naive-bdpt-for-comparison&#34;&gt;Implementing Naive BDPT for Comparison&lt;/h2&gt;
&lt;p&gt;It is the power heuristic we mentioned above. In a practical MIS bidirectional path tracer, we always wrap some components of MIS weight, so that we can easily switch between balanced heuristic and power heuristic.&lt;/p&gt;
&lt;p&gt;However a naive BDPT implementation doesn&amp;rsquo;t need to be done separately because we can just set the exponent with the value of 0. It will make your code cleaner and shorter if you also want naive bidirectional path tracer for comparison.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference:&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://graphics.stanford.edu/papers/veach_thesis/&#34;&gt;Robust Monte Carlo Methods For Light Transport Simulation. Eric Veach&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.smallvcm.com&#34;&gt;Small VCM renderer&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.edxgraphics.com/blog/calculating-the-directional-probability-of-primary-rays&#34;&gt;Calculating the directional probability of primary rays&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://www.youtube.com/watch?v=QhJhVkbCgVU&#34;&gt;Bidirectional path tracing&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://rendering.aspone.cz/Default.aspx&#34;&gt;Wonderball&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;http://www.cescg.org/CESCG98/PDornbach/&#34;&gt;Implementation of bidirectional ray tracing algorithm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bidirectional Path Tracing</title>
      <link>https://agraphicsguynotes.com/posts/naive_bidirectional_path_tracing/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/naive_bidirectional_path_tracing/</guid>
      <description>&lt;p&gt;I posted a &lt;a href=&#34;https://agraphicsguynotes.com/posts/basics_about_path_tracing/&#34;&gt;blog&lt;/a&gt; about path tracing some time ago, I didn&amp;rsquo;t regard it as a simple algorithm until I got my hands dirty on bidirectional path tracing. It really took me quite a while to get everything hooked up. Getting BDPT (short for bidirectional path tracing) converging to the same result with path tracing is far from a trivial task, any tiny bug hidden in the renderer will drag you into a nightmare. Those kind of bugs are not the same with the ones that usually appear in real time rendering, like the ones that can easily be exposed with some tools like Nsight, it may cost much more time if only a small component is missing in the target equations, which are totally crazy math.&lt;/p&gt;
&lt;p&gt;Since traditional cornell box setting is pretty friendly to path tracing, I made a little change on it, the light source is a spot light facing towards right up. That said most of the scene is lit by the small direct illuminated area instead of the light, that just makes it a very unfriendly path tracing scene.&lt;/p&gt;
&lt;p&gt;The following images are generated by BDPT(left), light tracing(middle) and path tracing(right) with roughly the same amount of time.&lt;/p&gt;


&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/bdpt_64.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/bdpt_64.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/lt_50.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/lt_50.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/pt_128.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/pt_128.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;Path tracing generates the most noisy image even if it is scaled down by four times, bidirectional path tracing result is better, however light tracing definitely gets the best result with least noise in it.&lt;/p&gt;
&lt;h1 id=&#34;rendering-equation&#34;&gt;Rendering Equation&lt;/h1&gt;
&lt;p&gt;Starting from rendering equation:&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = L_{e}(\omega_{o}) &amp;#43; \int L_{i}(\omega_{i}) *f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;As we can see from this classical equation, L appears at both sides of the equation which makes it a recursive one. By expending this equation for a specific path, we will have something like this:&lt;/p&gt;

$$ f_k(x) = {\int...\int} W_e(x_0) G(x_0 \longleftrightarrow x_1 ) [ \prod_{i=1}^{k-1} \rho_s(x_i)G(x_i\longleftrightarrow x_{i&amp;#43;1} ) ] L_e(x_k) d(x_0) d(x_1) ... d(x_k) $$
&lt;p&gt;That is the equation considering a specific path with length of k+1 vertices between light and eye. Starting from where is really a matter of taste, traditional path tracing generates rays from eye point. Light tracing tries to solve it in a manner that is more similar to how nature works, generating rays from light. Bidirectional path tracing generates rays from both sides and connects those two path somehow.&lt;/p&gt;
&lt;h2 id=&#34;path-tracing&#34;&gt;Path tracing&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ve talked about path tracing in this &lt;a href=&#34;https://agraphicsguynotes.com/posts/basics_about_path_tracing/&#34;&gt;blog&lt;/a&gt;, those path are generated with regard to the bsdf. This time we will put it the other way. Since we already have the equation to be integrated, we only need to know the pdf of the specific path to evaluate the integral with Monte Carlo method.&lt;/p&gt;
&lt;p&gt;Suppose all of the vertices are generated w.r.t the area instead of solid angle, we have a pdf of this:&lt;/p&gt;

$$ p_k(x) = \prod_{i=0}^{k} p_a(x_i) $$
&lt;p&gt;However generating vertices in this manner is extremely inefficient and it is no path tracing we are talking about. A path tracer generates rays from eye point recursively. Forget about the aperture with finite size. The first vertex apart from the camera one is already decided by the camera, so no pdf is needed here. Each of the others will be generated based on the pdf w.r.t (projected) solid angle, we have a relationship between them:&lt;/p&gt;

$$ p_a(x_i) = p_{w^{\bot}}(x_{i-2} \rightarrow x_{i-1} \rightarrow x_i) G(x_{i-1}\longleftrightarrow x_i ) $$
&lt;p&gt;Dropping this into the P_k equation, we will have this:&lt;/p&gt;

$$ p_k(x) = \prod_{i=2}^{k}(p_{w^{\bot}}(x_{i-2} \rightarrow x_{i-1} \rightarrow x_i) G(x_{i-1}\longleftrightarrow x_i )) $$
&lt;p&gt;Also filling this into the extended rendering equation, we will have something like this:&lt;/p&gt;

$$ f(x)=L_e(x_k\rightarrow x_{k-1})\prod_{i=2}^{k}(bxdf(x_{i-2}\rightarrow x_{i-1}\rightarrow x_i)/p_{w^{\bot}}(x_{i-2} \rightarrow x_{i-1} \rightarrow x_i))$$
&lt;p&gt;We have less computation here because the g term has been cancelled with the components in the denominator.&lt;/p&gt;
&lt;p&gt;In a real practical path tracer, we usually use multiple importance sampling to sample both bsdf and light sources at the end of each path, instead of relying on bsdf alone which is highly inefficient for small light sources. The pdf may need to change correspondingly.&lt;/p&gt;
&lt;p&gt;One minor detail that used to confuse me so far is where is the g-term connecting the first vertex and eye point going? Why don&amp;rsquo;t we take the pdf of primary ray into account? Please refer to this &lt;a href=&#34;https://agraphicsguynotes.com/posts/the_missing_primary_ray_pdf_in_path_tracing/&#34;&gt;post&lt;/a&gt; for an explanation.&lt;/p&gt;
&lt;h2 id=&#34;light-tracing&#34;&gt;Light Tracing&lt;/h2&gt;
&lt;p&gt;Light tracing is the reverse version of path tracing. Instead of starting from the eye point, light tracing path origins from light sources. For a path tracer, it may be acceptable without special handling with MIS sampling both bsdf and light sources if light sources are big enough. However light tracing must connect light path vertices to the camera point explicitly, even if light aperture has a size in the scene, its small area will make light tracing converging in an extreme slow manner.&lt;/p&gt;
&lt;p&gt;Similarly we have the pdf of a specific light path in the following way:&lt;/p&gt;

$$ p_k(x)=p_a(x_k)p_{w^\bot}(x_k\rightarrow x_{k-1})G(x_k\rightarrow x_{k-1})\prod_{i=3}^{k}(p_{w^{\bot}}(x_{i}\rightarrow x_{i-1}\rightarrow x_{i-2})G(x_{i-1}\rightarrow x_{i-2})$$
&lt;p&gt;Note that it is only part of the whole pdf, because we haven&amp;rsquo;t consider eye vertex yet. As I mentioned above, in a light tracing algorithm we usually connect eye vertex with light tracing vertices as we progress in the light path, trying to hit the camera aperture with light rays is just not practical, not to mention it won&amp;rsquo;t work for pinhole camera at all.&lt;/p&gt;
&lt;p&gt;To connect eye point with light tracing vertices, we need to be clear with the importance function. For a detailed description of how to calculate it, I suggest my &lt;a href=&#34;https://agraphicsguynotes.com/posts/the_missing_primary_ray_pdf_in_path_tracing/&#34;&gt;other post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although we can make the path contribute to the pixel we are trying to calculate, it is very inefficient way of doing light tracing because most path will fail resulting into an extreme slow convergence rate. Instead, we usually allow all path to contribute to any pixels in the target image we are rendering.&lt;/p&gt;
&lt;p&gt;And light tracing achieves the best result out of the three method in the above comparison for a reason, the spot light change makes the scene pretty light tracing friendly because basically all light paths contribute to the image.&lt;/p&gt;
&lt;h2 id=&#34;bidirectional-path-tracing&#34;&gt;Bidirectional Path Tracing&lt;/h2&gt;
&lt;p&gt;Different from unidirectional path tracing, bidirectional path tracing generates path from both light sources and eye point and then combines each pair of vertices at two sides, including the traditional path tracing path and light tracing path. In some sense that bidirectional path tracing is a super set of path tracing and light tracing.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/bdpt.png&#34; width=&#34;700&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Here is a great demonstration from Eric Veach&amp;rsquo;s thesis. For a specific length of three, we have four cases above. The first two cases are what we usually do in path tracing, the third one is the light tracing we are talking about. The last one is the case when light ray actually hits the aperture and in most cases we just ignore the fourth case because it brings no differentiation comparing with the third case.&lt;/p&gt;
&lt;p&gt;Each case is good at rendering something. If the surface of the table is highly reflective, in other words with spiky brdf, the first case will generate the best result. However if the light source is very small or delta light source, like point light, and the table surface gets a matte look, the second one will be much better. Light tracing case may not be very powerful in this very case, however if there were some glasses, light tracing will deliver good looking caustics in an efficient manner.&lt;/p&gt;
&lt;p&gt;Since we&amp;rsquo;ve already talked about the first three cases, I&amp;rsquo;d like to mention another case which is not shown in the above image.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/naive_bidirectional_path_tracing/mcrt10.jpg&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;I won&amp;rsquo;t write down the equation here because it is pretty similar to the ones mentioned above. However one noticeable detail is that we missed one g-term in the denominator, it is the g-term between the connecting vertices that can&amp;rsquo;t be cancelled, we need to evaluate the g-term explicitly in this case, or the result will be biased.&lt;/p&gt;
&lt;p&gt;By combining all those cases, except the last one, we will get rich information from all paths we generated. A naive implementation is to average all those results of paths with specific length. For example, the averaging factor of five vertices path (including the light and eye vertex) is 1/5.&lt;/p&gt;
&lt;h1 id=&#34;further-work&#34;&gt;Further Work&lt;/h1&gt;
&lt;p&gt;As you can see from the top image, that BDPT generates noisy images than light tracing in the target scene. Sometimes it is worse than path tracing depending on what kind of scene it renders.&lt;/p&gt;
&lt;p&gt;Multiple importance sampling can solve this disturbing issue and that is what I&amp;rsquo;m going to work on, :).&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://graphics.stanford.edu/papers/veach_thesis/&#34;&gt;Robust Monte Carlo Methods For Light Transport Simulation. Eric Veach&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.smallvcm.com&#34;&gt;Small VCM renderer&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.edxgraphics.com/blog/calculating-the-directional-probability-of-primary-rays&#34;&gt;Calculating the directional probability of primary rays&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://www.youtube.com/watch?v=QhJhVkbCgVU&#34;&gt;Bidirectional path tracing&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://rendering.aspone.cz/Default.aspx&#34;&gt;Wonderball&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Physically Based Shading in Games</title>
      <link>https://agraphicsguynotes.com/posts/physically_based_shading_in_games/</link>
      <pubDate>Sat, 05 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/physically_based_shading_in_games/</guid>
      <description>&lt;p&gt;Physically based shading has been around for years, it not only eases the workflow for artist, but also delivers high quality shading with neglectable overhead, I see no reason to avoid it in today&amp;rsquo;s game. Here is an image taken from UE4 document.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/measured_materials.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;When the term first came out, I was totally no idea what this new stuff is. And it took me quite a while to get some basic idea on it because there are so many materials and some of them are a little confusing. I can&amp;rsquo;t say that I fully understand all of the theory simply because I don&amp;rsquo;t, however I would still like to write something that I knew and list something useful that I found in this blog. Hopefully it can be helpful for someone.&lt;/p&gt;
&lt;h1 id=&#34;physically-based-shading&#34;&gt;Physically Based Shading&lt;/h1&gt;
&lt;p&gt;Basically physically based shading performs shading operation in a more physical accurate way than the old common &amp;lsquo;ad-hoc&amp;rsquo; model, which is usually Phong or any modified version. Here is the basic form of Phong model (note that it is shading model listed below, not BRDF):&lt;/p&gt;

$$ I_p=\sum_{m\in lights}(k_dm_d(L_m\cdot N)&amp;#43;k_sm_s(R\cdot V)^\alpha)$$
&lt;p&gt;Ambient term is dropped since it is not involved in this topic. R is the reflected vector of the incident direction around normal. It has many parameters for artist to tune and special care needs to be taken in certain aspect to avoid unnatural look. Although this model is used ever since fixed function graphics hardware, it is apparently out-dated comparing with more advanced model like &amp;lsquo;microfacet&amp;rsquo; model.&lt;/p&gt;
&lt;h2 id=&#34;diffuse-and-specular&#34;&gt;Diffuse and Specular&lt;/h2&gt;
&lt;p&gt;Generally there are two categories of reflection, diffuse and specular. In the following diagram, the yellow ones are specular reflection and the rest are diffuse reflection.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/ds.png&#34; width=&#34;600&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Diffuse is the kind of reflection whose distribution is totally random. It happens when the photons actually penetrate into the surface and get out after several bounces. Depending on what the material inside the surface is, it can manifest itself as diffuse reflection or subsurface scattering which is a much more complex concept. There are several brdf model available for simulating it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lambert&lt;/li&gt;
&lt;li&gt;OrenNayar&lt;/li&gt;
&lt;li&gt;Burley&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;UE4 uses the simplest one, lambert, as their diffuse model because little difference can be noticed between those models. Although UE4 users can enable other diffuse model with simple macro switching in the uber shader, I seriously doubt if it helps on the visual quality of the final image. See the image below, it is basically no difference to me. I can&amp;rsquo;t justify the reason to do the switching.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/dm.png&#34; width=&#34;400&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Unlike diffuse reflection, specular happens right at the surface being shaded. And its reflected direction is highly dependent on the incident angle. Take an extreme case, mirror will reflect viewing angle into exact one direction. Microfacet model is the most commonly used bxdf for specular lighting. It assumes that the surface is composed of many small micro facets, each one among them is a pure specular surface or in other words, perfect mirror. Only micro facets with normal exactly the same with the half angle between incident and reflected direction will contribute. It is the aggregate behavior defines what the surface actually looks like. The more uniform those microfacet normals are, the smoother the surface is, vice versa. Here is the microfacet brdf:&lt;/p&gt;

$$ f(\omega_i,\omega_o,x) = \dfrac{F(\omega_i , h) G(\omega_i,\omega_o,h) D(h)}{4 cos(\theta_i) cos(\theta_o)} $$
&lt;p&gt;F is the fresnel term which will be mentioned later. D is the distribution term defining where these microfacets face, here is more information on it. G is a shadowing factor for masking and it is usually a less dominant term in this bxdf.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/mf.png&#34; width=&#34;600&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Wait a second, phong model also takes diffuse and specular into account, what is the big deal in this new PBS model?&lt;/p&gt;
&lt;p&gt;In term of diffuse reflection, they are the same. Both use lambert bxdf. However it is microfacet bxdf model which delivers much better result than the phong model. And another subtle difference is the way pbs model blends diffuse and specular.&lt;/p&gt;
&lt;h1 id=&#34;energy-conservation&#34;&gt;Energy Conservation&lt;/h1&gt;
&lt;p&gt;Energy conservation is one of the basic rules for every physical correct bxdf to obey. It says that the total amount of reflected energy of light can&amp;rsquo;t not be larger than received unless it is emissive material. It makes perfect sense. From a mathematics perspective, it can be expressed this way:&lt;/p&gt;

$$ \int_{\Omega}f_r(p,\omega_o,\omega_i) cos\theta_i d\omega_i \le 1$$
&lt;p&gt;Unfortunately bxdf of phong totally ignores this rule, forwarding the responsibility to artists in their workflow, which usually costs them a lot of time.&lt;/p&gt;
&lt;h2 id=&#34;specular-bxdf&#34;&gt;Specular bxdf&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s focus on the specular part first. Two things deserve our attention, reflection intensity and the high light shape. In the phong model, we have two parameters controlling each of these, which used to be defined as &amp;ldquo;Specular Color&amp;rdquo; and &amp;ldquo;Specular Power&amp;rdquo;.  Unfortunately, it is quite easy to break the rule of energy conservation because these two terms should be related to each other in order to preserve it. Simply put it, phong brdf is just not a normalized brdf.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/normalization_factor.jpg&#34; width=&#34;600&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;See the above image, the top row, as the specular power becomes higher, we get narrower high light. While the ugly fact is that the specular intensity never changes. That is just not true in our real life. As we can notice from the bottom row, the reflection becomes stronger as it narrows down. In order to simulate it in the phong model, it forces artist to increase specular color, sometimes even above one. And if there are only two values to tune, it may be acceptable, not to mention it is not accurate. What if the specular power is provided by a texture? Artist will have to make sure they have a cooresponding specular color texture and what is worse is that every pair of pixels from these two textures needs to be tuned.&lt;/p&gt;
&lt;p&gt;With the microfacet brdf model, artists have only one parameter defining both of the shape and intensity of the reflection. And that is the roughness, or glossyness in some engines. So only one parameter needs to be set and the shading policy will force it to be energy conservation no matter what value is provided by the artist. In other words, artists will have little chance to make mistake.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/pbr_theory_watermud.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Notice the mud and water have different reflection intensity. As a matter of fact their reflectivities are exactly the same, the only thing different is the gloss parameter. Here is another example from my offline renderer (I know I&amp;rsquo;m talking about physically based shading in real time rendering, however the theory behind these are exactly the same, so it can be used as a reference too):&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/ec1.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;These monkeys have different roughness values with the same base color. The reflection on the left-most one is clearly more sharp comparing with the right ones.&lt;/p&gt;
&lt;p&gt;There are also ways to make phong brdf energy conservative, check &lt;a href=&#34;http://www.thetenthplanet.de/archives/255&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;blend-between-diffuse-and-specular&#34;&gt;Blend between Diffuse and Specular&lt;/h2&gt;
&lt;p&gt;Besides the unnormalized feature of phong model, the other ugly thing that breaks energy conservation is the way it blends diffuse and specular. As a matter of fact, it doesn&amp;rsquo;t even blend them, it just add them together which makes little sense in a physical manner. From a micro perspective, photons either reflect at the surface or penetrate into it, they can do both at the same time. That said the total energy reflected by diffuse and specular should obey the rule of energy conservation. Obviously phong fails to keep it again.&lt;/p&gt;
&lt;p&gt;The right way of blending these two is to make sure the following equation goes right:&lt;/p&gt;

$$ m_d &amp;#43; m_s \le 1 $$
&lt;p&gt;UE4 uses a specific parameter called Mettalic to lerp between diffuse and specular. One minor detail in UE4 material system to be noticed is that even if metallic is zero, 8% percent specular reflection will still be present. Another paramter called specular is introduced to remove this 8% for some special materials. Here is an image showing a sphere with different metallic value. There is no diffuse reflection for surfaces with 1 as metallic parameter, that makes sense because metal doesn&amp;rsquo;t show any diffuse reflection in our real life.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/diffusetospecular.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;h1 id=&#34;fresnel-effect&#34;&gt;Fresnel Effect&lt;/h1&gt;
&lt;p&gt;The other big benefit that can&amp;rsquo;t be ignored is the fresnel effect. That said light reflection will be much higher at grazing angle. Rim color is what we used to simulated in the old ad-hoc model. Check here for the importance of fresnel effect, the author takes many real life examples convincing readers how importance fresnel effect is in our daily life. Here is one sphere demonstrating how importance fresnel effect is, we get much more brighter reflection at grazing angle. And that is just the work of fresnel.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/physically_based_shading_in_games/fresnel01-600x600-293724547.png&#34; width=&#34;400&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;For a real time solution for fresnel equation, Schlick&amp;rsquo;s approximation is enough most of the time.&lt;/p&gt;

$$ R = R(0) &amp;#43; ( 1 - R(0) ) ( 1 - cos\theta)^5$$
&lt;p&gt;It is relatively easy to see from this image that when at grazing angle, fresnel value will approach 1 no matter what value R(0) is provided in the material system. That is exactly why we are seeing bright reflection around the contour of this sphere. One can also check &lt;a href=&#34;https://seblagarde.wordpress.com/2013/04/29/memo-on-fresnel-equations/&#34;&gt;here&lt;/a&gt; for further information.&lt;/p&gt;
&lt;p&gt;Valuable resources&lt;/p&gt;
&lt;p&gt;Here are the courses on pbs at each year siggraph:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://renderwonk.com/publications/s2010-shading-course/&#34;&gt;Siggraph 2010 pbs course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.selfshadow.com/publications/s2012-shading-course/&#34;&gt;Siggraph 2012 pbs course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.selfshadow.com/publications/s2013-shading-course/&#34;&gt;Siggraph 2013 pbs course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.selfshadow.com/publications/s2014-shading-course/&#34;&gt;Siggraph 2014 pbs course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.selfshadow.com/publications/s2015-shading-course/&#34;&gt;Siggraph 2015 pbs course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;I guess the features listed above are not all about pbs in games. However they are definitely the importance ones.&lt;/p&gt;
&lt;p&gt;And as we can see from the above images, pbs introduces quite a lot of new nice features in games. It is too good to be ignored. Of course it needs more instructions than before, however texture operations are roughly the same. Since even mobile devices can achieve very high GFLOPS, those extra instructions in physically based shading won&amp;rsquo;t affect much performance on modern graphics hardware, which just gives us no excuses to avoid it.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://www.rorydriscoll.com/2009/01/25/energy-conservation-in-games/&#34;&gt;Energy Conservation in Games&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.farbrausch.de/~fg/stuff/phong.pdfhttp://www.farbrausch.de/~fg/stuff/phong.pdf&#34;&gt;Phong Normalization Factor derivation. Blinn-Phong normalization factor&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.reedbeta.com/blog/2013/07/31/hows-the-ndf-really-defined/&#34;&gt;How is the NDF really defined&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;http://www.thetenthplanet.de/archives/255&#34;&gt;The Blinn-Phong Normalization Zoo&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;https://www.marmoset.co/toolbag/learn/pbr-theory&#34;&gt;Basic Theory of Physically Based Rendering&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;https://docs.unrealengine.com/latest/INT/Engine/Rendering/Materials/PhysicallyBased/index.html&#34;&gt;Physically Based Materials&lt;/a&gt;&lt;br&gt;
[7] &lt;a href=&#34;https://www.unrealengine.com/blog/physically-based-shading-in-ue4&#34;&gt;Physically Based Shading In UE4&lt;/a&gt;&lt;br&gt;
[8] &lt;a href=&#34;https://seblagarde.wordpress.com/2013/04/29/memo-on-fresnel-equations/&#34;&gt;Memo on Fresnel equations&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Glass Material Simulated by Microfacet BXDF</title>
      <link>https://agraphicsguynotes.com/posts/glass_material_simulated_by_microfacet_bxdf/</link>
      <pubDate>Wed, 11 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/glass_material_simulated_by_microfacet_bxdf/</guid>
      <description>&lt;p&gt;Microfacet model can not only be used for rough metal, it can also be used to simulate rough glass material. This blog is about rendering glass material with microfacet model. Basically all of the theory comes from this &lt;a href=&#34;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html&#34;&gt;paper&lt;/a&gt;. Different from the pure refraction model mentioned in my previous blog, the bxdf mentioned here can also refract a single incident ray into multiple directions instead of just one.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/glass_material_simulated_by_microfacet_bxdf/refract_microfacet.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;The above image is generated with &lt;a href=&#34;https://sort-renderer.com&#34;&gt;my renderer&lt;/a&gt;. Each pixel takes 1024 samples and it took me an hour to finish it. As we can see from this image, this bxdf model handles both reflection and refraction. The roughness parameter for both of the objects is 0.04, which makes them look quite similar to combination of pure reflection and refraction bxdf.&lt;/p&gt;
&lt;h1 id=&#34;snells-law-for-refraction-direction&#34;&gt;Snell&amp;rsquo;s Law for Refraction Direction&lt;/h1&gt;
&lt;p&gt;Comparing with reflecting an incident direction along a specific normal, refracting it is much more complicated. I won&amp;rsquo;t mention anything about derivation of this Snell&amp;rsquo;s law, it is more of physics than computer graphics. However an important concept that we should know is index of refraction, IOR in short, it defines how light, or any other radiation, propagates through a specific medium. In physics, it affects the speed of light and the wavelength of light. Each media may have different IOR, some of the common ones are listed below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Volume Type&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;IOR&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Vacuum&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Air&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.000293&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Water&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ice&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.52&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Flint&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Diamond&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2.42&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The direct reason that we care about this IOR is because it redirects incident ray through the surface between mediums with different IOR. And the way incident ray is refracted obeys this law:&lt;/p&gt;

$$ \dfrac{sin\theta_1}{sin\theta_2} = \dfrac{n_2}{n_1} $$
&lt;p&gt; $ \theta $ 
 is the angle between ray and the normal, n is the corresponding IOR. Some basic math shows the equation for computing refracted direction goes this way:&lt;/p&gt;

$$ V_r = -\dfrac{n_i}{n_o}V_i &amp;#43; N( \dfrac{n_i}{n_o} (V_iN) - \sqrt{1-\dfrac{{n_i}^2}{{n_o}^2}(1-(V_iN)^2) } ) $$
&lt;p&gt;N must be in the same hemisphere with incident ray in this equation.  $ 1-\dfrac{{n_i}^2}{{n_o}^2}(1-(V*N)^2) $ 
 can also be negative for certain cases, it happens when light propagates from a medium with higher IOR to a another medium with lower IOR and the incident angle is larger than a threshold, which we call critical angle. And critical angle can be calculated this way:&lt;/p&gt;

$$ \theta_{critical} = arcsin( \dfrac{n_o}{n_i} ) $$
&lt;p&gt;Handling total internal reflection is crucially important to achieve good result because otherwise some rays will be terminated inside the medium.&lt;/p&gt;
&lt;h1 id=&#34;combine-refraction-with-reflection&#34;&gt;Combine Refraction with Reflection&lt;/h1&gt;
&lt;p&gt;Only handling refraction will never deliver convinced result because there is no such a thing that only refracts light and totally ignores reflected light, at least most stuff in our daily life don&amp;rsquo;t. The image below is a comparison between those two, as we can notice from it, there is no reflection of area light at the surface of the left objects and some of the light is totally consumed by them. Those are the results caused by totally ignoring reflection during bxdf evaluation.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/glass_material_simulated_by_microfacet_bxdf/compare_refraction.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;In order to achieve better result like the right ones, it is necessary to evaluate both reflection and refraction. Regarding the rendering equation:&lt;/p&gt;

$$ L_{o}(\omega_{o}) = L_e(\omega_i) &amp;#43; \int_{\Omega} L_{i}(\omega_{i}) *f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}$$
&lt;p&gt;For certain surfaces, actually most of the time,  $ \Omega $ 
 means the hemisphere that the normal points to. However for semi-transparent surfaces,  $ \Omega $ 
 is the whole sphere instead of half one. One importance policy during this bxdf evaluation is how to generate rays around the whole sphere.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start from the simple one by assuming the material is pure-refracted. That said incident ray will only be refracted along one single direction and of course there will also be one single reflected ray, everything stays sharp. And we&amp;rsquo;ve already knew how to generate reflected ray and refracted ray with given IORs. Since fresnel term decides how much flux is reflected, it makes sense to use this value to decide when to sample reflected or refracted ray.&lt;/p&gt;
&lt;p&gt;Pure reflection and refraction bxdf are listed here:&lt;/p&gt;

$$ f(\omega_{i}, \omega_{o} ) = \dfrac{F_r(T(\omega_o),\omega_o) \delta( \omega_i - R(\omega_o) )}{|cos(\theta_{i})|} $$

$$ f(\omega_i , \omega_o) = ( 1 - F_r(\omega_i,\omega_o) ) \dfrac{{\eta_o}^2}{{\eta_i}^2}\dfrac{\delta( \omega_i - T(\omega_o) )}{|cos(\theta_{i})|} $$
&lt;p&gt;Btdf for pure refraction is the same with &lt;a href=&#34;https://agraphicsguynotes.com/posts/derivation_of_pure_specular_reflection_brdf/&#34;&gt;previous blog post&lt;/a&gt;. However a subtle difference for pure reflection brdf definitely deserves our attention, it is the transmittance direction, instead of reflected direction, that we use to evaluate fresnel term. In special case, which total inner reflection happens, fresnel term should be exactly one. Although it makes sense, I&amp;rsquo;ve no much idea how to prove these with mathematics, however that&amp;rsquo;s how other renderers do it. And I&amp;rsquo;m gonna stick to their solution.&lt;/p&gt;
&lt;h1 id=&#34;microfacet-for-rough-glass-material&#34;&gt;Microfacet for Rough Glass Material&lt;/h1&gt;
&lt;p&gt;Microfacet model is usually used for simulating metal surfaces in realtime rendering. It assumes the macro surface composes many small micro flat surfaces, each one uses perfect mirror reflection bxdf. With a simple change in this assumption that micro surfaces are not perfect mirror reflection surfaces but perfect transmittance surfaces, the same theory can be used to simulate rough glass. Here is an image with three monkeys, each with a different value of roughness.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/glass_material_simulated_by_microfacet_bxdf/different_roughness.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;The bxdf is the sum of microfacet reflection brdf and refraction btdf:&lt;/p&gt;

$$ f( \omega_o, \omega_i ) = f_r( \omega_o , \omega_i ) &amp;#43; f_t( \omega_o , \omega_i ) $$
&lt;p&gt;Microfacet brdf is mentioned before:&lt;/p&gt;

$$ f(\omega_i,\omega_o,x) = \dfrac{F(\omega_i , h) G(\omega_i,\omega_o,h) D(h)}{4 cos(\theta_i) cos(\theta_o)}$$
&lt;p&gt;Microfacet btdf is a little more complex:&lt;/p&gt;

$$ f(\omega_i,\omega_o,x) = \dfrac{|\omega_i \cdot h| |\omega_o \cdot h|}{|\omega_i \cdot n| | \omega_o \cdot n| }\dfrac{ {\eta_o}^2 ( 1 - F(\omega_i , h) ) G(\omega_i,\omega_o,h) D(h)}{{(\eta_i (\omega_i \cdot h) &amp;#43; \eta_o (\omega_o \cdot h))^2 }}$$
&lt;p&gt;Another big difference for transmittance ray is that $ \dfrac{\omega_h}{\omega_o} $ is different.&lt;/p&gt;

$$ |\dfrac{\omega_h}{\omega_o}| = \dfrac{{\eta_o}^2 | \omega_o \cdot h |}{(\eta_i (\omega_i \cdot h) &amp;#43; \eta_o (\omega_o \cdot h))^2}$$
&lt;p&gt;Sampling refracted ray direction is similar to the method method &lt;a href=&#34;https://agraphicsguynotes.com/posts/sample_microfacet_brdf/&#34;&gt;here&lt;/a&gt;. First a normal needs to be sampled respecting to a given pdf and then choose between reflected and refracted direction by fresnel term, finally evaluate the corresponding bxdf with the generated ray.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html&#34;&gt;Microfacet Models for Refraction through Rough Surfaces&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Snell%27s_law&#34;&gt;Snell&amp;rsquo;s Law&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Microfacet BRDF</title>
      <link>https://agraphicsguynotes.com/posts/sample_microfacet_brdf/</link>
      <pubDate>Sun, 01 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/sample_microfacet_brdf/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m working on microfacet brdf model for my renderer these days, noticing that it is more than necessary to provide a separate sampling method for microfacet brdf instead of using the default one, which is usually used for diffuse like surfaces and highly inefficient for brdf with spiky shape, such as mirror like surfaces. The following image is one generated by the default sampling method:&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/sample_microfacet_brdf/oldsampling.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/sample_microfacet_brdf/oldsampling.png&#34; width=&#34;700&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;The left monkey has pure reflection brdf which is mentioned in my previous blog &lt;a href=&#34;http://localhost:1313/posts/derivation_of_pure_specular_reflection_brdf/&#34;&gt;post&lt;/a&gt;, the right one uses the microfacet model with zero as roughness value. I was expecting similar result for both of the monkeys while things turned out to be wrong here, we can barely see the reflection from the monkey. Actually there is nothing wrong, the fact is that the convergence rate of default sampling the microfacet model with 0 roughness is extremely low. As long as there are enough samples, it will reach the appearance similar to the left one. However the number of samples being enough can be arbitrarily high depending on how spiky your brdf is.&lt;/p&gt;
&lt;p&gt;The right way of sampling microfacet brdf is mentioned in this &lt;a href=&#34;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html&#34;&gt;paper&lt;/a&gt;. What I want to write down in this blog is how these conclusions are derived from the original microfacet model. Using these better sampling methods, we will get similar result for those two monkeys eventually.&lt;/p&gt;
&lt;h1 id=&#34;why-default-sampling-method-is-inefficient&#34;&gt;Why Default Sampling Method is Inefficient&lt;/h1&gt;
&lt;p&gt;So why is this default sampling method inefficient for mirror like brdf model. First of all, the default sampling method goes with the following pdf:&lt;/p&gt;

$$ p_h(\omega) = \dfrac{cos(\theta)}{\pi} $$
&lt;p&gt;It achieves good result for diffuse surfaces like Lambert, OrenNayar. In fact, it is not a sampling method for diffuse brdfs, a sampling method for lambert brdf will respect a pdf with constant value, it also involves the cosine factor located in the rendering equation or LTE.&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = \int L_{i}(\omega_{i}) *f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;Since it is extreme difficult, if not impossible, to have any knowledge on the incident radiance, the only thing we have is the brdf and cosine factor. The sampling method mentioned above could be efficient if the product of brdf and cosine factor is the dominant one in this equation. It works pretty well in practice most of the time.&lt;/p&gt;
&lt;p&gt;However the efficiency of default sampling method tanks quickly as the brdf becomes the dominant factor with roughness going to zero. For microfacet brdf here, its extreme form is like a dirac-delta function, it is almost impossible to get a sample with brdf having a non-zero value. And for the lucky ones that do have non-zero brdf values, it could reach super high value due to its low probability, introducing high variance to the sampling result. In other words, the convergence rate is quite low, which is exactly why we are seeing the above image. It can be even treated as a bug. Obviously, we need a better way to sample these microfacet brdfs.&lt;/p&gt;
&lt;h1 id=&#34;microfacet-model&#34;&gt;Microfacet Model&lt;/h1&gt;
&lt;p&gt;Microfacet Model is quite hot in real time rendering these years, it is the basics of physically based shading algorithm. Basically, its basic form is like the following:&lt;/p&gt;

$$ f(\omega_i,\omega_o,x) = \dfrac{F(\omega_i , h) G(\omega_i,\omega_o,h) D(h)}{4 cos(\theta_i) cos(\theta_o)}$$
&lt;p&gt;The first component is Fresnel, the second one is a shadowing factor called G term, the last one is normal distribution function (NDF). Comparing with the old ad-hoc bxdf model, microfacet model obeys the rule of energy conservation which allows the artist to change the roughness of the material through one parameter instead of two, specular color and specular power.&lt;/p&gt;
&lt;p&gt;Among all those factors in this equation, the NDF term is usually the dominant one. The specific shape of NDF is largely affected by the roughness of the bxdf. In order to sample the microfacet brdf model, it is usually to sample NDF first getting a random microfacet normal that respects the NDF and then reflect the exitence radiance along the normal to generate the incident direction.&lt;/p&gt;
&lt;p&gt;There are several kinds of NDF around. The following three will be covered in this blog and all that will be mentioned are isotropic ones.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GGX&lt;/li&gt;
&lt;li&gt;Beckmann&lt;/li&gt;
&lt;li&gt;Blinn&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before we proceed, there is one rule that all NDF should follow:&lt;/p&gt;

$$ \int_{\Omega} D(m) cos(\theta_m) d\omega = 1$$
&lt;p&gt;We will use this equation in our following derivation. Explaining the equation is outside the scope of this blog, however one can refer &lt;a href=&#34;http://www.reedbeta.com/blog/hows-the-ndf-really-defined/&#34;&gt;here&lt;/a&gt; for further detail.&lt;/p&gt;
&lt;h2 id=&#34;sampling-ggx&#34;&gt;Sampling GGX&lt;/h2&gt;
&lt;p&gt;Here is the basic form of GGX:&lt;/p&gt;

$$ D(h) = \dfrac{\alpha^2}{\pi ((\alpha^2-1) cos^2\theta &amp;#43; 1 ) ^2} $$
&lt;p&gt;So the pdf respecting solid angle is like this:&lt;/p&gt;

$$ p_h(\omega)=\dfrac{\alpha^2cos\theta}{\pi((\alpha^2-1) cos^2\theta&amp;#43;1)^2}$$
&lt;p&gt;Instead of sampling solid angle directly, we usually use spherical coordinate to sample it. So it is not the pdf respecting the solid angle that we are interested, it is the pdf respecting the spherical coordinates.&lt;/p&gt;

$$ p_h(\theta,\phi)=\dfrac{\alpha^2cos\theta sin\theta}{\pi((\alpha^2-1) cos^2\theta&amp;#43;1)^2}$$
&lt;p&gt;The following task is quite straight forward, which is basically sampling according to a specific pdf. Inversion method goes pretty well for all NDFs in this blog. Notice that $ \phi $ is not even in this equation, that said the NDF is totally isotropic and we can sample $ \phi $ uniformly. For detail proof, please refer &lt;a href=&#34;https://www.tobias-franke.eu/log/2014/03/30/notes_on_importance_sampling.html&#34;&gt;here&lt;/a&gt;. The only thing left here is how to sample $ \theta $. In order to do that, we need to get the pdf for $ \theta $ first.&lt;/p&gt;

$$ p_h(\theta)=\int_{0}^{2\pi}p_h(\theta,\phi)d\phi = \dfrac{2\alpha^2cos\theta sin\theta}{((\alpha^2-1) cos^2\theta&amp;#43;1)^2} $$
&lt;p&gt;Let&amp;rsquo;s calculate the CDF next:&lt;/p&gt;

$$ \begin{array} {lcl} P_h(\theta) &amp;amp; = &amp;amp; \int_{0}^{\theta} \dfrac{2\alpha^2 cos(t) sin(t)}{(cos^2t(\alpha^2-1)&amp;#43;1)^2}dt \\\\ &amp;amp; = &amp;amp; \int_{\theta}^{0} \dfrac{\alpha^2}{(cos^2t(\alpha^2-1)&amp;#43;1)^2}d(cos^2t) \\\\ &amp;amp; = &amp;amp; \dfrac{\alpha^2}{\alpha^2-1} \int_{0}^{\theta} d{\dfrac{1}{cos^2t(\alpha^2-1)&amp;#43;1}} \\\\ &amp;amp; = &amp;amp; \dfrac{\alpha^2}{\alpha^2-1} {(\dfrac{1}{cos^2\theta(\alpha^2-1)&amp;#43;1}-\dfrac{1}{\alpha^2})} \\\\ &amp;amp; = &amp;amp; \dfrac{\alpha^2}{cos^2\theta(\alpha^2-1)^2&amp;#43;(\alpha^2-1)}-\dfrac{1}{\alpha^2-1} \end{array} $$
&lt;p&gt;With a random number ranging from 0 to 1 $ \epsilon $ that is equal to this CDF, we have the following equation:&lt;/p&gt;

$$ \epsilon =\dfrac{\alpha^2}{cos^2\theta(\alpha^2-1)^2 &amp;#43; (\alpha^2-1)} - \dfrac{1}{\alpha^2-1}$$
&lt;p&gt;To solve this equation requires no more knowledge than mathematic in junior high school, I guess there is no need to show the whole process. The final solution of the above equation is:&lt;/p&gt;
&lt;p&gt; $ \theta =arccos\sqrt{\dfrac{1-\epsilon}{\epsilon(\alpha^2-1)+1}} $ 

or
 $ \theta =arctan(\alpha\sqrt{\dfrac{\epsilon}{1-\epsilon}}) $ 
&lt;/p&gt;
&lt;p&gt;The above two equations are exactly the same thing. Choosing which one to use is just a matter of taste.&lt;/p&gt;
&lt;h2 id=&#34;sampling-beckmann&#34;&gt;Sampling Beckmann&lt;/h2&gt;
&lt;p&gt;The derivation of sampling method for Beckmann is quite similar to the above procedure. Here is the Beckmann distribution:&lt;/p&gt;

$$ D(h) =\frac{1}{\pi\alpha^2cos^4\theta}e^{-\frac{tan^2\theta}{\alpha^2}}$$
&lt;p&gt;The pdf respecting spherical coordinate is like this:&lt;/p&gt;

$$ p_h(\theta,\phi)=\frac{sin\theta}{\pi\alpha^2cos^3\theta}e^{-\frac{tan^2\theta}{\alpha^2}}$$
&lt;p&gt;Again, $ \phi $ can be sampled uniformly. The pdf for $ \theta $ should be this:&lt;/p&gt;

$$ p_h(\theta)=\frac{2sin\theta}{\alpha^2cos^3\theta}e^{-\frac{tan^2\theta}{\alpha^2}}$$
&lt;p&gt;The CDF for $ \theta $ can be calculated this way:&lt;/p&gt;

$$ \begin{array} {lcl} P_h(\theta) &amp;amp; = &amp;amp; \int_{0}^{\theta} \frac{2sin(t)}{\alpha^2cos^3t}e^{-\frac{tan^2t}{\alpha^2}}dt \\\\ &amp;amp; = &amp;amp; \int_{0}^{\theta} \frac{-2}{\alpha^2cos^3t}e^{-\frac{tan^2t}{\alpha^2}}d(cos(t)) \\\\ &amp;amp; = &amp;amp;\int_{0}^{\theta} \frac{1}{\alpha^2}e^{-\frac{tan^2t}{\alpha^2}}d(\frac{1}{cos^2(t)}) \\\\ &amp;amp; = &amp;amp; \int_{0}^{\theta} \frac{1}{\alpha^2}e^{\frac{1}{\alpha^2}(1-\frac{1}{\cos^2t})}d(\frac{1}{cos^2(t)}) \\\\ &amp;amp; = &amp;amp; \int_{\theta}^{0} d(e^{\frac{1}{\alpha^2}(1-\frac{1}{\cos^2t})}) \\\\ &amp;amp; = &amp;amp; 1- e^{\frac{1}{\alpha^2}(1-\frac{1}{\cos^2\theta})} \end{array} $$
&lt;p&gt;Solving the equation of $ P_h(\theta) = \epsilon $ gives the following solution:&lt;/p&gt;
&lt;p&gt; $ \theta =arccos\sqrt{\dfrac{1}{1-\alpha^2 ln(1-\epsilon)}} $ 
 or
 $ \theta =arctan\sqrt{-\alpha^2\ln(1-\epsilon)} $ 
&lt;/p&gt;
&lt;h2 id=&#34;sampling-blinn&#34;&gt;Sampling Blinn&lt;/h2&gt;
&lt;p&gt;Here is the Blinn NDF:&lt;/p&gt;

$$ D(h)=\dfrac{\alpha&amp;#43;2}{2\pi}(cos\theta)^\alpha$$
&lt;p&gt;The pdf respecting to spherical coordinate is:&lt;/p&gt;

$$ p_h(\theta,\phi)=\dfrac{\alpha&amp;#43;2}{2\pi}(cos\theta)^{\alpha&amp;#43;1}sin\theta $$
&lt;p&gt;Isolating $ \phi $ gives the following:&lt;/p&gt;

$$ p_h(\theta) = (\alpha&amp;#43;2)(cos\theta)^{\alpha&amp;#43;1}sin\theta $$
&lt;p&gt;This one is a lot simpler than the previous two, here is the CDF:&lt;/p&gt;

$$ \begin{array} {lcl} P_h(\theta) &amp;amp; = &amp;amp; \int_{0}^{\theta} (\alpha&amp;#43;2)cos(t)^{\alpha&amp;#43;1}sin(t)d(t) \\\\ &amp;amp; = &amp;amp; \int_{\theta}^{0} (\alpha&amp;#43;2)cos(t)^{\alpha&amp;#43;1}d(cos(t)) \\\\ &amp;amp; = &amp;amp;\int_{\theta}^{0} d(cos(t)^{\alpha&amp;#43;2}) \\\\ &amp;amp; = &amp;amp;{1-cos(\theta)^{\alpha&amp;#43;2}} \end{array} $$
&lt;p&gt;Here is the relation between random number and $ \theta $&lt;/p&gt;

$$ \theta=arccos((1-\epsilon)^{\frac{1}{\alpha&amp;#43;2}})$$
&lt;p&gt;Since $ \epsilon $ is a uniformly distributed random number between 0 and 1, $ 1-\epsilon $ is the same. So we can simplify the above equation with the following one:&lt;/p&gt;

$$ \theta=arccos(\epsilon^{\frac{1}{\alpha&amp;#43;2}})$$
&lt;p&gt;A little more about this sampling method. I&amp;rsquo;m not quite confident with this solution, although I can see nothing wrong in this derivation. PBRT gives a similar solution for Blinn which is:&lt;/p&gt;

$$ \theta=arccos(\epsilon^{\frac{1}{\alpha&amp;#43;1}})$$
&lt;p&gt;And another open-source ray tracer, &lt;a href=&#34;https://www.mitsuba-renderer.org/&#34;&gt;mitsuba&lt;/a&gt;, also adopts this way of sampling. I don&amp;rsquo;t quite understand the derivation in the book, so I&amp;rsquo;ll just stick to this one, which is also the one mentioned &lt;a href=&#34;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html&#34;&gt;here&lt;/a&gt;. I tried the pbrt&amp;rsquo;s way of sampling, only minor difference can be noticed from the image.&lt;/p&gt;
&lt;h1 id=&#34;one-extra-step&#34;&gt;One Extra Step&lt;/h1&gt;
&lt;p&gt;There is still one step further from being done. It is the incident direction sampling that we are interested, not the normal. The reason we are sampling the normal instead of incident direction directly is because NDF is usually the dominant factor in microfacet model. Generating the incident direction after sampling normal is a more efficient way than sampling the incident direction itself.&lt;/p&gt;
&lt;p&gt;However the way we calculate PDF given an incident direction is different from the above ones respecting half-vector, whether it&amp;rsquo;s solid angle or spherical coordinate. What we have so far is the pdf for half-vector, a transformation is necessary.&lt;/p&gt;

$$ p(\theta) = p_h(\theta) \dfrac{d\omega_h}{d\omega_i} = \dfrac{p_h(\theta)}{4 (\omega_o \cdot \omega_h)} $$
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Here are the images generated after and before the new sampling method.&lt;/p&gt;


















&lt;style&gt;
    .ba-slider10b97b22b77d4af64917b9b2ab6df72b {
        position: relative;
        overflow: hidden;
        width: 90%;
         
    }
    
    .ba-slider10b97b22b77d4af64917b9b2ab6df72b img {
        width: 100%;
        display:block;
        max-width:none;
    }
    
    .ba-slider10b97b22b77d4af64917b9b2ab6df72b .resize {
        position: absolute;
        top:0;
        left: 0;
        height: 100%;
        width: 50%;
        overflow: hidden;
    }

    .ba-slider10b97b22b77d4af64917b9b2ab6df72b .handle {  
        position:absolute; 
        left:50%;
        top:0;
        bottom:0;
        width:2px;
         
        
        background: rgba(0,0,0,.4);
        cursor: ew-resize;
    }
    
    .ba-slider10b97b22b77d4af64917b9b2ab6df72b .handle:after {   
        position: absolute;
        top: 50%;
        width: 10px;
        height: 64px;
        margin: -32px 0 0 -4px;
    
        content:&#39; &#39;;
        color:white;
        font-weight:bold;
        font-size:1px;
        text-align:center;
        line-height:64px;
    
        background: rgb(128, 128, 128, 0.7);
        border-radius: 0%;
    }

     
    .ba-container10b97b22b77d4af64917b9b2ab6df72b {
        position: relative;
        text-align: center;
        color: white;
    }

     
    .ba-bottom-left10b97b22b77d4af64917b9b2ab6df72b {
        position: absolute;
        bottom: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-left10b97b22b77d4af64917b9b2ab6df72b {
        position: absolute;
        top: 8px;
        left: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;;
    }

     
    .ba-top-right10b97b22b77d4af64917b9b2ab6df72b {
        position: absolute;
        top: 8px;
        right: 16px;
        font-weight: bold;
        white-space: nowrap;
        color: white;
    }

     
    .ba-bottom-right10b97b22b77d4af64917b9b2ab6df72b {
        position: absolute;
        bottom: 8px;
        right: 16px;
        white-space: nowrap;
        font-weight: bold;
        color: white;
    }

     
    .ba-centered10b97b22b77d4af64917b9b2ab6df72b {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
    }
&lt;/style&gt;

&lt;div class=&#34;ba-slider10b97b22b77d4af64917b9b2ab6df72b&#34; style=&#34;margin: auto;&#34;&gt;
    &lt;div class=&#34;ba-container10b97b22b77d4af64917b9b2ab6df72b&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/sample_microfacet_brdf/sampling_default.png&gt;&lt;img&gt;
        &lt;div class=&#34;ba-bottom-right10b97b22b77d4af64917b9b2ab6df72b&#34;&gt;Before&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;resize&#34;&gt;
        &lt;img src=https://agraphicsguynotes.com/img/posts/sample_microfacet_brdf/sampling_ndf.png&gt;
        &lt;div class=&#34;ba-bottom-left10b97b22b77d4af64917b9b2ab6df72b&#34;&gt;After&lt;/div&gt;
    &lt;/div&gt;
    &lt;span class=&#34;handle&#34;&gt;&lt;/span&gt;
&lt;/div&gt;


    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;


&lt;script&gt;
(function($) {
  function drags(dragElement, resizeElement, container) {
    
    
    dragElement.on(&#39;mousedown.ba-events touchstart.ba-events&#39;, function(e) {
      
      dragElement.addClass(&#39;ba-draggable&#39;);
      resizeElement.addClass(&#39;ba-resizable&#39;);
      
      
      var startX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
      
      
      var dragWidth = dragElement.outerWidth(),
          posX = dragElement.offset().left + dragWidth - startX,
          containerOffset = container.offset().left,
          containerWidth = container.outerWidth();
   
      
      minLeft = containerOffset + 10;
      maxLeft = containerOffset + containerWidth - dragWidth - 10;
      
      
      dragElement.parents().on(&#34;mousemove.ba-events touchmove.ba-events&#34;, function(e) {
        
        
        var moveX = (e.pageX) ? e.pageX : e.originalEvent.touches[0].pageX;
        
        leftValue = moveX + posX - dragWidth;
        
        
        if ( leftValue &lt; minLeft) {
          leftValue = minLeft;
        } else if (leftValue &gt; maxLeft) {
          leftValue = maxLeft;
        }
        
        
        widthValue = (leftValue + dragWidth/2 - containerOffset)*100/containerWidth+&#39;%&#39;;
        
        
        $(&#39;.ba-draggable&#39;).css(&#39;left&#39;, widthValue);
        $(&#39;.ba-resizable&#39;).css(&#39;width&#39;, widthValue);
      
      }).on(&#39;mouseup.ba-events touchend.ba-events touchcancel.ba-events&#39;, function(){
        dragElement.removeClass(&#39;ba-draggable&#39;);
        resizeElement.removeClass(&#39;ba-resizable&#39;);
        
        $(this).off(&#39;.ba-events&#39;); 
      });
      e.preventDefault();
    });
  }

  
  $.fn.beforeAfter = function() {
    var cur = this;
    
    var width = cur.width()+&#39;px&#39;;
    cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    
    drags(cur.find(&#39;.handle&#39;), cur.find(&#39;.resize&#39;), cur);

    
    
    $(window).resize(function(){
      var width = cur.width()+&#39;px&#39;;
      cur.find(&#39;.resize img&#39;).css(&#39;width&#39;, width);
    });
  }
}(jQuery));

$(&#39;.ba-slider&#39;+&#34;10b97b22b77d4af64917b9b2ab6df72b&#34;).beforeAfter();
&lt;/script&gt;


&lt;p&gt;There are 64 samples per pixel and GGX is chosen as NDF, the three monkeys have different roughness values(0.0,0.5,1.0). As we can see from the image that the left-most monkey gets much better result than the default sampling method. And for the other two monkeys, we have similar result with the cosine-pdf sampling method.&lt;/p&gt;
&lt;p&gt;There are already some research work improving the sampling method mentioned in this blog. I may need to try their method once I have some free cycles.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference:&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html&#34;&gt;Microfacet Models for Refraction through Rough Surfaces&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://en.wikipedia.org/wiki/Dirac_delta_function&#34;&gt;Dirac delta function&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.reedbeta.com/blog/2013/07/31/hows-the-ndf-really-defined/&#34;&gt;How is the NDF really defined&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://www.tobias-franke.eu/log/2014/03/30/notes_on_importance_sampling.html&#34;&gt;Notes on importance sampling&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Derivation of pure Specular Reflection BRDF</title>
      <link>https://agraphicsguynotes.com/posts/derivation_of_pure_specular_reflection_brdf/</link>
      <pubDate>Thu, 22 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/derivation_of_pure_specular_reflection_brdf/</guid>
      <description>&lt;p&gt;The book &amp;ldquo;Physically Based Rendering&amp;rdquo; already explains it, however I found it a little bit confusing the first time I read the chapters, which are chapters 8.2.2/8.2.3. And I also saw that there is one error of this chapter mentioned by Jérémy Riviere in the &lt;a href=&#34;https://www.pbrt.org/errata-2ed.html&#34;&gt;errata page&lt;/a&gt;. Although he provides a correct change on the equation however it is not clearly connected with the following context.&lt;/p&gt;
&lt;p&gt;This is a memo for me recording the derivation of specular reflection in a more clear way.&lt;/p&gt;
&lt;h1 id=&#34;questions-after-reading-the-two-chapters&#34;&gt;Questions after Reading the two Chapters&lt;/h1&gt;
&lt;p&gt;A couple of questions occurred to me last time I read these two chapters.&lt;/p&gt;
&lt;p&gt;There is an equation at the page of 439 describing the relationship between brdf, fresnel and incident/existence radiance.&lt;/p&gt;

$$ L_{o}(\omega_o) = f_r( \omega_o , \omega_i ) L_i(\omega_i) = F_r(\omega_i) L_i(\omega_i)$$
&lt;p&gt;Looking at the first part of this equation, it says:&lt;/p&gt;

$$ L_{o}(\omega_o) = f_r( \omega_o , \omega_i ) L_i(\omega_i) $$
&lt;p&gt;Little transformation shows something really weird:&lt;/p&gt;

$$ f_r( \omega_o , \omega_i ) = \dfrac{L_{o}(\omega_o)}{L_i(\omega_i)} $$
&lt;p&gt;I was quite confused by this equation at first. Where are the missing solid angle and cosine part? It looks like a mistake to me, so I tried to look for it in the errata page and found Jérémy Riviere&amp;rsquo;s correction about this line. It says &amp;ldquo;In the first displayed equation, the middle term should be an integral over directions \omega, along the lines of the standard reflection equation.&amp;rdquo;, the equation should be the following way according to him:&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = \int L_{i}(\omega_{i}) f( \omega_{i}, \omega_{o} ) |cos(\theta_{i})| d\omega_{i}} $$
&lt;p&gt;This equation itself is a correct one, it is the basic form of LTE without the emission component. However it doesn&amp;rsquo;t explain anything in the following context in a clear way. And things get more and more blurry if the integral is extended to the last part of the first equation. Actually you can&amp;rsquo;t just extend the integral to the third part at all, because Fresnel is not delta function, they are not equal at all. If the integral doesn&amp;rsquo;t goes to the third one, we have another confusion that is why we can replace the integral of brdf and others with a single Fresnel directly?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d say that everything is right so far except the very first one. However it is jut not clear to me how it comes with the conclusion. I spend two hours on it before I figured out a better explanation of everything above and drawing the same conclusion with pbrt&amp;rsquo;s.&lt;/p&gt;
&lt;h1 id=&#34;reflectionbrdf&#34;&gt;Reflection BRDF&lt;/h1&gt;
&lt;h2 id=&#34;relationship-between-fresnel-and-radiance&#34;&gt;Relationship between Fresnel and Radiance&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s first see the relationship between Fresnel and incident/existence radiance. Here is the definition from wiki &amp;ldquo;When light moves from a medium of a given &lt;a href=&#34;https://en.wikipedia.org/wiki/Refractive_index&#34;&gt;refractive index&lt;/a&gt; n1 into a second medium with refractive index n2, both &lt;a href=&#34;https://en.wikipedia.org/wiki/Reflection_(physics)&#34;&gt;reflection&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Refraction&#34;&gt;refraction&lt;/a&gt; of the light may occur. The Fresnel equations describe what fraction of the light is reflected and what fraction is refracted (i.e., transmitted).&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To be honest I didn&amp;rsquo;t quite understand the whole theory of Fresnel, the most important part of this line is &amp;ldquo;what fraction of light&amp;rdquo;. To be more precise, I think it should be &amp;ldquo;what fraction of flux&amp;rdquo; and that&amp;rsquo;s exactly how the book uses it at the page of 442. Let&amp;rsquo;s use it the same way here:&lt;/p&gt;

$$ d\Phi_o = F_r(\omega_i) d\Phi_i $$
&lt;p&gt;Extend it in the following way:&lt;/p&gt;

$$ L_o(\omega_o) cos( \theta_o) sin(\theta_o) dA d\phi_o d\theta_o = F_r(\omega_i) L_i(\omega_i) cos(\theta_i) sin(\theta_i) dA d\phi_i d\theta_i$$
&lt;p&gt;And we have the following condition for pure reflection surface.&lt;/p&gt;

$$ \theta_i = \theta_o $$

$$ \phi_i = \phi_o &amp;#43; \pi $$
&lt;p&gt;That said a number of factors are cancelled out. Dropping those components, we have the new equation which is much simpler than before:&lt;/p&gt;

$$ L_o(\omega_o) = F_r(R(\omega_o)) L_i(R(\omega_o)) $$
&lt;p&gt;R is the reflection vector of its input vector around normal.&lt;/p&gt;
&lt;p&gt;That is exactly the equation mentioned first, although without the middle part. Now let&amp;rsquo;s find the middle part.&lt;/p&gt;
&lt;h2 id=&#34;relationship-between-brdf-and-radiance&#34;&gt;Relationship between BRDF and Radiance&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start from the rendering equation without emission component, which is exactly how Jérémy Riviere suggested to change it.&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = \int L_{i}(\omega_{i}) f( \omega_{i}, \omega_{o} ) |cos(\theta_{i})| d\omega_{i}}$$
&lt;p&gt;What we want is the exact form of BRDF, however the only thing we have so far is that it is a delta function for pure reflection surfaces, so let&amp;rsquo;s go with it first, defining the brdf this way:&lt;/p&gt;

$$ f( \omega_{i}, \omega_{o} ) = f_{other}(\omega_{i}, \omega_{o} ) \delta( \omega_i - R(\omega_o) ) $$
&lt;p&gt; $ f_{other} $ 
 is our new target now. Drop it into the LTE mentioned above, we have this:&lt;/p&gt;

$$ \begin{array} {lcl} L_{o}(\omega_{o}) &amp;amp; = &amp;amp; \int L_{i}(\omega_{i}) f_{other}(\omega_{i}, \omega_{o} ) \delta( \omega_i - R(\omega_o) ) |cos(\theta_{i})| d\omega_{i} \\ &amp;amp; = &amp;amp; L_{i}(R(\omega_{o})) f_{other}(R(\omega_{o}), \omega_{o} ) |cos(R(\theta_{o}))| \end{array} $$
&lt;h2 id=&#34;connect-them-together&#34;&gt;Connect them together&lt;/h2&gt;
&lt;p&gt;Now we can connect them together reaching the following equation:&lt;/p&gt;

$$ L_{o}(\omega_{o}) =L_{i}(R(\omega_{o})) f_{other}(R(\omega_{o}), \omega_{o} ) |cos(R(\theta_{o}))| =F_r(R(\omega_o)) L_i(R(\omega_o)) $$
&lt;p&gt;Please be noted that this is quite similar to the one mentioned first except that we have an extra cosine term here. And a big difference between this one and the suggestion from Jérémy Riviere is that there is no complex integral at all. The first part of this equation is not that important any more, focusing on the following two parts and dropping the radiance value, we have the exact form of the missing part of our brdf.&lt;/p&gt;

$$ f_{other}(R(\omega_{o}), \omega_{o} ) = \dfrac{F_r(R(\omega_o))}{|cos(R(\theta_{o}))|} $$
&lt;p&gt;Putting them together, we have the final BRDF equation:&lt;/p&gt;

$$ f(\omega_{i}, \omega_{o} ) = \dfrac{F_r(\omega_i) \delta( \omega_i - R(\omega_o) )}{|cos(\theta_{i})|} $$
&lt;h1 id=&#34;refraction-btdf&#34;&gt;Refraction BTDF&lt;/h1&gt;
&lt;p&gt;Relationship between Fresnel and radiance won&amp;rsquo;t be mentioned here because it is well explained in the book at the pages of 442 and 443. Here it is:&lt;/p&gt;

$$ L_o(\omega_o) = ( 1 - F_r(\omega_i) ) \dfrac{{\eta_o}^2}{{\eta_i}^2} L_i(\omega_i) $$
&lt;p&gt;Derivation of the relationship between radiance and btdf is also similar to the one mentioned above, except the fact that R is replaced with T representing the transmittance direction, T can be calculated through Snell&amp;rsquo;s law. Refraction depending on different wavelength is also ignored here. Here is the final conclusion drawn by PBRT:&lt;/p&gt;

$$ f(\omega_i , \omega_o) = ( 1 - F_r(\omega_i) ) \dfrac{{\eta_o}^2}{{\eta_i}^2}\dfrac{\delta( \omega_i - T(\omega_o) )}{|cos(\theta_{i})|} $$
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering, second edition&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monte Carlo Integral with Multiple Importance Sampling</title>
      <link>https://agraphicsguynotes.com/posts/monte_carlo_integral_with_multiple_importance_sampling/</link>
      <pubDate>Mon, 10 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/monte_carlo_integral_with_multiple_importance_sampling/</guid>
      <description>&lt;p&gt;The book physically based rendering doesn&amp;rsquo;t spend too much effort explaining MIS, however it does mention it. In order to be more familiar with MIS(Multiple Importance Sampling), I spent some time reading Veach&amp;rsquo;s thesis. The whole thesis is relatively long, however the chapter about MIS is kind of independent to the other ones. Worth taking some notes in case I forget later.&lt;/p&gt;
&lt;h1 id=&#34;monte-carlo-integral&#34;&gt;Monte Carlo Integral&lt;/h1&gt;
&lt;p&gt;Monte Carlo tries to solve integral problem by random sampling. Basically it takes N independent samples randomly across the whole domain and use these samples to evaluate the integral.&lt;/p&gt;

$$ I = \int_{\Omega} f(x) \, \mathrm{d}x $$
&lt;p&gt;Assuming here is the integral to be evaluated. Monte Carlo method is quite simple. First, N independent samples  $ x_0,x_1, ... x_n $ 
 are generated according to some property density function p(x). Then the estimator is like this:&lt;/p&gt;

$$ F_{N} = \dfrac{1}{N} \sum\limits_{i=1}^{N} \dfrac{f(x_i)}{p(x_i)} $$
&lt;p&gt;The estimator is unbiased. In other words, the average of the estimator is exactly the integral we target on.&lt;/p&gt;

$$ \begin{array} {lcl} E[F_{N}] &amp;amp; = &amp;amp; E[\dfrac{1}{N} \sum\limits_{i=1}^{N} \dfrac{f(x_i)}{p(x_i)}] \\\\ &amp;amp; = &amp;amp; \dfrac{1}{N}\sum\limits_{i=1}^{N}\int_{\Omega} \dfrac{f(x_i)}{p(x_i)} p(x_i) d(x) \\\\ &amp;amp; = &amp;amp;\int_{\Omega} f(x) \, \mathrm{d}x \\\\ &amp;amp; = &amp;amp; I \end{array} $$
&lt;p&gt;Just having the average value equaling to the integral doesn&amp;rsquo;t solve the problem, it is necessary to make sure that it converge to the right one as the number of samples grows. The variance approaches to zero if N keeps growing.&lt;/p&gt;

$$ \begin{array} {lcl} V[F_{N}] &amp;amp; = &amp;amp; V[\dfrac{1}{N} \sum\limits_{i=1}^{N} \dfrac{f(x_i)}{p(x_i)}] \\\\ &amp;amp; = &amp;amp; \dfrac{1}{N^2}\sum\limits_{i=1}^{N} V[\dfrac{f(x_i)}{p(x_i)}] \\\\ &amp;amp; = &amp;amp;\dfrac{1}{N}V[\dfrac{f(x)}{p(x)}] \\\\ &amp;amp; = &amp;amp; \dfrac{1}{N}( \int_{\Omega} \dfrac{f^2(x)}{p(i)} d(x) - I ) \end{array} $$
&lt;p&gt;According to Chebychev&amp;rsquo;s inequality, we have:&lt;/p&gt;

$$ Pr\{|F_{N} - I|\ge\sqrt{\dfrac{V[\dfrac{f}{p}]}{N\delta}}\}\le\delta $$
&lt;p&gt;That said the estimator convergences to the value of integral. It is consistent.&lt;/p&gt;
&lt;p&gt;Combined with the two properties, the conclusion is that as long as we have enough samples, the result gets to the correct one we want to evaluate in the first place. And that is how we usually solve the integral problems in computer graphics.&lt;/p&gt;
&lt;p&gt;There are several benefits with this method:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monte Carlo is general is relatively simple. Sampling according to specific pdf and evaluating the values at sampling positions are all that is needed to perform the calculation.&lt;/li&gt;
&lt;li&gt;Evaluating multi-dimensional integral is straightforward. Taking samples over the whole multi-dimensional domain is the only difference.&lt;/li&gt;
&lt;li&gt;It converges to correct value at a rate of  $ O(N^{0.5})$ 
 and the fact holds for integrals with any number of dimensions.&lt;/li&gt;
&lt;li&gt;Comparing with quadrature method, it is not sensitive to the smoothness of the function. Even if there is discontinuity at unknown position in the function, Monte Carlo can also deliver the same rate of convergence, which is  $ O(N^{0.5})$ 
.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;consistent-and-unbiased&#34;&gt;Consistent and Unbiased&lt;/h2&gt;
&lt;p&gt;There are two terms mentioned above, consistent and unbiased. Although they may look similar to each other at first glance, they are totally different concept.&lt;/p&gt;
&lt;p&gt;The estimator is unbiased if:&lt;/p&gt;

$$ E[F_{N}-I] = 0 $$
&lt;p&gt;An estimator is said to be consistent if it satisfy the following condition:&lt;/p&gt;

$$ \lim_{x\to\infty} P[|F_{N}-I|&amp;gt;\epsilon]= 0 $$
&lt;p&gt;According to this wiki page, an unbiased estimator which converges to the integral value is said to be consistent. These two terms look quite similar to me, I actually thought one is the other&amp;rsquo;s super set. However it is not the case.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; $ F_{N}(X) = X_{1} $ 
&lt;br&gt;
This one is an unbiased estimator, however it never converges to anything.&lt;/li&gt;
&lt;li&gt; $ F_{N}(X) = \dfrac{1}{N}\sum x_i + \dfrac{1}{N} $ 
&lt;br&gt;
This is a consistent estimator, while it is biased.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My take here is that for unbiased estimator, as long as we compute the  $ F_N$ 
 multiple times and averaging the result will approach the right value. For consistent estimator, we need to increase the number of samples, which is N in this case, to achieve the right result. Photon mapping is a consistent method while it is not unbiased. It is consistent because you will approach the correct result by increasing the number of samples or photons. It is biased because you will never get something right by averaging the photon mapping result generated with just one sample, caustics won&amp;rsquo;t be sharp at all.&lt;/p&gt;
&lt;h1 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h1&gt;
&lt;p&gt;The first thing to do with Monte Carlo method is to pick a pdf first. Any pdf will give the right result, the simplest pdf for sampling is uniform sampling across the whole domain. Take a simple example here, to evaluate the following integral.&lt;/p&gt;

$$ I = \int_{0}^{1} f(x) dx $$
&lt;p&gt;The Monte Carlo estimator is simply the average of all sampled values:&lt;/p&gt;

$$ F_N = \dfrac{1}{N} \sum\limits_{i=1}^{N} f(x_i) $$
&lt;p&gt;However the problem is that the convergence rate may not be good if uniform sampling is used. Here is a further bad example from the book physically based rendering:&lt;/p&gt;
&lt;p&gt;Here is the integral we want to evaluate:&lt;/p&gt;

$$ f(x)= \begin{cases} 0.01&amp;amp;:x\in [0,0.01) \\ 1.01&amp;amp;:x\in [0.01,1) \end{cases} $$
&lt;p&gt;The integral value should be exactly one. Here is the pdf chosen for sampling:&lt;/p&gt;

$$ p(x)= \begin{cases} 99.01&amp;amp;:x\in [0,0.01) \\ 0.01&amp;amp;:x\in [0.01,1) \end{cases} $$
&lt;p&gt;It is totally valid to solve the integral with Monte Carlo by this sampling. It will converge to the right value as the sample number increases, however the convergence speed is terrifically bad. Most of the samples taken will reside in the range of [0,0.01), which all give the estimated value of 0.0001(actually it is a little more than 0.0001). What is even worse is that when some are sampled in the range of [0.01,1), they give extremely high values, which is 101 in this case. On average, a huge number of samples are needed to get the right value. The variance of the sampling with this pdf is unnecessarily high.&lt;/p&gt;
&lt;p&gt;On the other extreme, if the pdf is proportional to the integrated function, it gives you perfect correct result with just one sample. However it is never practical, because we won&amp;rsquo;t have certain knowledge about the function to be integrated most of the time. On the occasional cases where we know, we won&amp;rsquo;t bother to use Monte Carlo at all, because it may be integrated analytically.&lt;/p&gt;
&lt;p&gt;The general idea of importance sampling is to evaluate the samples where they contribute more to the final result. The perfect case is the one mentioned above, however it is not practical most of the time. Even if the perfect pdf is difficult to find, finding a pdf with similar shape of the integrated function still provides great value because it will take samples where they contribute more. And that is called importance sampling, a practical way of reducing the variance without introducing too much overhead.&lt;/p&gt;
&lt;p&gt;Here is the most common case in computer graphics, the function to be integrated is the rendering equation:&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = \int L_{i}(\omega_{i}) *f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;Emission is dropped here for simplicity. The difficulty of evaluating this integral comes from certain aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrated function is a product of three different components, brdf, incoming radiance and cos factor.&lt;/li&gt;
&lt;li&gt;Although we can take samples according to cos factor and we may be able to take samples according to brdf, there is no way to have any knowledge about the incoming radiance for even slightly complex scene.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A typical way of solving this integral with Monte Carlo is to simply drop some factors with the hope that the dominate factors are not them. For example, by taking samples with the pdf proportional to the cos factor, it works pretty good for lambertain materials most of the time.&lt;/p&gt;
&lt;h1 id=&#34;multiple-importance-sampling&#34;&gt;Multiple Importance Sampling&lt;/h1&gt;
&lt;p&gt;Importance sampling increases the convergence rate, however it is not everything. Sometimes you may need to take sample with different pdfs according to the situation.&lt;/p&gt;
&lt;p&gt;For example, for matte surfaces with small size light sources, it is better to sample the light source instead of the brdf. Because sampling the brdf will very likely miss the light and those samples that don&amp;rsquo;t miss the light will give relatively large result due to the small pdf value. On the contrary. for glossy surfaces with large area light sources, sampling the brdf will be better. Taking samples on the light source will probably contribute nothing to the final image and it really depends on how spiky the brdf is. The more spiky the brdf is, the better sampling brdf is. Below is an image of this situation. On the left it takes samples on the light source, there is clearly very low convergence rate for the top-right reflection. In the middle, it takes samples according to the brdf, for the lambertain surfaces, it works pretty badly with small light source. The right one is the result generated with MIS, we&amp;rsquo;ll get back later.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/monte_carlo_integral_with_multiple_importance_sampling/mis.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;In general, the rule of thumb is to generate samples according to the pdf that is proportional to the dominant factor in the rendering equation. Although we might be able to switch between sampling brdf and sampling light source for the above case depending on the surfaces and light sources, most of the time we don&amp;rsquo;t even know which one of the three is the dominant one and that is the problem.&lt;/p&gt;
&lt;p&gt;Instead of seeking to pick a perfect pdf for sampling, MIS goes another way by blending the results generated with different pdfs together without introducing noticeable noise.&lt;/p&gt;
&lt;p&gt;Suppose  $ n_i $ 
 samples are taken for  $ p_i(x) $ 
 among n pdfs. The MIS estimator is simply:&lt;/p&gt;

$$ F_{mis} = \sum\limits_{i=1}^n \dfrac{1}{n_i} \sum\limits_{j=1}^{n_i} w_i(X_{i,j})\dfrac{f(X_{i,j})}{p_i(X_{i,j})}$$
&lt;p&gt;For MIS estimator, the following conditions need to hold true:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; $ \sum\limits_{i=1}^{n}w_i(x) = 1 $ if $ f(x) \neq 0 $ 
&lt;/li&gt;
&lt;li&gt; $ w_i(x) = 0 $ if $ p_i(x) = 0$ 
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The MIS estimator is an unbiased one if the above conditions are fulfilled. Here is the proof:&lt;/p&gt;

$$ \begin{array} {lcl} E[F_{mis}] &amp;amp; = &amp;amp; E[ \sum\limits_{i=1}^n \dfrac{1}{n_i} \sum\limits_{j=1}^{n_i} w_i(X_{i,j})\dfrac{f(X_{i,j})}{p_i(X_{i,j})}] \\\\ &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^n \dfrac{1}{n_i} \sum\limits_{j=1}^{n_i} w_i(x)\dfrac{f(x)}{p_i(x)} p_i(x) dx \\\\ &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^n \dfrac{1}{n_i} \sum\limits_{j=1}^{n_i} w_i(x) f(x) dx \\\\ &amp;amp; = &amp;amp; \int \sum\limits_{i=1}^n w_i(x) f(x) dx \\\\ &amp;amp; = &amp;amp; \int f(x) dx \end{array} $$
&lt;p&gt;Actually the Monte Carlo estimator can be seen as a specialized MIS estimator where one sample is taken according to a pdf and all pdfs are exactly the same, N samples are taken in total. Being written this way will make it more clear:&lt;/p&gt;

$$ F_{mis} = \sum\limits_{i=1}^N \dfrac{1}{1} \sum\limits_{j=1}^{1} w_i(X_{i,j})\dfrac{f(X_{i,j})}{p_i(X_{i,j})}$$
&lt;p&gt;where all  $ p_i(x) = p(x)$ 
 and all  $ w_i(x) = \dfrac{1}{N} $ 
. As a specialized MIS estimator, it has one issue that is the variance is additive and it misses the purpose we want to achieve in the first place.&lt;/p&gt;
&lt;p&gt;Another special case of MIS estimator is to divide the whole domain into several and take samples in those domain separately. That is basically the method mentioned above, switching between brdf and light source depending on the environment. However it is not practical due to its simplicity. Sometimes you don&amp;rsquo;t know which one to sample.&lt;/p&gt;
&lt;h2 id=&#34;balance-heuristic&#34;&gt;Balance Heuristic&lt;/h2&gt;
&lt;p&gt;Finding good weight functions is crucial to the algorithm itself. The balance heuristic is a good one to start with.&lt;/p&gt;

$$ w_i(x) = \dfrac{n_i p_i(x)}{\sum\limits_{k}n_kp_k(x)} $$
&lt;p&gt;By putting it into the MIS estimator, we have the following equation:&lt;/p&gt;

$$ F_{mis} = \sum\limits_{i=1}^n \sum\limits_{j=1}^{n_i} \dfrac{f(X_{i,j})}{\sum\limits_{k}^{n}n_kp_k(x)}$$
&lt;p&gt;The variance will be reduced with this estimator. I don&amp;rsquo;t know the accurate proof of it. However from the equation, the estimator will be good as long as one of the pdf is similar to the function to be integrated. For example, let&amp;rsquo;s take a two pdf MIS case here:&lt;/p&gt;

$$ F_{mis} = \dfrac{f(X_{0,0})}{p_0(X_{0,0})&amp;#43;p_1(X_{0,0})} &amp;#43; \dfrac{f(X_{1,0})}{p_0(X_{1,0})&amp;#43;p_1(X_{1,0})} $$
&lt;p&gt;Assume the first pdf is a good match for the function, at places where the value of this function is high, the first pdf will likely to take samples there and even if the second one is very small, the result won&amp;rsquo;t change much unless it is very high which will dimmer the result. However if the second pdf is extremely spiky, values of pdf across the domain outside this spiky area will be quite small and that affects little to the final result with MIS. Of course the domain with high pdf values will be affected for sure, the result there will be smaller. So pdf with extremely spiky shape should be avoided unless the function itself to be integrated is one.&lt;/p&gt;
&lt;p&gt;Although MIS can reduce variance a lot, that doesn&amp;rsquo;t mean it is free to pick any pdf. Poor picking of pdf will result worse even if with MIS, like the one mentioned above. In general as long as the shape of pdf is similar to that of integrated function across part of the domain, it is a good one. The right most image above is the one generated with balance heuristic MIS.&lt;/p&gt;
&lt;h2 id=&#34;other-heuristics&#34;&gt;Other Heuristics&lt;/h2&gt;
&lt;p&gt;There are also other heuristics available. The general idea of the other heuristics is to sharpen the shape of the weighting function, making values near 1 larger and values near 0 smaller.&lt;/p&gt;
&lt;h3 id=&#34;the-power-heuristic&#34;&gt;The power heuristic:&lt;/h3&gt;

$$ w_i = \dfrac{q_i^{\beta}}{\sum\limits_{k}q_k^{\beta}} $$
&lt;p&gt; $ \beta $ i
 s 2 in pbrt implementation.&lt;/p&gt;
&lt;h3 id=&#34;the-cutoff-heuristic&#34;&gt;The cutoff heuristic:&lt;/h3&gt;

$$ w_i= \begin{cases} 0&amp;amp;:q_i&amp;lt;\alpha q_{max} \\\\ \dfrac{q_i}{\sum\limits_{k}{\{q_k|q_k\ge\alpha q_{max}\}}}&amp;amp;:otherwise \end{cases} $$
&lt;p&gt; $ q_{max} $ 
 is the maximum one among all  $ q_i$s 
.&lt;/p&gt;
&lt;h3 id=&#34;the-maximum-heuristic&#34;&gt;The maximum heuristic:&lt;/h3&gt;

$$ w_i= \begin{cases}  1&amp;amp;:q_i=q_{max} \\ 0&amp;amp;:otherwise \end{cases} $$
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Monte Carlo method is a powerful tool solving the integrals in computer graphics. With importance sampling, the variance can be reduced a lot. And MIS makes it even practical by merging samples taken from different pdfs. Although it is not a perfect solution without any addition to the variance, it works pretty well in general.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering, second edition&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.cs.columbia.edu/~keenan/Projects/Other/BiasInRendering.pdf&#34;&gt;Bias in Rendering&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://graphics.stanford.edu/papers/veach_thesis/&#34;&gt;Robust Monte Carlo Method for Light Transport Simulation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basics about path tracing</title>
      <link>https://agraphicsguynotes.com/posts/basics_about_path_tracing/</link>
      <pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/basics_about_path_tracing/</guid>
      <description>&lt;p&gt;I tried path tracing two years ago in my ray tracer. However without taking some notes, I&amp;rsquo;ve already forgotten almost everything about it. Recently I reviewed the theory and the code, picked up something from it. I&amp;rsquo;d like to take some notes so that I can get it immediately next time I forget it. Before everything, here is a Cornell box scene rendered with path tracing:&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basics_about_path_tracing/pathtracing_ref.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;I really want to render something different using my ray tracer, however it doesn&amp;rsquo;t have a GUI editor so far. Everything is through script and editing a scene through script is really a painful experience. I do plan to add GUI support using QT, however I don&amp;rsquo;t have time for it these days. Hopefully I will have some time in the near future.&lt;/p&gt;
&lt;h1 id=&#34;what-is-path-tracing&#34;&gt;What is Path Tracing&lt;/h1&gt;
&lt;p&gt;There are quite a lot of global illumination algorithms around. Radiosity is usually used to generate light map, I have briefly mentioned it in my previous blog post. However it fails to deliver correct result if mirror-like or highly glossy materials are present in the scene. Irraidiance cache is another kind of algorithm that simulates diffuse reflections between surfaces, it also captures effects like color bleeding, but fails when there is specular surfaces. Photon mapping can adapts well to those specular surfaces and it can also catch effects like color bleeding, it can even generate beautiful caustics. Although photon mapping is consistent, it is not unbiased.&lt;/p&gt;
&lt;p&gt;Path tracing is a type of ray tracing algorithm which solves the rendering equation very well. And it is unbiased and consistent which means that as the number of samples increases per-pixel, the result is heading for the physically correct value. It is simple and easy to implement once you have mastered the math behind it. Here is the simplest path tracing algorithm implementation I&amp;rsquo;ve ever seen, only 99 lines of C++. Although it is small, it has everything, soft shadow, caustics, color bleeding, reflection and refraction and it is even multi-threaded.&lt;/p&gt;
&lt;p&gt;A whitted ray tracing only handles reflection of one bounce. A path tracing extends it by generating paths recursively so that it accounts light contribution of any bounces instead of just one bounce. Effects like color bleeding requires at least light of two bounces. So it won&amp;rsquo;t be available in a whitted ray tracing. There are several questions to be answered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the math behind it?&lt;/li&gt;
&lt;li&gt;How to generate the path with specific number of bounces?&lt;/li&gt;
&lt;li&gt;How to generate infinite number of paths with finite resources of computation?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;math-behind-it&#34;&gt;Math behind it&lt;/h2&gt;
&lt;p&gt;Basically every algorithm tries to simulate the famous rendering equation. It is also a good start point for path tracing theory.&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = L_{e}(\omega_{o}) &amp;#43; \int L_{i}(\omega_{i}) *f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;The difficulty for simulating this equation is that L is appeared at both side of the equation. And L itself is a function of position and direction, that said the L at different side of this equation may have different values, most of the time they are not the same. In order to compute the Lo on the left, it is necessary to get the radiance values for Li first. Notice that we may need multiple Li since it is an integral. Actually Lo is no essentially different from Li, they are all radiance values. So &amp;hellip; in order to generate the Li, we need other radiance values which is implicit in this equation. And it is a recursive problem, never ends. That is why rendering equation is very hard to solve.&lt;/p&gt;
&lt;p&gt;We are now rewriting this equation removing the difference between Lo and Li, they are all L&lt;/p&gt;

$$ {L(\omega_{o}) = L_{e}(\omega_{o}) &amp;#43; \int L(\omega_{i}) * f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;In order to make it more clear for paths with different number of bounces, let&amp;rsquo;s redefine the L and brdf this way:&lt;/p&gt;

$$ {L(\omega) = L( x_{0} \to x_{1} )} $$
&lt;p&gt;The left L means that radiance value along the direction  $ {\omega} $ 
 , while the right one represents the radiance value from point x0 to x1.&lt;/p&gt;

$$ {f( \omega_{i}, \omega_{o} ) = f( x_{0} \to x_{1} \to x_{2} ) } $$
&lt;p&gt;This one is similar. The left one is the traditional four-dimensional brdf, the right one is the brdf value at point x1, the two directions are determined by point x0 and x2.&lt;/p&gt;
&lt;p&gt;Now it is the time to expand the equation. Replace the radiance value on the right with the equation itself. For any path, let&amp;rsquo;s define the first vertex is  $ x_0 $ 
, the second one is  $ x_1 $ 
 and so on.  $ \theta_1 $ 
 stands for the angle between surface normal and the direction pointing from  $ x_1 $ 
 to  $ x_2 $ 
.  $ {\omega_{1}}$ 
 is the solid angle around the point  $ x_1 $.
&lt;/p&gt;

$$ {L(x_{1} \to x_{0}) = L_{e}(x_{1} \to x_{0}) &amp;#43; \int L(x_{2} \to x_{1}) * f( x_{2} \to x_{1} \to x_{0} ) *cos(\theta_{1}) d\omega_{1}} $$
&lt;p&gt;Although we redefined rendering equation this way, we are still integrating over the solid angle instead of area because that&amp;rsquo;s exactly how we generate path later. If we replace the L value on the right with the equation itself, we have something like this:&lt;/p&gt;

$ \begin{array} {lcl} L(x_{1} \to x_{0}) &amp;amp; = &amp;amp; L_{e}(x_{1} \to x_{0}) &amp;#43; \int (L_{e}(x_{2} \to x_{1}) &amp;#43; \\ &amp;amp; &amp;amp; \int L(x_{3} \to x_{2}) * f( x_{3} \to x_{2} \to x_{1} ) *cos(\theta_{2}) d\omega_{2}) * f( x_{2} \to x_{1} \to x_{0} ) *cos(\theta_{1}) d\omega_{1} \\ &amp;amp; = &amp;amp; L_{e}(x_{1} \to x_{0}) &amp;#43; \\ &amp;amp; &amp;amp; \int L_{e}(x_{2} \to x_{1}) * f( x_{2} \to x_{1} \to x_{0} ) *cos(\theta_{1}) d\omega_{1} &amp;#43; \\ &amp;amp; &amp;amp; \int \int L(x_{3} \to x_{2}) * f( x_{3} \to x_{2} \to x_{1} ) *cos(\theta_{2}) * f( x_{2} \to x_{1} \to x_{0} ) *cos(\theta_{1}) d\omega_{1}d\omega_{2} \end{array} $
&lt;p&gt;It is a very long equation. The simple truth here is that it represents 0 bounces contribution (intersected object is emissive) in the third line of this equation, 1 bounce in the fourth line of it and further bounces in the last line. Maybe expend it again will be more clear, however I don&amp;rsquo;t think I can show that long a equation in such small space. In case it is too long, let&amp;rsquo;s define some basic terms first.&lt;/p&gt;
&lt;p&gt;It is not hard to get the light contribution of n bounces, n is from 1 to infinite.&lt;/p&gt;

$$ P(n) = {\int...\int} L_{e}(x_{n&amp;#43;1} \to x_{n}) * \prod\limits_{i=1}^n (f( x_{i&amp;#43;1} \to x_{i} \to x_{i-1} ) *cos(\theta_{i})) d\omega_{1}d\omega_{2}...d\omega_{n} $$
&lt;p&gt;With the above equations replacing the extended rendering equation, we have a quite simpler form.&lt;/p&gt;

$$ { L(x_{1} \to x_{0}) = L_{e}(x_{1} \to x_{0}) &amp;#43; \sum\limits_{n=1}^{\infty} P(n) }  $$
&lt;p&gt;That is the exact form we used to simulate rendering equation with path tracing. What it says is relatively clear comparing with the original rendering equation, radiance from a specific point to another is the summation of the emissive energy and the light contribution with every number of bounces. That leaves us only two problems to solve.&lt;/p&gt;
&lt;h2 id=&#34;generate-a-path-with-specific-number-of-bounces&#34;&gt;Generate a path with specific number of bounces&lt;/h2&gt;
&lt;p&gt;Now we are focusing on P(n). With Monte Carlo, we only need to generate a number of paths with n time bounces to simulate the integral. Given a specific number &amp;ldquo;n&amp;rdquo;, how to generate a path with the exact number of bounces. The answer is you can randomly pick the points anywhere you want with any probability density function(pdf), however different form of rendering equation is needed if picking point through surfaces instead of solid angle. I never seen any path tracing implementation picking sample points on surfaces, may be you can do that in cornell case. However it will introduce high variance for a moderate scene. Although PBRT does start the theory with this way, it ends up to generate path through brdf eventually.&lt;/p&gt;
&lt;p&gt;The first point is determined by your camera ray, it is fixed. To generate the second point, we need to sample a direction from the first point. Again, there are infinite number of pdf. The rule is that the more the pdf looks like the function to be integrated, which is the multiplication of brdf, cosine and radiance in this case, the faster it converges to accurate result. This is importance sampling. Actually switching from sampling surfaces to sampling solid angle is already some kind of importance sampling, because it increases the chances of non-zero contribution paths.&lt;/p&gt;
&lt;p&gt;The issue here is that we have totally no idea on what the distribution of radiance is almost all time. There is no way to generate a pdf that is similar to the function to be integrated. Typically sampling a direction by a cosine-weighted pdf works pretty well in practice. Special care needs to be paid for delta surfaces, like mirror. After sampling a direction, it is easy and straight forward to find the second point in the path we are looking for. And then repeat the process again and again until you have n-1 vertices in your path.&lt;/p&gt;
&lt;p&gt;For the last vertex, it should be exactly on a light surface. This time sampling a point on light surfaces is better than sampling brdf alone because sampling brdf will probably miss the light. Of course that&amp;rsquo;s for diffuse surfaces. For delta surfaces or highly glossy surfaces, sampling a brdf will be better.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/basics_about_path_tracing/mis.png&#34; width=&#34;800&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Here is an example taken from here. The image on the left samples light source, the glossier the surface is, the worse the result is. As for the middle one which samples brdf, we can see there is extremely low convergence rate for small light sources. The right most one uses multiple importance sampling, it gives pretty good result. And for the best part, you don&amp;rsquo;t even need to know which pdf is similar to the integrated function, it just works.&lt;/p&gt;
&lt;h2 id=&#34;sampling-infinitive-number-of-paths-with-finite-resources&#34;&gt;Sampling infinitive number of paths with finite resources&lt;/h2&gt;
&lt;p&gt;We have only one question left, how to generate result for infinitive number of bounces. Russion roullete is the key here.&lt;/p&gt;

$$ { L(x_{1} \to x_{0}) = L_{e}(x_{1} \to x_{0}) &amp;#43; \sum\limits_{n=1}^{\infty} P(n) }  $$
&lt;p&gt;Since it is assumed that the more bounces in the path, the less contribution it has to the final image. There are cases breaking this assumption for sure, however it is true most of the time. If we define a function like this:&lt;/p&gt;

$$ P^{&amp;#39;}(n)= \begin{cases} \frac{P(n)}{T}&amp;amp;:t&amp;lt;T \\ 0&amp;amp;:t\ge T \end{cases} $$
&lt;p&gt;We have certain probability to terminate the path without introducing any bias. Putting this into the rendering equation:&lt;/p&gt;

$$ { L(x_{1} \to x_{0}) = L_{e}(x_{1} \to x_{0}) &amp;#43; P(0) &amp;#43; P(1) &amp;#43; P(2) &amp;#43; \sum\limits_{n=3}^{\infty} P^{&amp;#39;}(n) }  $$
&lt;p&gt;It seems look better, however it is still needed for infinite number of paths to generate the result. Let&amp;rsquo;s go further:&lt;/p&gt;

$$ { L(x_{1} \to x_{0}) = L_{e}(x_{1} \to x_{0}) &amp;#43; P(0) &amp;#43; P(1) &amp;#43; P(2) &amp;#43; \frac{1}{T} * ( P(3) &amp;#43; \frac{1}{T} * ( P(4) &amp;#43; \frac{1}{T} * ( P(5) &amp;#43; ... ))) }  $$
&lt;p&gt;We have non-zero probability of reaching path with any number of bounces. In a practical case, it will most likely to terminate the whole computation after several number of bounces without introducing any bias in the rendering equation.&lt;/p&gt;
&lt;h1 id=&#34;optimization&#34;&gt;Optimization&lt;/h1&gt;
&lt;p&gt;Path tracing is an elegant algorithm, it captures all effects in the rendering equation. However it usually takes a great number of samples per pixel to reach acceptable result, that means a lot of time consumed by computing. There are a couple of ways to improve the performance of path tracing.&lt;/p&gt;
&lt;h2 id=&#34;reuse-generated-path&#34;&gt;Reuse Generated Path&lt;/h2&gt;
&lt;p&gt;There is totally no need to regenerate path from scratch for paths with different number of bounces. One can generate a path with russion routtele and reuse each vertex multiple times. Usually only one path is generated for each pixel sample, you can compute result for P(1), P(2), P(3) if you have a path of three bounces. Of course you can have multiple sample per pixel.&lt;/p&gt;
&lt;h2 id=&#34;multi-threaded-optimization&#34;&gt;Multi-threaded Optimization&lt;/h2&gt;
&lt;p&gt;Instead of modifying the algorithm itself, hardware resources are also valuable if used well. So I implemented multi-threading ray tracing to accelerate the whole process. The general idea is quite simple, divide the image into a number of tiles of the same size. Generate a task for each tile and push it into a task queue. Each thread picks a task once it is idle. If the queue is empty, it just terminates itself. The number of tiles should not be exactly the same with the number of thread allocated for best performance because some of them may idle if they finish their task first.&lt;/p&gt;
&lt;p&gt;It turned out a simple multi-threading model will boost the performance by more than five times on my Intel CPU, which is Xeon(R) E3-1230 v3.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering, second edition&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://www.cs.columbia.edu/~keenan/Projects/Other/BiasInRendering.pdf&#34;&gt;Bias in Rendering&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://www.kevinbeason.com/smallpt/&#34;&gt;smallpt: Blobal Illumination in 99 lines of C++&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://graphics.stanford.edu/papers/veach_thesis/&#34;&gt;Robust Monte Carlo Method for Light Transport Simulation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Directional Light Map from the Ground Up</title>
      <link>https://agraphicsguynotes.com/posts/directional_light_map_from_the_groud_up/</link>
      <pubDate>Wed, 24 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/directional_light_map_from_the_groud_up/</guid>
      <description>&lt;p&gt;A couple of days ago, an artist asked me what exactly a directional light map is. I found this &lt;a href=&#34;https://docs.unity3d.com/Manual/LightmappingDirectional.html&#34;&gt;page&lt;/a&gt; talking about the solutions in Unity 3D and explained to him. And I am reading the book &amp;ldquo;Real time rendering“ these days, it also talks about directional light map, which gives a more detailed and low-level explanation on it. Sounds like a very interesting topic, so I decided to take some notes.&lt;/p&gt;
&lt;p&gt;Although directional light map is nothing new, it is widely used in multiple commercial game engines, like Unreal Engine, Unity 3D and Source Engine. It is an advanced version of classical light map that everyone knows about. Traditional light map won&amp;rsquo;t consider normal map variance, which leads to very flat and diffuse look on surfaces. Directional light map tries to solve the problem by baking more information so that it will take normal map into account.&lt;/p&gt;


&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/directional_light_map_from_the_ground_up/1345306920_9262.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/directional_light_map_from_the_ground_up/1345306920_9262.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/directional_light_map_from_the_ground_up/1345309000_1629.png&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/directional_light_map_from_the_ground_up/1345309000_1629.png&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;p&gt;Left image shows the old light map technique. Right image demonstrates how directional light map can greatly enhance visual quality. As we can see from the two, it is apparent that directional light map shows more detail than a light map does.&lt;/p&gt;
&lt;p&gt;What exactly is a light map from the perspective of mathematics. The answer is quite simple, it stores irradiance values for each sampled location on surface. Instead of talking about directional light map alone, I decided to start from the very basic, the rendering equation. Hopefully it will gives the reader a more low-level and mathematics interpretation and a deeper understanding of this method.&lt;/p&gt;
&lt;h1 id=&#34;light-map&#34;&gt;Light Map&lt;/h1&gt;
&lt;h2 id=&#34;rendering-equation&#34;&gt;Rendering Equation&lt;/h2&gt;
&lt;p&gt;Basically everything in computer graphics, whether it is offline rendering or real time rendering, tries to simulate the famous rendering equation. The only difference is that how much assumption it has to make to design the algorithm. Some places little limitation, like path tracing. Some puts more, like ambient occlusion. The less assumption it is made, it is more likely to achieve better result.&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = \int L_{i}(\omega_{i}) *f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;In certain cases, you may have a Le component in this equation, which represents emissive materials. Since it is not involved in the context of this article, it is not shown here. Li and Lo represent radiance value for the output and input angle. &amp;ldquo;f&amp;rdquo; is the four dimensional brdf function.  $\theta_{i}$ 
is the angle between  $ \omega_{i} $ 
 and normal. Don&amp;rsquo;t be afraid of this integral, what it says is to accumulate contributions of light from all angles, nothing more.&lt;/p&gt;
&lt;p&gt;For traditional light map, there are a couple of assumptions made:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All surfaces in the scene is purely diffuse which behaves as exactly lambertian surfaces.&lt;/li&gt;
&lt;li&gt;All objects are static.&lt;/li&gt;
&lt;li&gt;All lights are static.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a scene like this, no light will change if the viewing angle is different across frames. That said we can totally bake all lighting information into textures in a pre-process step and use these textures for real time rendering. Those textures are the light map. They were first used in Quake. The question is in what form the lighting information should be stored in light map. We will answer the question in the following sections.&lt;/p&gt;
&lt;h2 id=&#34;radiance-and-irradiance&#34;&gt;Radiance and irradiance&lt;/h2&gt;
&lt;p&gt;Here are a couple of basic terms in computer graphics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flux. Energy that passes through a specific area per-time.&lt;/li&gt;
&lt;li&gt;Irradiance. Flux per unit-area&lt;/li&gt;
&lt;li&gt;radiance. Flux per unit-area per solid angle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is radiance that human can perceive and its value is not changed along a certain direction if there is only vacuum. The following equation demonstrates how to get irradiance value from radiance:&lt;/p&gt;

$$ { E = \int L_{i}(\omega_{i}) *cos(\theta_{i}) d\omega_{i}} $$
&lt;h2 id=&#34;lambertian-surface-feature&#34;&gt;Lambertian surface feature&lt;/h2&gt;
&lt;p&gt;Lambertian surface appears equally bright from all viewing directions and reflects all incident light. It has a very nice feature, the brdf function returns a constant value regardless of the two angles for any specific position. The below equation is the directional-hemisphere reflectance equation:&lt;/p&gt;

$$ \rho_{d} = \int f( \omega_{i}, \omega_{o} ) *cos(\theta_{i}) d\omega_{i} $$
&lt;p&gt;Notice that  $ \rho_{d} $ 
is a function of position. Since we are focusing on a single position, it is dropped in the above equation. Because brdf is a constant value, we have the following equation after isolating this factor out of the integral:&lt;/p&gt;

$$ \rho_{d} = f( \omega_{i}, \omega_{o} ) * \int cos(\theta_{i}) d\omega_{i} $$
&lt;p&gt;The integral component is actually  $ \pi $ 
:&lt;/p&gt;

$$ \begin{array} {lcl} \int cos(\theta_{i}) d\omega_{i} &amp;amp; = &amp;amp; \int_{0}^{2\pi} \int_{0}^{\frac{\pi}{2}} cos(\theta_{i}) sin(\theta_{i}) d\theta_i d\phi \\\\ &amp;amp; = &amp;amp; \int_{0}^{2\pi} d\phi \int_{0}^{\frac{\pi}{2}} cos(\theta_{i}) sin(\theta_{i}) d\theta_i \\\\ &amp;amp; = &amp;amp; 2\pi \int_{0}^{\frac{\pi}{2}} cos(\theta_{i}) sin(\theta_{i}) d\theta_i \\\\ &amp;amp; = &amp;amp; 0.5 * \pi \int_{0}^{\frac{\pi}{2}} sin(2\theta_{i}) d(2\theta_i) \\\\ &amp;amp; = &amp;amp; \pi \end{array} $$
&lt;p&gt;So we have the final equation for our lambertian brdf, which is extremely simple:&lt;/p&gt;

$$ f(\omega_{i}, \omega_{o}) = \dfrac{\rho_{d}}{\pi} $$
&lt;p&gt;If we substitute the brdf function in the rendering equation with this one, here is what we will get:&lt;/p&gt;

$$ {L_{o}(\omega_{o}) = \dfrac{\rho_{d}}{\pi} * \int L_{i}(\omega_{i}) *cos(\theta_{i}) d\omega_{i}} $$
&lt;p&gt;The lucky thing is that the integral part of this equation turns out to be irradiance. And irradiance is what needs to be stored in light map. As a matter of fact, light map was also named irradiance map by some people.&lt;/p&gt;
&lt;h2 id=&#34;radiosity-algorithm&#34;&gt;Radiosity Algorithm&lt;/h2&gt;
&lt;p&gt;Radiosity is usually the algorithm for calculating those values. Basically it divides the scene into many small patches and generates form factors between each two patches and then makes some interations. During the first iteration, only emissive patches have energy, they distribute those energy to other visible patches. After the first iteration, patches directly visible to emissive patches receive some energy, however there won&amp;rsquo;t be effects like color bleeding or multiple light bounces. Some patches will be totally black. With the second iteration, there are more patches with energy that can be distributed to other patches. Even if patches not directly visible to emissive patches, usually light sources, will be lit this time. After several rounds of iteration, it will reach equilibrium. It should give a good result. Check &lt;a href=&#34;https://en.wikipedia.org/wiki/Radiosity_(computer_graphics)&#34;&gt;here&lt;/a&gt; for more detailed explanation.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/directional_light_map_from_the_ground_up/680px-radiosity_progress.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;The algorithm is not suitable for non-lambertian brdf surfaces. For example, mirrors will not be rendered correctly with this technique alone because the lambertian brdf is totally different from a highly reflective brdf. And it is extremely slow.&lt;/p&gt;
&lt;h2 id=&#34;light-map-limitation&#34;&gt;Light Map Limitation&lt;/h2&gt;
&lt;p&gt;The robustness of an algorithm depends on the assumption it makes. Since we are focusing on static objects and static lights, those two are sort of acceptable. You can totally add dynamic light contribution during rendering. However not all of the surfaces are lambertian in the real case. Some have mirror like appearance, which will cause caustic and that&amp;rsquo;s why there won&amp;rsquo;t be caustic in light map. Anyway, those are not serious limitation.&lt;/p&gt;
&lt;p&gt;One big advantage of light map over dynamic light computation during rendering is that there will be multiple light bounces, like color bleeding and soft shadow, although they are all static most of the time. So it usually represents low-frequency information, which is another implicit assumption made by light map algorithm. With this assumption, resolution of light map can be substantially reduced. We don&amp;rsquo;t usually allocate large memory for light map so that there is a one on one correspondence between texels in light map and texels in diffuse map. Light map usually needs to cover the whole scene. Although diffuse map also needs to cover the whole scene, it is usually tiled or repeated when sampled. It won&amp;rsquo;t be the same for light map, it is usually not tiled. The issue is if there is no one on one relationship between texels, there is no way to bake detailed information like variations caused by normal map. That&amp;rsquo;s why the scene looks flat with only light map.&lt;/p&gt;
&lt;p&gt;In order to show variance brought by normal map, directional light map is introduced to solve the issue.&lt;/p&gt;
&lt;h1 id=&#34;directional-light-map&#34;&gt;Directional Light Map&lt;/h1&gt;
&lt;p&gt;The reason we don&amp;rsquo;t have normal map variance when rendering with light map is that the irradiance value collected during pre-processing is integrated over the hemisphere around the geometry normal, not the surface normal defined through normal map or other method. It is valid to use the surface normal during preprocessing, however it will ends up in the issue of large light map memory that we tried to avoid in the first place.&lt;/p&gt;
&lt;p&gt;Another impractical solution is to generate irradiance environment map at every position sampled in the scene. During rendering, the irradiance value could be retrieved through irradiance environment map and the surface normal. Regardless of the memory usage, it is the ideal solution for the issue. An advantage over the solution mentioned above is that it will support for dynamic normal because irradiance value is available for every direction. However the huge memory requirement renders it totally useless.&lt;/p&gt;
&lt;h2 id=&#34;spherical-harmonics&#34;&gt;Spherical Harmonics&lt;/h2&gt;
&lt;p&gt;One practical solution for directional light map is using spherical harmonics to represent the irradiance environment map. Usually a couple of float numbers are needed to store the irradiance environment map and it is much more practical comparing with storing irradiance map directly.&lt;/p&gt;
&lt;p&gt;The disadvantage is that there is still a great amount of memory needs to be stored in light map and it takes some extra cost than a normal light map algorithm because it is necessary to reconstruct the irradiance value with spherical harmonics.&lt;/p&gt;
&lt;h2 id=&#34;radiosity-normal-map&#34;&gt;Radiosity Normal Map&lt;/h2&gt;
&lt;p&gt;A much more practical solution is proposed by Valve, it is called radiosity normal map by them. It is not physically correct, however it works pretty well in the real case.&lt;/p&gt;
&lt;p&gt;Instead of integrating over the hemisphere around geometry normal, three different basis are used during baking irradiance. That gives three different irradiance value for each sampled position.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/directional_light_map_from_the_ground_up/basis.png&#34; width=&#34;300&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;Those three basis are not arbitrarily chosen. They are pairwise orthogonal and cover the whole hemisphere. The reconstruction process is relatively simple:&lt;/p&gt;

$$ \begin{array} {lcl} color &amp;amp;=&amp;amp; dot( normal , basis[0] )^2 * lightmap\_color[0] &amp;#43; \\ &amp;amp; &amp;amp; dot( normal, basis[1] )^2 * lightmap\_color[1] &amp;#43; \\ &amp;amp; &amp;amp; dot( normal , basis[2] )^2 * lightmap\_color[2] \end{array} $$
&lt;p&gt;Please note that the equation above is not exactly the same with the one mention in the paper by valve, it is the one from real time rendeing. Because the equation in the paper is incorrect. Since it is orthogonal basis that the normal projects on, there is no need to normalize the factors because the summation of the squared projected values is exactly 1.&lt;/p&gt;
&lt;p&gt;Source engine by Valve uses this method. And Unreal Engine 3 uses a simpler version of it, only one irradiance value and three intensity value (three numbers) are stored. Instead of blending among the three, intensity values are blended and then multiply the irradiance value. So varying per-pixel values in normal map can only produce different brightnesses of the lightmap texel color instead of different colors.&lt;/p&gt;
&lt;p&gt;If normal is not used for other purpose, normal map can be changed in another different form. Instead of storing normal in it, the squared projected values of the normal on the basis can be stored in it. And it can save many hardware cycles during real time calculation. &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.406.4625&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Here&lt;/a&gt; is more detailed explanation.&lt;/p&gt;
&lt;h2 id=&#34;other-methods&#34;&gt;Other methods&lt;/h2&gt;
&lt;p&gt;There are also other methods targeting on directional light map. Anyway, each of them tries to add more detail through their pre-baked light map and the basic rule always stands out, taking normal into account during light calculating of each frame rendering.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Directional light map is an extension to traditional light map algorithm, it takes surface normal intro account during rendering, which brings much more detailed information to the final generated image.&lt;/p&gt;
&lt;p&gt;Although it may introduces a couple of extra instructions comparing with the old normal map, it is quite affordable even on mobile devices, that&amp;rsquo;s why UE3 has an internal implementation of it on mobile. Given the extra quality it brings, those cost should be considered worthy.&lt;/p&gt;
&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://docs.unity3d.com/Manual/LightmappingDirectional.html&#34;&gt;Directional Light Map, Unity document&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://web.student.tuwien.ac.at/~e9907459/docs_tutorials/directionalLightmaps.pdf&#34;&gt;Directional Lightmap&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;http://pbrt.org/&#34;&gt;Physically Based Rendering&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;http://www.realtimerendering.com/&#34;&gt;Real Time Rendering&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unleash the power of Direct3D 12</title>
      <link>https://agraphicsguynotes.com/posts/unleash_the_power_of_direct3d_12/</link>
      <pubDate>Wed, 17 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/unleash_the_power_of_direct3d_12/</guid>
      <description>&lt;p&gt;Direct3D12 is about to come. There are several presentations available in GDC this and last year talking about the new features in it. I&amp;rsquo;m gonna list some of the changes that D3D12 introduces in this post. It only covers some of the changes.&lt;/p&gt;
&lt;h1 id=&#34;the-big-picture&#34;&gt;The big picture&lt;/h1&gt;
&lt;p&gt;Different from its predecessors, D3D12 mainly aims at reducing CPU overhead. Improving CPU performance is the first priority in this new API. Of course that&amp;rsquo;s not to say there is nothing more. Besides better CPU performance, it has some feature improving GPU performance. Apart from performance improvement, there are also some new graphics features available on latest hardware, such as conservative rasterization, raster ordered view, etc. In this article, I&amp;rsquo;m gonna talk something about the new features in D3D12 that improves performance. New graphics features are won&amp;rsquo;t be mentioned.&lt;/p&gt;
&lt;p&gt;The followings are the most important changes between 11 and 12:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;D3D11 uses an immediate context with one or more deferred context. Deferred context is rarely used because most of the driver work is still in one single thread, like resolving resource dependencies. There is no immediate context in d3d12 anymore, everything is deferred. With this change, it allows game developers to utilize the   multi-thread power of their hardware.&lt;/li&gt;
&lt;li&gt;Hardware states used to be merged into several separated state group, like rasterizer state, depth stencil state. And programmable shaders are set through dedicated interfaces, such as VSSetShader. That&amp;rsquo;s the old model in d3d11. In the brand new d3d12 model, we have PSO(pipeline state object) grouping almost all of the states and shaders together as a single object.&lt;/li&gt;
&lt;li&gt;Memory used to be strongly typed in old d3d. Developers have full control on it now. For buffers, we have buffer heap and it is totally valid for games developers to place multiple vertex buffer data and index buffer data in the same heap. This is no such as thing called vertex buffer object or index buffer object anymore, every buffer is just generic data. Though, there is still flags indicating the usages of buffers.&lt;/li&gt;
&lt;li&gt;There are 128 shader resource views for per programmable stage in D3D11. The number is limitless for d3d12. However, it is set in a whole new different way. Root signature is the window for doing all the resource binding stuff. There are still some resources to be set through old-school interface, like vertex buffer and index buffer. For the ones we care, like textures, constant data, samplers, it is no longer working this way.&lt;/li&gt;
&lt;li&gt;D3D11 tracks the resource states through reference counting, making sure that resource lifetime and residency management is well handled and avoids resource hazard. It is used to be a strange topic for some PC developers because game developers are not required to handle the issues themselves. In D3D12, developer needs to resolve these things themselves. It does intrudoce more work, but this is one of the key changes to allow multi-thread rendering, which simpliy puts is to allow issuing draw calls in different threads.&lt;/li&gt;
&lt;li&gt;D3D11 will accumulate commands for at most three frames before flushing the command list to hardware push buffer, the number is basically transparent to programmers. Now it is developers&amp;rsquo; responsibility to manage it explicitly, fencing is a necessary thing in D3D12.&lt;/li&gt;
&lt;li&gt;And D3D12 users should also manage back buffer explicitly from now on, D3D runtime won&amp;rsquo;t babysit it for us.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above list is by no means a full review of what d3d12 has brought to us. But they are considered some of the most important changes.&lt;/p&gt;
&lt;h1 id=&#34;explicit-memory-management&#34;&gt;Explicit Memory Management:&lt;/h1&gt;
&lt;p&gt;Memory in D3D11 used to be strongly typed during initialization and declaration. A vertex buffer is created through the interface called createvertexbuffer, very self-explanatory, it just returns an object that represents the resource of vertex buffer. We can fill the data during creation or later. There are some drawbacks with the resource management model. Memory address is transparent to the developers, who are out of control. There may be some small pieces memory between resources which will never be used for a decent-sized resource, it depends on the specific implementation of drivers. And little detail information is revealed to public.&lt;/p&gt;
&lt;p&gt;The good news in D3D12 is that we will have full control on the memory allocation and residency management. Let&amp;rsquo;s take an example, we want a vertex buffer and an index buffer for a specific model, it is a typical scene. What we have to do in D3D11 is to create two separate resources. Here is the new way, it is necessary to create a heap for buffers first. The size of the heap can be the sum of the size of total vertices data and indices data or more if we like. After creating the heap, we can copy the vertex and index data into the heap to some location. Where and when to copy the data is totally up to us. We can create a vertex buffer view and index buffer view according to the addresses we put them. Those views will be used to set the buffers. We have a lot more control during the initialization. What&amp;rsquo;s more nature is that we are treating vertex and index data as generic ones, they are just some memory. Regardless of their usages, there are no essentially difference. Even amazing is that we can put constant buffer in the same heap, it is totally a valid way of doing it. Those geometry information is parsed from files and developers are free to choose what kind of memory to use for them. There is just no reason to give up that flexibility in graphics interface.&lt;/p&gt;
&lt;h1 id=&#34;explicit-resource-management&#34;&gt;Explicit Resource Management:&lt;/h1&gt;
&lt;h2 id=&#34;resource-lifetime-and-residency&#34;&gt;Resource lifetime and residency&lt;/h2&gt;
&lt;p&gt;Resource lifetime and residency management used to be handled by D3D11 runtime. Programmers usually don&amp;rsquo;t need to care much about it. The sacrifices we have to make for this convenience is more CPU overhead through a lot of reference counting under the hood and the common &amp;lsquo;solution&amp;rsquo;, which is more of workarounds to me, is to find varies ways to reduce the number of draw calls.&lt;/p&gt;
&lt;p&gt;In order to reduce the CPU overhead, D3D12 programmers need to manage resource life time and residency explicitly. In this way, there is no need for D3D12 runtime to add reference counting for the purpose of lifetime and residency management, it will assume that everything is handled by programmer with their higher level knowledge. For example, in the old d3d, if we want to change something in a vertex buffer, we can map it first with &amp;ldquo;discard&amp;rdquo; flag, we will get a memory address immediately after this function call without flushing all API commands that relate to this specific vertex buffer, what happens is actually that another piece of memory is returned to us by driver and this very piece of memory will be used as the vertex buffer at a latter point. Through that way, the driver doesn&amp;rsquo;t need to flush any related command calls before returning the memory address and it looks decent to the programmers. The efficiency will be lower if the size of memory is quite large. Of course there are other methods for updating the resource. In the new model, we are responsible for everything. It works in a map-persistent way similar to OpenGL4. We can map the buffer at the initialization stage and never unmap it. We can change the memory anytime we want. One simple rule holds here, since CPU and GPU are working asynchronously, it is necessary to make sure that we don&amp;rsquo;t stamp on the memory in use by GPU. Usually a ring/circular buffer is used to avoid conflict.&lt;/p&gt;
&lt;h2 id=&#34;resource-hazard-tracking&#34;&gt;Resource hazard tracking&lt;/h2&gt;
&lt;p&gt;Similar to resource lifetime and residency management, resource hazard is another issue that belongs to the programmers&amp;rsquo; responsibilities from now on.&lt;/p&gt;
&lt;p&gt;Suppose we have a shadow map algorithm, a shadow map generation pass is performed first, followed with a shadow masking pass. In the first pass, a texture is used as a render target and it is then used as a shader resource view for reading in shadow masking pass. D3D11 runtime and driver will make sure to finish the shadow map generation pass before the execution of shadow masking pass, it won&amp;rsquo;t in D3D12. D3D12 runtime will only guarantee that shadow map generation pass is issued earlier than shadow masking pass, in other words, instructions for the first pass will be before the instructions in the second pass in the command list or push buffer. However what it doesn&amp;rsquo;t promise is that shadow masking pass is finished before the beginning of shadow masking pass, which may result in invalid reading of shadow map in shadow masking pass just because the first pass may not finish itself. That is a typical resource hazard case.&lt;/p&gt;
&lt;p&gt;What we need to do in D3D12 is to add a resource barrier to make sure everything is in the right state. With a resource barrier, the driver will wait for the first pass to finish before proceeding to the next ones. A big difference is that programmers are in control now.&lt;/p&gt;
&lt;p&gt;For better performance, a group of resource barriers can be triggered together. Resource barrier can even be split into two parts, begin and end. It will avoid dummy idle waits if there is gonna be one.&lt;/p&gt;
&lt;h1 id=&#34;resource-binding&#34;&gt;Resource binding:&lt;/h1&gt;
&lt;p&gt;Resources, like vertex buffer and render target, are still set in a similar way. Only the ones we care are mentioned in this section. They are the famous draw call breakers, constant data, textures and samplers. We have four type of views in D3D11, shader resource view, render target view, depth stencil view and unordered access view. There will be more in 12, vertex buffer view, index buffer view, constant buffer view, stream output buffer view. Good thing is they are no longer d3d object anymore. Most of the new comers are just simple, transparent structures well defined in the d3d12 header file, we can see everything in the structure. For SRV, RTV, DSV, UAV and CBV, a heap needs to be created first. Each should have a dedicated heap, we can&amp;rsquo;t mixed them in one single heap, except that we can mix SRV, CBV, UAV in a same heap. Within the heap, we can create as many views as we want. Most of the views are set through specific interfaces. However for SRV, CBV and UAV, we need to set them through a new object, called root signature.&lt;/p&gt;
&lt;h2 id=&#34;root-signature&#34;&gt;Root Signature:&lt;/h2&gt;
&lt;p&gt;Root Signature is a whole new concept. It behaves like a window for binding resources to shaders. There are three type of data can be set through it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Descriptor table. It is nothing mystery. Just an offset in the heap and the number of descriptors(views) to be set.&lt;/li&gt;
&lt;li&gt;Descriptor. It should be the same with view, just different names.&lt;/li&gt;
&lt;li&gt;Constant Data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each has some unique features. Descriptor table can bind multiple resources in one go. While it requires two extra memory fetches on GPU, get the memory of descriptor table, then get the descriptor before reaching the data we are interested. Putting constant data in root signature will get the best performance in term of GPU, because there is no extra memory fetch at all. Descriptor is someone middle, we can only set one resource once and it only introduces one extra memory fetch.&lt;/p&gt;
&lt;p&gt;And don&amp;rsquo;t put too much data in root signature, because d3d runtime will version it under the hood. The specific detail is not revealed, however that should be the reason developers don&amp;rsquo;t need to maintain root signature&amp;rsquo;s lifetime before the draw call is executed on GPU. Another thing to be noted is to reduce changes in root signature. Each change will trigger some cost in the runtime and driver. Change it only if it is necessary.&lt;/p&gt;
&lt;h2 id=&#34;pipeline-state-object&#34;&gt;Pipeline State Object:&lt;/h2&gt;
&lt;p&gt;A single PSO almost captures all of the hardware states except those ones that are easy to be set, like view port and scissor rect. It usually needs to be created during initialization. There are some benefits by introducing PSO:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shader compiling will be done after PSO is created. There won&amp;rsquo;t be any shader recompiling that stalls the graphics pipeline during rendering. On some old hardware without ROPs, switching between different blend states may even trigger shader recompile.&lt;/li&gt;
&lt;li&gt;There are usually several states to be set before issuing a draw call and those hardware instructions used to be generated on the fly in rendering loop, which take some CPU cycles. With PSO, all of the hardware instructions could be pre-baked during initialization.&lt;/li&gt;
&lt;li&gt;A lot of validation work needs to be done right before issuing a draw call, for example checking if the bound textures are valid ones. They are gone now.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are something deserve our attention when using PSO:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create them on separate threads to avoid stalling rendering.&lt;/li&gt;
&lt;li&gt;Use default values for fields that we don&amp;rsquo;t care. Most of game engines have their high level cache of PSO, setting default values for don&amp;rsquo;t-care field will also result in better cache hit rate.&lt;/li&gt;
&lt;li&gt;Avoid frequent switching hardware states by using similar PSOs among successive draw calls.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;draw-callissuing&#34;&gt;Draw Call Issuing&lt;/h1&gt;
&lt;p&gt;There are also some dramatic changes going on in the draw call execution model between 11 and 12.&lt;/p&gt;
&lt;p&gt;11 is pretty simple, we want to issue a draw call or other API call, we feed it to the immediate context. Of course it doesn&amp;rsquo;t mean that the draw call is performed by GPU immediately, however it does return immediately, the command will be buffered for later execution. A big disadvantage is that immediate context is not thread-safe. There is no way to submit draw calls across multiple threads. 11 tried to distribute CPU overhead across multiple cpu cores through the introduction of deferred context. Although we can submit more draw calls through the deferred context in another thread, most of the heavy lifting is still done in one single thread, the main rendering thread which immediate context is working. It doesn&amp;rsquo;t work too well, no where near what d3d12 brings.&lt;/p&gt;
&lt;p&gt;12 solved the issue in a perfect way. There is no immediate context anymore. What we have now is a brand new method for submitting draw calls.&lt;/p&gt;
&lt;p&gt;There is command queue. More like a low level concept which used to exist in drivers. There are three types, graphics, compute and copy queue. Each is a super set of the following one. The graphics command queue can perform any kind of method, draw calls, dispatch calls and copy commands. The compute queue can&amp;rsquo;t do any graphics command and copy queue can only perform copy instructions. We can have multiple command queues available in our program. The hardware may overlap some operations if possible.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s focus on graphics queue. The draw calls can&amp;rsquo;t be put in the command queue directly. Command queue only takes in command list. A command list is a bunch of commands to be executed by GPU, we can record the command list by submitting draw calls to it. And command list is thread-safe. That said we can allocate many command lists and each one of them is handled in a separate thread. In that way we can do the real multi-thread rendering which will distribute the cpu overhead during command recording across multiple cpu cores. Recording of command lists takes most of the time and executing them on a command queue doesn&amp;rsquo;t take much cpu overhead on CPU. And we don&amp;rsquo;t need to generate the command lists in the same order they are submitted.&lt;/p&gt;
&lt;p&gt;If something is visible in this frame, it is highly possible that it will be available in the next frame except that it may be in different position. However the set of commands of generating it could be exactly the same, only some constant data, such as view matrix, is changed. So we waste a big amount of time to do something that we&amp;rsquo;ve already done in the previous frame. Bundle tries to solve the problem by pre-baking all of the hardware instructions for certain number of draw calls, which is usually 10+. Bundle behaves like a smaller version of command list and it should be reusable across frames, not intend to regenerate every frame.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;This post only summarize some of the important changes introduced in D3D12. It doesn&amp;rsquo;t cover all of the changes.
There are things like conservative rasterization, raster ordered view and a bunch of other fancy graphics features introduced in D3D12 too.&lt;/p&gt;
&lt;p&gt;Comparing with its predecessor, D3D12 brings a lot more flexibility, which comes along with lots of responsibility.
And similar APIs, like Vulkan, Metal, share a common way of how modern graphics API works.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References:&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://schedule.gdconf.com/session/advanced-directx12-graphics-and-performance-presented-by-microsoft&#34;&gt;Advanced DirectX12 Graphics And Performance&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;http://schedule.gdconf.com/session/better-power-better-performance-your-game-on-directx12-presented-by-microsoft&#34;&gt;Better Game, Better Performance : Your Game on DirectX12&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://channel9.msdn.com/Events/Build/2014/3-564&#34;&gt;Direct3D12 API Preview&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;http://gdcvault.com/play/1020791/&#34;&gt;Approaching Zero Driver Overhead&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/Getting-the-best-out-of-D3D12.ppsx&#34;&gt;Getting the best out of Direct3D12&lt;/a&gt;&lt;br&gt;
[6] &lt;a href=&#34;http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/D3D12-A-new-meaning-for-efficiency-and-performance.ppsx&#34;&gt;Direct3D12 A new Meaning and Efficiency for Performance&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tessellation on DX11</title>
      <link>https://agraphicsguynotes.com/posts/tessellation_on_d3d11/</link>
      <pubDate>Tue, 16 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://agraphicsguynotes.com/posts/tessellation_on_d3d11/</guid>
      <description>&lt;p&gt;One of the most important changes that DX11 has made is the brand new feature called tessellation. By introducing three more stages, graphics programmer can tessellate their triangles on the fly. There are some benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Models with more geometry detail. With phong tessellation, it smoothes the silhouette so that no sharp edge corner will be visible.  Combined with displacement map, tessellation can produce bump surfaces in a much more realistic way than what can be achieved with normal map or POM.&lt;/li&gt;
&lt;li&gt;Less calculation on full detailed model. It saves a lot of time by placing intense computation, like bone matrix multiplication, in vertex shader which only processes control points of the model.&lt;/li&gt;
&lt;li&gt;Dynamic LOD. Since tessellation factors, which determines how to tessellate the object, are calculated on the fly during rendering, level of detail can be decided based on factors like distance, angle, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this article, I&amp;rsquo;d like to mention some detail that I learned about tessellation.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction:&lt;/h1&gt;
&lt;p&gt;What is tessellation? This should be a question to be addressed first before everything.&lt;/p&gt;
&lt;p&gt;Tessellation is a new feature that enables graphics programmer to tessellate their triangles on the fly so that the geometry detail will be enriched. There are three stages right after vertex shader, hull shader, tessellator and domain shader with each stage performing separate functionality.&lt;/p&gt;
&lt;p&gt;There are two different functions in Hull Shader, one is the hull shader itself and the other is the constant function which runs on a per-patch basis. In OpenGL 4, it is called tessellation control shader and there is no extra per-patch constant function. Anyway, they are still pretty similar to each other. And they both take in control points and generate some tessellation factors and per-patch constant data.&lt;/p&gt;
&lt;p&gt;Tessellator is a fixed function stage. It can&amp;rsquo;t be programmed directly and neither can it be setup through API interfaces, it is affected by tessellation factors, the output of hull shader (or TCS in OpenGL). It generates those tessellated triangles under the hood and feeds them with the constant data to the next stage, domain shader. One thing to be noted is that the vertices generated in tessellator are located in barycentric coordinate. It is programmer&amp;rsquo;s job to evaluate the final clip space position with the intermediate data in the barycentric coordinate.&lt;/p&gt;
&lt;p&gt;Domain shader is the place for calculating the positions of generated vertices. Whether it is phong tessellation, PN or flat tessellation is decided here. And it is also responsible for generating vertices in clip space, like VS does without tessellation enabled. Of course it can just pass the vertices to the next stage if this is what is needed.&lt;/p&gt;
&lt;p&gt;Here is a comparison between tessellation, POM and normal map, those captures come from the detail tessellation demo from D3D SDK. The left one is normal map, the quality quickly breaks down when viewed at a wide angle. POM solves the problem with a ray marching method, however it still shows artifacts at the silhouette of objects. SPOM can solve the problem, however it is far beyond the topic of this blog. Tessellation gives the perfect result, the rocks look really bumpy here.&lt;/p&gt;


&lt;style&gt;
    .row {
        display: -ms-flexbox;  
        display: flex;
        -ms-flex-wrap: wrap;  
        flex-wrap: wrap;
        padding: 0 4px;
    }
    
     
    .column {
        -ms-flex: 33%;  
        flex: 33%;
        padding: 0 4px;
    }
&lt;/style&gt;
&lt;div class=&#34;row&#34;&gt;

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/normalmap.jpg&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/normalmap.jpg&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/pom.jpg&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/pom.jpg&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

  
  
  
  &lt;div class=&#34;column&#34;&gt;
        &lt;a href=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/tessellation.jpg&#34;&gt;
        &lt;img
            src=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/tessellation.jpg&#34;
            style=&#34;width:100%&#34;
            alt=&#34;&#34;&gt;
        &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

&lt;h1 id=&#34;more-detail&#34;&gt;More Detail:&lt;/h1&gt;
&lt;p&gt;Before diving into the details, let&amp;rsquo;s see how to enable tessellation through the API. There is no specific functions to enable tessellation. To enable it, you need to create a valid hull shader and domain shader. By binding those shaders in the context, tessellation will be enabled. Of course it is necessary to adjust your shader code in VS. Without tessellation, vertex shader normally outputs at least clip space position. With tessellation, VS can choose only to pass the data to hull shader most of the time.&lt;/p&gt;
&lt;p&gt;Besides, the primitive topology needs to be changed accordingly. If you use a low level of detail modal as control point set, it should be &amp;ldquo;D3D11_PRIMITIVE_TOPOLOGY_3_CONTROL_POINT_PATCHLIST&amp;rdquo;. Another typical scene is to use &amp;ldquo;D3D11_PRIMITIVE_TOPOLOGY_POINTLIST&amp;rdquo; for landscape rendering, each point will be tessellated to a quad after tessellator. Anyway, as long as the information fits your algorithm, any type of primitive topology is fine.&lt;/p&gt;
&lt;h2 id=&#34;hull-shader&#34;&gt;Hull Shader:&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s talk about hull shader then. A hull shader is the first of the three new stages to implement tessellation. There are two functions, constant function and hull shader.&lt;/p&gt;
&lt;p&gt;A constant function runs on a per-batch basis, it outputs some constant data for tessellator and domain shader. &amp;ldquo;SV_TessFactor&amp;rdquo; and &amp;ldquo;SV_InsideTessFactor&amp;rdquo; are needed by tessellator. And each different type of primitive may need different number of these factors. For example, there should be one inner tess-factor and three outer tess-factors in the constant data. A constant function is also free to generate more data, tessellator will ignore them, however they could be consumed by domain shader. Here is a simple constant function that tessellates all of the triangles:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#0f0;font-weight:bold&#34;&gt;#define MAX_POINTS 32
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#0f0;font-weight:bold&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Control point, this is generated from VS
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;struct&lt;/span&gt; VS_CONTROL_POINT_OUTPUT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    float3 vPosition : WORLDPOS;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    float2 vUV       : TEXCOORD0;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    float3 vTangent  : TANGENT;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Hull shader output
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;struct&lt;/span&gt; HS_CONSTANT_DATA_OUTPUT
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; Edges[&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;3&lt;/span&gt;] : SV_TessFactor;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; Inside : SV_InsideTessFactor;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;// Hull shader Entry
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;HS_CONSTANT_DATA_OUTPUT ConstantHS( InputPatch&amp;lt;VS_CONTROL_POINT_OUTPUT, MAX_POINTS&amp;gt; ip, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                    uint i : SV_OutputControlPointID,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                    uint PatchID : SV_PrimitiveID )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    HS_CONSTANT_DATA_OUTPUT output  = (HS_CONSTANT_DATA_OUTPUT)&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// In this simple example, density is purely a constant, but it could be anything
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// calculated from the input data.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;	&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;float&lt;/span&gt; density = &lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;64.0f&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	output.Edges[&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0&lt;/span&gt;] = density;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	output.Edges[&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;1&lt;/span&gt;] = density;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	output.Edges[&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;2&lt;/span&gt;] = density;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	output.Inside = density;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;return&lt;/span&gt; output;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In a real scenario, you may want to adjust your tessellation factor based on information that is already available. Dynamic LOD could be done here through calculating the tessellation factors based on distance between eye point and patch. Apart from all, one can also set up 0 as a factor, it is a valid number for tessellation factor. And tessellator will automatically drop the patch with a zero tessellation factor. Since SV_POSITION is not determined yet, it is impossible for the hardware to do clipping. However it is totally valid to do some culling in this function. If you are certain that the patch won&amp;rsquo;t be visible at all, you can set 0 as its tessellation factor to prevent it from further processing. Just be very careful when trying hull culling, because it may reject patches that could be visible after displacement.&lt;/p&gt;
&lt;p&gt;A Hull shader is the per-control point function. Sometimes, it just passes the data through. Here is some piece of code of a hull shader:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;struct&lt;/span&gt; HS_OUTPUT{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    float3 vPosition : WORLDPOS;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    float2 vUV       : TEXCOORD0;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    float3 vTangent  : TANGENT; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[domain(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;tri&amp;#34;&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[partitioning(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;pow2&amp;#34;&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[outputtopology(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;triangle_cw&amp;#34;&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[outputcontrolpoints(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;3&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[patchconstantfunc(&lt;span style=&#34;color:#0ff;font-weight:bold&#34;&gt;&amp;#34;ConstantsHS&amp;#34;&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[maxtessfactor(&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;64.0&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;HS_CONTROL_POINT_OUTPUT HS( InputPatch&amp;lt;VS_CONTROL_POINT_OUTPUT, MAX_POINTS&amp;gt; ip, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                            uint i : SV_OutputControlPointID,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                            uint PatchID : SV_PrimitiveID)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    HS_CONTROL_POINT_OUTPUT output = (HS_CONTROL_POINT_OUTPUT)&lt;span style=&#34;color:#ff0;font-weight:bold&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#007f7f&#34;&gt;// Copy inputs to outputs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007f7f&#34;&gt;&lt;/span&gt;    output.vPosition = inputPatch[uCPID].vPosition;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    output.vUV = inputPatch[uCPID].vUV;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    output.vTangent = inputPatch[uCPID].vUV;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#fff;font-weight:bold&#34;&gt;return&lt;/span&gt; output;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Different from the constant function or other c functions, there are some parameters setup before the functions. Most of them are self-explanatory. Refer to &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/win32/direct3d11/direct3d-11-advanced-stages-hull-shader-design?redirectedfrom=MSDN&#34;&gt;here&lt;/a&gt; if you are confused by any of them. In Unreal Engine 4, more information will be generated in hull shader instead of just passing through the original data. Some of information will be used to fix some crack issues introduced in tessellation. Other information will be used to generate curved surface from control points.&lt;/p&gt;
&lt;h2 id=&#34;tessellator&#34;&gt;Tessellator:&lt;/h2&gt;
&lt;p&gt;There is not much to say about tessellator since it is non-programmable. However understanding the exact way of tessellating primitives will help even if you are not in control of every thing.&lt;/p&gt;
&lt;p&gt;A triangle is taken as an example here. Other primitives like quad behave in a similar way. There are five steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The three edges are tessellated first.&lt;/li&gt;
&lt;li&gt;The triangle is divided into concentric triangles according to the inner tessellation factor.&lt;/li&gt;
&lt;li&gt;The connection between the largest inner triangle and the original one is dropped.&lt;/li&gt;
&lt;li&gt;The outer edges are then subdivided according to the three outer tessellation factor.&lt;/li&gt;
&lt;li&gt;The outer ring is filled with triangles by connecting the points from step 2 with the points from step 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/tess_step.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/tess_step.png&#34; width=&#34;500&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;The graph is taken from &amp;ldquo;An Introduction to Tessellation Shaders&amp;rdquo; in OpenGL Insights. For this very case, the outer tessellation factors are 3,3,2, and the inner one is 5.&lt;/p&gt;
&lt;p&gt;Another detail deserves our attention is how to divide a specific edge given a tess-factor.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/divide.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/divide.png&#34; width=&#34;500&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;The above graph reveals some detail here. I am not gonna dig into it because it is not mentioned in DX SDK doc and may be different on different platforms. However one important thing to be noticed here is that we have a sudden change as tess factor goes from 2.0 to 2.5 in the first column. &amp;ldquo;Equal&amp;rdquo;, &amp;ldquo;Even&amp;rdquo; and &amp;ldquo;Odd&amp;rdquo; are actually the partitioning parameter setup in hull shader. With &amp;ldquo;equal&amp;rdquo; as partitioning method, you may have a sudden change even if you tess factor changes smoothly, that means you may have a similar sudden change in the shape of the geometry. In order to avoid it, choose the other two if possible.&lt;/p&gt;
&lt;h2 id=&#34;domain-shader&#34;&gt;Domain Shader:&lt;/h2&gt;
&lt;p&gt;Domain shader is like another vertex shader. However they are not exactly the same. Here are some differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Domain Shader is right behind tessellator.&lt;/li&gt;
&lt;li&gt;There will be per-patch constant data available, while only vertex data is available in VS unless you pass it through constant data.&lt;/li&gt;
&lt;li&gt;The vertices taking in are located in barycentric coordinate. You have to interpolate all attributes in a way you desire.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other should be similiar. If GS is disabled, either VS or DS needs to output clip space position. If you want to implement your tessellation with displacement map, here is the place to go. You can read a texture here and adjust the height of the vertex according to some value, which is usually displacement in displacement map.&lt;/p&gt;
&lt;p&gt;The way position attributes are interpolated matters here. It defines what kind of tessellation you are using, whether it is flat tessellation, phone tessellation and pn tessellation. Cry Engine 3 provides all three methods by default. Unreal Engine 4 has flat tessellation and pn tessellation. Of course if you want a phone tessellation in UE4, it is quite easy to implement one.&lt;/p&gt;
&lt;h3 id=&#34;flat-tessellation&#34;&gt;Flat tessellation:&lt;/h3&gt;
&lt;p&gt;As it implies by its name, flat tessellation just interpolate position values linearly, which will be totally flat. It doesn&amp;rsquo;t make much sense to use the method alone. Usually it is combined with displacement map to create a bumpy surface.&lt;/p&gt;
&lt;h3 id=&#34;pn-tessellation&#34;&gt;PN tessellation:&lt;/h3&gt;
&lt;p&gt;PN tessellation treats the control points as bezier control points of a curved surface. Different from flat tessellation, PN tessellation will change the shape of the silhouette even if there is no displacement map attached. Its goal is to smooth the silhouette of objects to avoid edge corners. For a further detailed explanation, please check it &lt;a href=&#34;http://ogldev.atspace.co.uk/www/tutorial31/tutorial31.html&#34;&gt;here&lt;/a&gt;. And since UE4 is open-sourced, you can also get its implementation from github.&lt;/p&gt;
&lt;figure&gt;&lt;a href=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/pn.png&#34;&gt;&lt;img src=&#34;https://agraphicsguynotes.com/img/posts/tessellation_on_d3d11/pn.png&#34; width=&#34;300&#34;/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;As we can see from the above image, the silhouette will be smoother with PN tessellation.&lt;/p&gt;
&lt;h3 id=&#34;phong-tessellation&#34;&gt;Phong tessellation:&lt;/h3&gt;
&lt;p&gt;Targeting the same goal with PN triangle, phong tessellation requires less computation to generate similar effects. Here are the steps for phong tessellation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate tangent plane for each vertex in the triangle.&lt;/li&gt;
&lt;li&gt;Interpolate the position values linearly.&lt;/li&gt;
&lt;li&gt;Project the interpolated vertex position to three tangent planes, you will have three new projected vertices.&lt;/li&gt;
&lt;li&gt;Interpolate the three vertices again to get the final position.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;issues&#34;&gt;Issues:&lt;/h1&gt;
&lt;p&gt;Tessellation also has some issues if not done well. A well known one is the crack issue. A crack is born if vertices that share the same world position have different attribute which have some influence on the final position. The general rule to fix these issues is to unify all attributes that matter.&lt;/p&gt;
&lt;p&gt;Here are some common issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cracks in PN/Phong tessellation hard edges. By hard edge, it means that an edge is shared by two connected triangle, each has a different geometric normal. Both of PN and Phong tessellation will depend on geometric normal to smooth the surface. Differences between normals will cause displacement along different directions, a crack is born here.&lt;/li&gt;
&lt;li&gt;Cracks can also be caused by texture coordinate difference. Under the circumstance that displacement map is used, texture coordinate is a factor for displacement. If two vertices sharing the same world position have different texture coordinate, there will be cracks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generally, those cracks could be eliminated by well defined 3d model. However that puts too much burden on artists. A more practical way of doing this is to fix it through some dedicated algorithm.&lt;/p&gt;
&lt;p&gt;There are several algorithms available. &lt;a href=&#34;https://developer.nvidia.com/sites/default/files/akamai/gamedev/files/gdc12/GDC12_DUDASH_MyTessellationHasCracks.pdf&#34;&gt;Here&lt;/a&gt; are some by NVIDIA. Some of them tried to unify the attributes during pre-processing, like Dominate Data Algorithm. Others aims at generating unified attributes on the run, such as AEN(Average Edge Normal).&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h1&gt;
&lt;p&gt;Tessellation is a brand new feature available on DX11. It offers developers a whole new way to deliver geometry detail to gamers.  It is already widely used in PC games, like Metro 2033. Both of Unreal Engine and Cry Engine have internal integration of tessellation. And it is already well supported on quite a few games in the market.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References:&lt;/h1&gt;
&lt;p&gt;[1] OpenGL Insight. &lt;a href=&#34;http://openglinsights.com/&#34;&gt;http://openglinsights.com/&lt;/a&gt;&lt;br&gt;
[2] PN triangle tessellation. &lt;a href=&#34;http://ogldev.atspace.co.uk/www/tutorial31/tutorial31.html&#34;&gt;http://ogldev.atspace.co.uk/www/tutorial31/tutorial31.html&lt;/a&gt;&lt;br&gt;
[3] Microsoft DirectX SDK. &lt;a href=&#34;https://www.microsoft.com/en-us/download/details.aspx?id=6812&#34;&gt;https://www.microsoft.com/en-us/download/details.aspx?id=6812&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>